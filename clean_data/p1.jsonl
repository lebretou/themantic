{"speaker": "Interviewer", "text": "Duncan: Okay, I guess we can go ahead and start. So I will start with some more abstract questions. So when you hear the word \"multi-agent system,\" what does it mean? For example, how is it different from just a single agent or a single LLM?"}
{"speaker": "Participant", "text": "So, when we talk about it, it's not like there should be one answer to why it's different. But first of all, when we talk about difference between single-agent and multi-agent, when we talk about an agent, we say it should be an LLM, but multi-agent is not... there will be multiple agents. First of all, when we break down the word, if we talk about a product, let's say a particular use case with X, there are 10 agents used, and one agent is a manager agent or orchestration agent, but that doesn't mean the remaining 10 agents will have an LLM. First of all, they are differentiated with that. An agentic workflow is mainly because one agent is involved, and in 99% of cases, only a single agentic workflow has an LLM. But in multiple agentic workflows, it doesn't mean an LLM will be involved. We can take a RAG pipeline. RAG is a basic pipeline and is a use case of a single agent workflow, but multiple agents is not just about LLM. It's about the end-to-end flow with an LLM, how each entity should work, and the main role in that is the orchestration agent. Because when we talk about an agent, it's not just a sequential flow from start to end. Multi-agent is basically not from start to end, or independent agents. If we talk about a space of multiple multi-agent workflows, multiple agents are there, not connected with each other. It's the job of the orchestrator agent to identify which agent to use next. If we create a workflow like A, B, C, D, E, so it's one workflow of orchestration. But if the use case changes without any code change, every time it's not going to be ABCDE, it's DCBA. Like, this agentic workflow will change. So we can say it differs from a single agentic workflow. In a single agent workflow, we will always have to change the pattern of how each agent is going to interact."}
{"speaker": "Interviewer", "text": "I see. It's interesting you said that, like, multi-agent is not a linear, sequential process. I feel like sometimes they can't, right? Like, sometimes it's just multiple agents each having different tool calls or access to different tools. But then the whole pipeline is still linear."}
{"speaker": "Participant", "text": "Yeah, the pipeline is linear, but depending on the user input, sometimes this agent will be needed and sometimes not. So I'll take an example where suppose a prompt is used with an LLM giving a response. For a particular case, we will get a correct response, so it will pass through the pipeline. Sometimes we do not get a correct response, so it will go to the fallback. And sometimes in fallback, we update the prompt based on route feedback. Prompt updation will occur, and this cycle will continue. So like this, the flow changes."}
{"speaker": "Interviewer", "text": "Yeah, I see. Alright, okay. I'll go to the next question. So, I think you already talked a little bit about it in the Google form that you took, but what is your role and experience in developing or using multi-agent systems?"}
{"speaker": "Participant", "text": "So, basically... so you're talking about my academic background, or before that, as well?"}
{"speaker": "Interviewer", "text": "Oh, anything. Just anything."}
{"speaker": "Participant", "text": "So, I'm an international student here, first of all. I did my undergraduate degree in computer science, and now I'm doing my master's."}
{"speaker": "Interviewer", "text": "So."}
{"speaker": "Participant", "text": "I graduated in 2022, and after that, for 3 years, I've worked as an LLM engineer at Morgan Stanley. My experience starts from there, where I have worked on agentic workflows. So, whereas a financial firm, they do not have structured data, they have unstructured data—I mean, just PDFs. So, no Python code can detect the pattern to read a PDF and get structured data. We need an agentic AI system there. Initially, we started with a single prompt, that is an LLM, then we expanded to a single agent workflow, then we expanded to multi-prompt, multi-agent. Each data source is different, each data type is different. So, it's like an infinite possibility of how much data we will get. That part is done by a manual analyst who writes a document. So, my multi-agent workflow is: the first agent is reading the data. There is another agent. When we read the data, we get the page. So, we use vector embeddings to get the correct pages because documents are more than 100 pages. Once we get the correct pages, suppose we have to extract 10 data points. As I can say, there are infinite possibilities of data, so we can't write prompts for each data type. And we can't write a generic prompt. But the manual analyst prepared documentation, which I have an agent that reads and creates a prompt for me. So I'm using LLM to create the prompt itself and then use that prompt to extract the data that I want. Then in other regions, we are doing validation, data storage, and the normal process. That's what I worked on during my time at Morgan Stanley. After coming to Maryland, at UMD, I have been working as a research scientist at the iSchool and research assistant at the iSchool under Cody Button, where I'm mainly using LLMs in agentic workflows for crisis fact-checking. There is a project going on under Universal Media. It's a dataset they have created, which is a dataset of social data science involving crisis-related data. It's already there for textual data. I'm working with LLMs in agentic workflows to get data for images and other modalities. Along with that, I'm also working with Professor Tiani Ro, where we have just completed our research on design charts, where we are using LLM agentic workflows to fine-tune a model. We are doing LoRA fine-tuning and full fine-tuning, where we have created a dataset using the ShaRQ5 dataset, where the basic goal is to create a JSON with design choices. With just reading the CSV, our fine-tuned LLM can give particular industry-level, specific industry-level data."}
{"speaker": "Interviewer", "text": "I see. So for those projects that you just described, like what kind of framework did you use? Like LangChain, LanGraph?"}
{"speaker": "Participant", "text": "Yeah, LangChain and LangServe."}
{"speaker": "Interviewer", "text": "Then channeling graph, okay."}
{"speaker": "Participant", "text": "Yeah."}
{"speaker": "Interviewer", "text": "I see."}
{"speaker": "Participant", "text": "Oh, I'm exploring MCP and Google ADK framework in a recent hackathon project as well."}
{"speaker": "Interviewer", "text": "I said, okay. Yeah, it seems like LangGraph is still the more popular choice."}
{"speaker": "Participant", "text": "More popular choice, yeah."}
{"speaker": "Interviewer", "text": "Yeah."}
{"speaker": "Participant", "text": "It's derived from LangChain, and LangChain—because Google's ADK is coming now with MCP, Google ADK. But previously, from the start, it's LangChain that started to develop everything."}
{"speaker": "Interviewer", "text": "Yeah. Have you ever had a chance to use one of those visual tools, like ones with nodes as agents, and then you can draw links?"}
{"speaker": "Participant", "text": "Cool."}
{"speaker": "Interviewer", "text": "Like, there's this tool called... I guess I can share my screen and show you real quick."}
{"speaker": "Participant", "text": "Yeah."}
{"speaker": "Interviewer", "text": "Sure. Okay. So I think people have come out with... can you see my screen?"}
{"speaker": "Participant", "text": "Yeah, I can see."}
{"speaker": "Interviewer", "text": "Yeah, people came out with, like, this kind of interface, but I guess it's not really fitting for your tasks, because they're more for, like, daily tasks. So they have, like, some type of trigger that, like, say whenever you get a Gmail, a new email, it triggers the pipeline, and then it does something."}
{"speaker": "Participant", "text": "So, as you're talking about visualization and notes, I haven't particularly used any, but I've used Neo4J."}
{"speaker": "Interviewer", "text": "Oh, what is it called?"}
{"speaker": "Participant", "text": "It's called NeoForge. It's a graph database. So basically, an agent—it's—when it was developed, it wasn't developed for agent workflow. It's a graph entity database where we do normal SQL-like queries. But later on, they integrated agents into that workflow."}
{"speaker": "Interviewer", "text": "Oh, okay."}
{"speaker": "Participant", "text": "And it's integrated with data sources integration with anything, even Google Cloud, AWS, HDR. You can directly connect your cloud environment. But it's not open source, and it's for commercial use. So basically, when I cannot take it on long-term, when I do some small class projects or hackathons, I use that. I'll basically create an account and get GCP free credits for one month."}
{"speaker": "Interviewer", "text": "Okay."}
{"speaker": "Participant", "text": "End user, yeah."}
{"speaker": "Interviewer", "text": "Okay, so for the first project that you talked about, the one that structures data."}
{"speaker": "Participant", "text": "I cannot provide a corrected version of \"Like…\" as the text appears to be incomplete or contains only filler speech. Please provide the full text that needs to be corrected."}
{"speaker": "Interviewer", "text": "Is the system perhaps, like, going to reuse the systems over and over, right? Whenever there's new data that comes in? Or is it pretty automatic? It just triggers itself?"}
{"speaker": "Participant", "text": "Yes, it automatically triggers itself because we have a collaboration with Azure and JCP, so we have a data lake where we store data. Whenever new data comes, we've created a PubSub request. So PubSub is basically publisher and subscriber. Whenever any data comes, we get a trigger request, and our pipeline will start."}
{"speaker": "Interviewer", "text": "Okay. And then."}
{"speaker": "Participant", "text": "And one more point, we also target the volume because we do not do one-to-one processing. We do processing on a bunch of volume, so we are based on fiscal year and the quarter we are in because in financial terms, the data volume changes by quarter. So we target at which data volume should trigger."}
{"speaker": "Interviewer", "text": "Okay."}
{"speaker": "Participant", "text": "Oops."}
{"speaker": "Interviewer", "text": "I see. And then you said that the reason why you had an LLM is because the data was in PDF, right? And you kind of need..."}
{"speaker": "Participant", "text": "Data was in PDF, and it's not just PDF. Because if it's in PDF and the pattern is structured, we can use regular expressions or Python code to detect that pattern in a code format and get the data. But in PDF, things change. Companies—because the companies writing the reports, they are not AI systems, nor any system was giving the structure. They were humans writing those reports. And your money may fall in."}
{"speaker": "Interviewer", "text": "And handwriting, okay."}
{"speaker": "Participant", "text": "It's not handwriting, it's like a Word document."}
{"speaker": "Interviewer", "text": "Oh, okay."}
{"speaker": "Participant", "text": "A document, it's like, it's not handwriting, but it's not different than handwriting. They're writing a report, like, in a notes document using a computer, but those are human analysts. They won't follow the same pattern. Companies' patterns change, report types change, so the uncertainty in there is too much."}
{"speaker": "Interviewer", "text": "Okay, I see. And it seems like, given a task, a pipeline is probably the best choice to automate different parts."}
{"speaker": "Participant", "text": "Yes."}
{"speaker": "Interviewer", "text": "Okay. Alright, I'll jump to the next question. So, again, let's still talk about the data project. Could you walk me through from when you had the initial idea to when you had the concrete multi-agent system architecture set up? So, like, what were the first couple of things that you decided? Like, the roles of the agents, or did you sketch out the architecture?"}
{"speaker": "Participant", "text": "Yeah, so as I said, first of all, our initial goal was not to go directly towards multi-agent systems, because as I was a professional LLM engineer there, I've worked for 3 years. So initially, there was a Pythonic and manual analyst available. First, we eliminated the manual part with direct prompting. So we just wrote a Pythonic script and replaced the manual part with just prompting using one algorithm. Then we moved on to a single agent workflow. Then when we moved on to the multi-agent workflow, so first is our PubSub part that we added, where we are getting the volume of data and determining when to trigger our pipeline. And as soon as we trigger it, the second agent we added is to read the data, which is the PDF. And then when we read the PDF, we found out that if we pass the entire PDF, our token limit will shoot up."}
{"speaker": "Interviewer", "text": "Right."}
{"speaker": "Participant", "text": "Because at a firm, cost is also an important factor. So, what we did was foresee the vector database. We got to know about Mistral database. Why Mistral? Because Mistral has a very scalable DB when we scale it with the help of Google Cloud. And its natural language processing is very good, as we tested in our experiments. So, what we did was add an agent that, first of all, will take these new documents. If we get any new documents and old documents, there are two criteria. If we get a new document, we take the document first, include it in the Mistral vector database. Then, there is another database where we define what kind of data we need to extract from a document. This changes from year to year, quarter to quarter, and company to company. So, based on that, we get a company name and match it to the database with which fiscal year we are processing and which quarter we are processing. So, we get, like, X amount of data points we need to extract. First of all, once we get these data points, we get a data definition. Then we use another agent that uses this data definition to query the vector database to get the page numbers. Then we have a shrunken size PDF where this data will be available. We only need to query this PDF. So one agent will return these PDF page numbers. Then there will be another agent which will use these X data point names we have gotten and there is a data rules document we have. Each data point has a rule, basically instructions on how to extract the data. So we use that and pass it to the rules document. This is an LLM agent. What it will do is use this rules document with X amount of data sources and create a prompt for us for data extraction. And at the end, it will be in JSON format in which we get the data. Then there is an LLM executor agent who will take this prompt and take these pages that we have gotten and use this prompt to extract the data, taking only the shrunken size document pages as input. Once we get the data, there are 3 to 4 other agents that will take this data, process it, validate it, and if everything is correct, push it into the database. This process also spans 4 to 5 regions, and at the end, there is an orchestration again. We also have validation to check if we are getting the correct data format or not."}
{"speaker": "Interviewer", "text": "Okay, so when you decide the system prompt for all of those agents, did you have to trial and error, or…"}
{"speaker": "Participant", "text": "Yes, we have to do trial and error. We have done trial and error making this timeline, like, a minimum of 6 to 6 months to 1 year. And how we have assessed the accuracy—because as a financial firm, we have historical data available. So first, we tested our pipeline with historical data. In that data, we have ground truth available for 10 years minimum. So we used that data to test our pipeline. Then we made it run in production."}
{"speaker": "Interviewer", "text": "Okay. Did it, like, eventually reach... Oh, like, what was the accuracy in the end? Like, probably not 100%, right? Or, I always worked."}
{"speaker": "Participant", "text": "It's more than 90%, and that's our aim."}
{"speaker": "Interviewer", "text": "More than I need."}
{"speaker": "Participant", "text": "Yeah."}
{"speaker": "Interviewer", "text": "Okay, and did you look into those, like, less than 10%? Like, what were the errors?"}
{"speaker": "Participant", "text": "So when we looked into that, it's basically data formatting errors. When the LLM couldn't give us the proper structure of the data, that's where the data is thrown out. Because when we talk about LLM and data extraction, it's rare that the LLM will fail. Because the LLM doesn't have space to hallucinate. We're giving the LLM the definition. We're saying, \"On this page, we have the data. All you need to do is map it to a particular data point. This data is for this field.\" So here, the LLM doesn't have space to think too much."}
{"speaker": "Interviewer", "text": "I see. So because it's mostly just, like, text-related tasks, and you guys have very specific definitions."}
{"speaker": "Participant", "text": "Yes."}
{"speaker": "Interviewer", "text": "Okay."}
{"speaker": "Participant", "text": "So, because of the definitions, it's not only that it defines things, it's a set of rules."}
{"speaker": "Interviewer", "text": "It's a flood."}
{"speaker": "Participant", "text": "You know, this step."}
{"speaker": "Interviewer", "text": "I see. And then, could you also talk about how you defined or eventually came up with the context and, for example, memory management of the entire orchestration between agents? Because I know this project used LangGraph, right? LangChain."}
{"speaker": "Participant", "text": "Yes, thank you."}
{"speaker": "Interviewer", "text": "I know that LangGraph has, like, their states—what's called the state—as their memory. So basically, like, each agent just either retrieves or, like, updates the states. That's, like, the shared context."}
{"speaker": "Participant", "text": "Okay, so first of all, how can I say, memory optimization—we have done that. Yeah, so first of all, we haven't focused much on how we should do the memory allocation, because we have used GCP. So when we talk about scaling, GCP automatically does that, because our pipeline, everything was done by GCP, so orchestration is not an issue for us."}
{"speaker": "Interviewer", "text": "Yeah, yeah."}
{"speaker": "Participant", "text": "We haven't, but we are focused on tokens, so our cost should be minimized."}
{"speaker": "Interviewer", "text": "Okay, yeah, that's fine, it's not always relevant. Okay. And then I think... Okay, I'm gonna ask this question. So, like, before you go into actually writing the code, when you're still trying to design and maybe think about the overall architecture, what was the hardest part for you? Or what was the biggest challenge?"}
{"speaker": "Participant", "text": "The biggest challenge was the orchestration part, like how these agents should work together. And the main part was... because this is the different thing that we are doing. We are not creating the problem; we are telling the LLM itself to create the problem. That was the hard challenge for us."}
{"speaker": "Interviewer", "text": "And to create a system for that, we have to do, like, many trial and errors, okay."}
{"speaker": "Participant", "text": "Yeah."}
{"speaker": "Interviewer", "text": "I see. Did you also have, like, the rules for the prompt writer?"}
{"speaker": "Participant", "text": "Yes, so that's what I'm talking about with the system prompt. So there we have mentioned many rules. When we read the document and create a prompt, we shouldn't get a distraction and all. So that's why we have provided many instructions that you should read only from this document to create a prompt. And with the prompt, you should give an output format of the prompt that you have created. So like that, we have specified many instructions."}
{"speaker": "Interviewer", "text": "Okay, I see. Yeah. And also, just a side note."}
{"speaker": "Participant", "text": "So..."}
{"speaker": "Interviewer", "text": "Actually, there have been, I think, 12 people signed up, and I think your project, or your experience, is like by far the most sophisticated. People—I can tell you some of the other responses. They're mostly like a pipeline for summarizing research papers, things like that."}
{"speaker": "Participant", "text": "Okay."}
{"speaker": "Interviewer", "text": "Yeah, and okay, so I want to talk a little bit about debugging. First of all, can you tell me how you usually interact with or monitor the system once it's up and running? And how do you verify the results? I guess you talked about it a little bit—you have ground truth. But let's say, did you have a monitoring interface or any methods that you used to monitor the results?"}
{"speaker": "Participant", "text": "So, first of all, as I said, we have ground truth. So, for initial debugging, we use the ground truth, but that type of debugging is for the end result. But before that, we have used Google Cloud Logging. Since our system is complicated, we can't just use a normal print statement and say \"your error\" with a try-catch and say \"your error has occurred.\" We are never going to find out at which module we have faced that error. So that's why we use Google Cloud Logging. Why? Because using Google Cloud Logging, it will give us the exact location, basically the exact module where the error has occurred. Then we can use the live Google Cloud debugger to get all the state variables at that point of logging and debug it. So yeah, these are the two kinds of debugging we have done. One is using the ground truth to get metric-level debugging, and the other is programmatic-level debugging."}
{"speaker": "Interviewer", "text": "Okay, so for Google Cloud, like, debugging—usually, like, what kind of errors does it throw out? Like, compiling errors, or…?"}
{"speaker": "Participant", "text": "It's sometimes there were resource errors, like not getting the model LLM, not getting the correct LLM, API key expired, or system variable changed. It's not syntactical. Sometimes we got syntactical errors, but that's rare. Most of them are our runtime memory errors."}
{"speaker": "Interviewer", "text": "I see. Okay. And then so for the... okay, more questions about Google Cloud. So because I haven't had a chance to look at it, like, what kind of system is that? Is there, like, a dashboard for you to monitor?"}
{"speaker": "Participant", "text": "Yeah, so Google Cloud Logging is a service with a kind of dashboard that gives us the ability to monitor everything. And apart from the other services we use, it's just for deploying and running our server live. So it shouldn't be on a system, but an actual server."}
{"speaker": "Interviewer", "text": "Okay. And then, like, what type of information will you see? Like latency? Or like…"}
{"speaker": "Participant", "text": "Latency, if a particular [agent] we have gotten an error result also, throughput of error, the frequency, what we are getting, the error of data, both errors occurring where data are—are we getting the data. What are the inputs, what are the outputs, data getting stored at the database, then you're getting the throughput of that also. So, like that, yeah."}
{"speaker": "Interviewer", "text": "Okay. So the next question is going to be a little bit abstract. Some people describe their relationship with AI systems in terms of how much they trust or rely on them. Do you feel like that's relevant?"}
{"speaker": "Participant", "text": "So, for my project, it was totally relevant. But, it's not like we are trusting AI. I'm using LLM to do a specific task, and I'm measuring it using my ground truth."}
{"speaker": "Interviewer", "text": "Yeah."}
{"speaker": "Participant", "text": "Yeah."}
{"speaker": "Interviewer", "text": "So it's just… there were two steps. It's not that I'm trusting AI, it's not like I'm not trusting AI. I'm using my own metric to determine if I should trust this prompt or not."}
{"speaker": "Participant", "text": "And at a point where I get the best from it in there."}
{"speaker": "Interviewer", "text": "And you're using strategies to make it more reliable or, like, consistent."}
{"speaker": "Participant", "text": "More consistent, yes."}
{"speaker": "Interviewer", "text": "Yeah, okay. And for the framework—the Google Cloud or LangGraph—are there any signals of, like, confidence or uncertainty in the system? This is also a little bit abstract, but I'm curious about that."}
{"speaker": "Participant", "text": "So, I haven't found any uncertainty at that time, but obviously there is uncertainty in response output format. However, in most cases, the LLM results are strictly relying on the JSON format."}
{"speaker": "Interviewer", "text": "Oh, sometimes they don't strictly follow?"}
{"speaker": "Participant", "text": "Yeah, sometimes they didn't follow."}
{"speaker": "Interviewer", "text": "Do you have a sense of why that might be, if you have already defined it?"}
{"speaker": "Participant", "text": "It's because, you know, sometimes it's because they are not clearly able to understand the structure, so that is a part of uncertainty and hallucination. But mainly, I think that occurs because we have updated it later on. If we have too much data, too many pages have been detected, then we are passing in multiple prompts. So, for that, they are minimized very much. So, it is the number of tokens you are passing. Okay. I think that's the answer for that."}
{"speaker": "Interviewer", "text": "Okay, and this might not be true, but like, have you ever encountered cases where, since you're comparing the final result with some ground truth, has there been any cases where the ground truth is correct, but then there's some problems in the process?"}
{"speaker": "Participant", "text": "So yeah, for the initial pipeline writing, we had that problem. It's in the validation agent, where the ground truth is correct, we are getting the correct response from the agent, but somehow our LLM isn't working. So our agent isn't specifying it as valid. But it's not an LLM issue, it's not an agentic issue, it was a programmatic issue—initial debugging. It's not an error from the LLM side, it's like a developer error. We missed something or some line isn't working as intended. It should be like an output format error. For example, if we're getting 1 in numeric but it's 1.0, both are the same, but the data types are different. That's one of the examples. So we fixed it, but it's like initial setup as we're writing the code, before testing also."}
{"speaker": "Interviewer", "text": "Okay, I see."}
{"speaker": "Participant", "text": "And then, okay."}
{"speaker": "Interviewer", "text": "The next question is more open-ended, so if you can design an ideal multi-agent system workflow, what kind of signals or information would be helpful?"}
{"speaker": "Participant", "text": "Basically, the first two should be the main agenda of the multi-agent workflow: input, output, and determining exactly which agent LLM should be needed. Is it actually needed? That's why we are using it, or are we just using it to make it sound like a fancy LLM agentic term? That distinction will be very much needed."}
{"speaker": "Interviewer", "text": "Okay. And input-output, okay."}
{"speaker": "Participant", "text": "Input-output is the base, we need that, and the goal also of the project. It doesn't mean, like, we should have 10, 20 agents there in our project."}
{"speaker": "Interviewer", "text": "I see. Okay. And then, I finally have some more open-ended questions. I think you also touched a little bit on it. So, like, just generally, what kind of problems do you think multi-agent systems are especially appropriate for and work the best?"}
{"speaker": "Participant", "text": "Oh, sorry, I couldn't get that part."}
{"speaker": "Interviewer", "text": "Okay, yeah, so what kind of tasks do you think a multi-agent system is most appropriate for, or when should we definitely choose a multi-agent system for…"}
{"speaker": "Participant", "text": "Where you should have at least one LLM call happening, and the other is that the process of the pipeline should include more than one operation. That includes, like, an end-to-end flow. What I mean by end-to-end flow is, if you are reading the data to output the data, so this is one operation. An extra operation should be included. Rather than just read and output, then we should use a multi-agent flow."}
{"speaker": "Interviewer", "text": "Okay. And then… okay. I see. Yeah, so… I watched this, I think it was a video from LangGraph, but… I think their definition was kind of similar. Basically, whenever you need to make multiple tool calls, it's good to have multiple agents, because the more available tools for a single LLM, the less reliable it can be. Sometimes it doesn't know how to choose the right tool."}
{"speaker": "Participant", "text": "Yeah, that's the right part. Yeah, sometimes it doesn't know that. And also, one more thing—there's no harm in having one operation, one tool. Sometimes people use multi-agent systems, but with only a single agent, they involve multiple operations."}
{"speaker": "Interviewer", "text": "Yeah."}
{"speaker": "Participant", "text": "Yeah, we can have a single tool, single operation."}
{"speaker": "Interviewer", "text": "Okay, I have a final question. We have 40 minutes. So I think this is a little bit similar, but what are some challenges that you can think of when you're developing—it doesn't matter if it's designing or actually implementing a multi-agent system. And what do you think can make this easier?"}
{"speaker": "Participant", "text": "When developing an agent, yeah, I rely on... so you're talking about it from a developer point of view, right?"}
{"speaker": "Interviewer", "text": "Yeah, like from a developer's point of view."}
{"speaker": "Participant", "text": "Yeah, I rely heavily on documentation."}
{"speaker": "Interviewer", "text": "on documentation?"}
{"speaker": "Participant", "text": "Yes, LangGraph LangChain documentation. That's the best resource you will get. Yeah, that's basically because developing an agent workflow is not that hard when we talk about it from a coding point of view, but how you orchestrate it—that's the orchestration part. That's the critical part. Because the code is very similar."}
{"speaker": "Interviewer", "text": "Yeah. And by all..."}
{"speaker": "Participant", "text": "Easy to..."}
{"speaker": "Interviewer", "text": "You mean, like,"}
{"speaker": "Participant", "text": "Connecting different agents together, input and output."}
{"speaker": "Interviewer", "text": "Infinite amount."}
{"speaker": "Participant", "text": "An agent should receive what output, which agent should receive what input from which agent, and which produces what output, and that output should be input to which agent, and which all possible agents. That's the main part. So it's not a coding part."}
{"speaker": "Interviewer", "text": "But it's an operational orchestration part. And then I guess, like, the system prompt as well, because you need to try…"}
{"speaker": "Participant", "text": "System problem. Yeah, that's for the LLM agent, because not every agent will be an LLM agent. Yeah. Okay. Cool."}
{"speaker": "Interviewer", "text": "Yeah, okay. And that was all the questions I have, but I think that was really helpful, because I think a lot of other people are just students, and you actually have some industry experience, so that's good. Yeah, and I'm also surprised because people have different uses for multi-agent systems—they use them for completely different purposes. Some of them even used something related to clinical. And there's also one response that uses a similar approach to yours, but they use multiple agents to do debates. Each agent debates against other agents to increase the overall accuracy of the response, I think."}
{"speaker": "Participant", "text": "Yeah, it will be, like, I don't know. Might be a different use case, but sometimes it's not needed, also."}
{"speaker": "Interviewer", "text": "They're going to find some benchmark that's going to prove their framework is better."}
{"speaker": "Participant", "text": "Yeah."}
{"speaker": "Interviewer", "text": "Yeah, it's not always about branching out. If you implement the same thing using a single agent or a single Python script."}
{"speaker": "Participant", "text": "You will get that benchmark, but it doesn't mean that it's suitable for multi-agent."}
{"speaker": "Interviewer", "text": "Okay."}
{"speaker": "Participant", "text": "But yeah, that's…"}
