{"speaker": "Interviewer", "text": "Alright, we can go ahead and start. So I'll first start with some more abstract questions. When you hear the term multi-agent system, what does it mean to you? And, for example, how do you see it as different from, say, a single agent or a single LLM model?"}
{"speaker": "Participant", "text": "So for the multi-agent system, I believe each agent should have different goals and be equipped with different tools or capabilities. For example, they can access different sources or interact with the environment in a more diverse way, so each agent can do what they do best. And therefore, they can collaborate in a more efficient way and achieve and complete more complicated tasks. So I think that's the strength of building the multi-agent system to me, yeah."}
{"speaker": "Interviewer", "text": "So first of all, each agent should have a different task or goal, and then they have access to different tools or some shared context."}
{"speaker": "Participant", "text": "Yeah, I think yes."}
{"speaker": "Interviewer", "text": "Okay, can you talk about your experience with using or developing multi-agent systems?"}
{"speaker": "Participant", "text": "Yeah, so I think I can share and show my screen? Yeah."}
{"speaker": "Interviewer", "text": "I'm ready to help, but the text you've provided only contains \"outlet.\" which appears to be incomplete or a fragment. Could you please provide the full interview text that needs to be transcribed and corrected?"}
{"speaker": "Participant", "text": "It has been... many, two years ago, I participated in two different projects involving multi-agent systems. Let me share my browser. I can see that? Can you see my screen?"}
{"speaker": "Interviewer", "text": "Yeah."}
{"speaker": "Participant", "text": "Yeah, there are basically two projects. So let me talk about the first one. So this one is somehow interesting, I believe. It's from two years ago, in the summer of 2023. I built a simulation platform which involves different agents with diverse personalities. They can travel around the map, as you can see to the right. And so this is also geographically grounded, so they can know which country they're in. And when agents that are geographically close to each other, a conversation could be triggered. So we can see in the call-out in the middle, it starts a conversation, and some participants who were involved started to talk to each other. And so here, all the agents, at that time, I think they're all GPT-3.5 based, and the only thing different is they have different personas. So you can see, I think, in the center. You can see they are initialized with different ages, different places of birth, and different habits, something like that. So we can build, we can build a very high-level goal, for example, they should share where they have gone."}
{"speaker": "Interviewer", "text": "I have to."}
{"speaker": "Participant", "text": "Into what's interesting—they see in different locations. But on this one, that's a very prototype because after one or two months of developing, we found there is an open source project which is developing in a faster way than ours, so we just aborted this project. So this is my very first-hand experience about the multi-agent system, but it's very simple, as you can see. And the second one is also in 2023. So we built systems that we wish to leverage the power of multi-agent systems to give a more calibrated confidence estimation for a certain question. So first, maybe I can talk about it a bit more in detail. So, for example, we want to give an agent one question, right? So an agent can give an answer and it can say this answer is 80% likely to be correct. So it's the confidence. But what we wished was that the confidence can be more calibrated. So, for example, if we select a bunch of questions and answers that the agent has 80% confidence in, we hope that 80% of these answers can actually be correct."}
{"speaker": "Interviewer", "text": "Match the accuracy."}
{"speaker": "Participant", "text": "Yeah, so that's… that means more calibrated. So here, we want to collaborate—we want to involve multiple agents in a debate system. So agent debate was a very popular topic in 2023. So we did that. We involved the agents, so we asked them to do a two-phase debate, and the agents can revise their initial answer based on other models' critiques. So you can see these agents are in different models. They can be ChatGPT, Mistral, or Cohere. So they have different internal knowledge, so they may be able to assist other agents to have a more comprehensive and considered answer. So basically, that's what we were doing, and finally we can see that it did help the models in their confidence estimation in multiple datasets. Yeah, so those are the two projects I have first-hand experience with in multi-agent systems. So it's not… they're both in very controlled, counter-experiment settings, so they couldn't do something free-form exploration. So yeah, I think that's a limitation in my experience."}
{"speaker": "Interviewer", "text": "Oh, but I think this is deviating away from our interview, but it's okay. I've already read quite a bit of literature on confidence collaboration, and then I see that the results are much better compared to self-verification or just sampling methods."}
{"speaker": "Participant", "text": "Yeah."}
{"speaker": "Interviewer", "text": "That's good. Okay. But then the accuracy—or not accuracy, the confidence. And then also the way that models update their confidence is still mostly just using prompts and then their self-correction, right?"}
{"speaker": "Participant", "text": "That's correct. We prompt them to do the update."}
{"speaker": "Interviewer", "text": "I see. Okay, and then I can proceed with the interview. So, could you talk a little bit about the development? For example, what framework did you use? I saw LangChain."}
{"speaker": "Participant", "text": "Yeah, of course, we only use LangChain. So we just consider the LangGraph also, but for such simple functionality, LangChain is good to perform some simple agent development."}
{"speaker": "Interviewer", "text": "Okay. And then, so I think one of the challenges you guys have is probably developing the system prompts, right?"}
{"speaker": "Participant", "text": "Yeah, so we actually didn't try to optimize the prompt in certain high-level methods. We just tried a few candidates for the prompt and we simply chose the one with the most calibrated confidence."}
{"speaker": "Interviewer", "text": "Okay. So I guess let's keep talking about the second project."}
{"speaker": "Participant", "text": "Yeah."}
{"speaker": "Interviewer", "text": "So it seems to me that the structure of the entire system is pretty intuitive, since you're just having the agents doing debates. Were there any changes to your initial idea of the structure to the final architecture, or the structure of the system?"}
{"speaker": "Participant", "text": "I think it's the final one is pretty similar, pretty close to the initial scope of the framework. Yeah, because there are actually only two stages. So the first stage has only one agent involved, and they give the initial result. And the second stage is to have a group discussion and change the first stage result. So yeah, that's because the framework is quite simple, so we didn't update the structure many times. And the initial result is good enough, so we didn't spend much time updating it."}
{"speaker": "Interviewer", "text": "Okay. I see. And then, I don't know if this is relevant to your specific projects, but as you may know, a lot of people use multi-agent systems for automation, and then something that they have to deal with is context management or memory among agents. For example, what are some shared variables and shared data, and what should be specific to an individual agent? Also, tool management—like what tools an agent should have access to. Did you have to deal with anything that I mentioned?"}
{"speaker": "Participant", "text": "So for the project I have been involved in, we didn't face this challenge because the context of the whole conversation is very manageable. Because there won't be many environment variables that are fed to the agent. Yep."}
{"speaker": "Interviewer", "text": "Okay, yeah, totally. And then let's talk a little bit about the implementation of the system, and back to when you were still developing them. So just talking about coding. So when you're using LangChain and you have a prototype of the system, how did you interact with and monitor the behavior of the system?"}
{"speaker": "Participant", "text": "I should give more detail about how to act or monitor the system."}
{"speaker": "Interviewer", "text": "Yeah, so for example, let's talk about the debate project, I guess. Okay, for that project, there are a couple of things you should probably care about, which is the collaboration, whether it's good or not, or whether the result is correct. Did you also pay attention to, let's say, the intermediate outputs from the agents? Like, what are some of the arguments?"}
{"speaker": "Participant", "text": "So in this project, there's nothing we call intermediate. I believe you're referring to some interactions between agents that aren't very relevant to the final result or should be visible to users in the intermediate stage. Is that correct?"}
{"speaker": "Interviewer", "text": "It's more so just… How did you make sure that each agent is doing what it should be doing, I guess?"}
{"speaker": "Participant", "text": "Yeah, because we also—give me one second to organize my words. So, I think the interaction between agents is actually data we'll be actively collecting. So, because this experiment setting is pretty simple, I believe, because they're only doing limited terms of interaction. And their interaction can make a difference in their final result. So, we will collect them all and investigate what's actively contributing in the conversation. So, yeah, so maybe this doesn't address your concern, but that's because the experiment setting is pretty simple. We don't simply read them all. We read the intermediate output manually, and we can see what's going on there."}
{"speaker": "Interviewer", "text": "Yeah, that's good. No, that's kind of the response I was looking for. Did you also observe any patterns? Like, for example, if something like... okay, the LLMs are initially overall pretty overconfident, and then they see some arguments, and then they drop their confidence."}
{"speaker": "Participant", "text": "I think so, yes, but I couldn't find—let me see if I can find some data. It's just from two years ago."}
{"speaker": "Interviewer", "text": "Okay."}
{"speaker": "Participant", "text": "Let me try if we collect some data here. Hmm, give me one second… Yes, I think the best approach is that we didn't keep a copy of the data, but it's mentioned in the appendix. Let me share my screen again."}
{"speaker": "Interviewer", "text": "Okay. Give me one second."}
{"speaker": "Participant", "text": "Yeah, so basically it's mentioning figure 4, so it shows before and after the second stage of the distribution of model captains change. So we… so first is the average accuracy, so you can see there the bins in the 0.2 and 0.5, they almost disappeared, and the confidence around 0.9, it increases a lot. And this is for the average confidence, so generally increases. And for the calibration, so a desired calibration is more close to the line in the graph. So we can see both increase in accuracy and calibration."}
{"speaker": "Interviewer", "text": "I see. Okay. And then, so when you're still just implementing the system, say, using LangChain, were there any common bugs or issues that you run into? Sorry, I know it's a long time ago."}
{"speaker": "Participant", "text": "It's been a long time, yeah. But I couldn't remember, actually, because I don't see other users having such problems when they are trying to incorporate a long list of actions. But here, because the agents in these settings are basically doing the conversation, they don't even incorporate the tools that the tour in... Yeah, that's building LangChain. So what we're doing is mostly about how the prompt, the memory, and the conversation work. So yeah, we didn't incorporate many advanced..."}
{"speaker": "Interviewer", "text": "Yeah. Was memory hard to manage?"}
{"speaker": "Participant", "text": "It's not hard to manage, because actually these two projects use one codebase. So the memory is actually more involved in the simulation part."}
{"speaker": "Interviewer", "text": "Okay."}
{"speaker": "Participant", "text": "Yeah."}
{"speaker": "Interviewer", "text": "How did you sort of manage the memory in this one?"}
{"speaker": "Participant", "text": "Basically, we only keep the four conversations because, right, because for GPT 3.5, the context window is pretty long. And when the context goes longer and beyond the context window that we can manage, we will use another agent to summarize the past experience, and then we only keep the summarized experience in the memory. So we simply truncate the long-term memory and only keep a summarized version."}
{"speaker": "Interviewer", "text": "That's it. And then, I guess, for some of the agents, basically in this project, each agent has sort of their own memory."}
{"speaker": "Participant", "text": "Yes."}
{"speaker": "Interviewer", "text": "They have some similar system prompts, I guess."}
{"speaker": "Participant", "text": "Yes, that exact thing."}
{"speaker": "Interviewer", "text": "Okay. I see. And then, since we're still talking about this project, I feel like the longer you run this simulation, were there any unexpected behaviors you saw from the agents?"}
{"speaker": "Participant", "text": "Yeah, so..."}
{"speaker": "Interviewer", "text": "Fascinating."}
{"speaker": "Participant", "text": "So we also tried different models, like some, I think Llama 2, or some even smaller models in 2023, and we did see some patterns that the agents try to repeat their words, and so the repeated output, or I'm sorry, I couldn't recall most of the..."}
{"speaker": "Interviewer", "text": "Okay."}
{"speaker": "Participant", "text": "No behaviors. So, I think they're mostly similar to those behaviors that LLMs are given a very long context. Especially when we are using a somewhat smaller model. For example, they will output—sorry, for example, they will sometimes forget, and row flipping is another phenomenon I observed. So it will forget the person that we gave it at first, which is also kept in the memory and the system prompt, and after a certain interaction with another agent, the agent will believe he's the other agent. My mistake. Yeah, so I think that's also investigated in another paper I co-authored, but it's not focused on the multi-agent system."}
{"speaker": "Interviewer", "text": "hyphen."}
{"speaker": "Participant", "text": "Yeah, this just repeated output or row flipping. There are the two abnormal behaviors I can't recall."}
{"speaker": "Interviewer", "text": "Okay. That's pretty interesting. So what was the initial goal of this project?"}
{"speaker": "Participant", "text": "So for this project, at the beginning, I was asked to collaborate because someone is doing the debate."}
{"speaker": "Interviewer", "text": "And that was another intern in my..."}
{"speaker": "Participant", "text": "In my undergraduate lab, I was in an undergrad study. And because the intern could not manage both the debate and the simulation, my advisor asked me to mainly develop the simulation project and develop the backbone—I mean, the multi-agent system interaction code. So yes, we didn't set a very specific goal for this. My advisor had multiple very open ideas, like he wants to do a simulation with more than 100 agents. But just as I mentioned, this project has been aborted."}
{"speaker": "Interviewer", "text": "Yeah."}
{"speaker": "Participant", "text": "There's one interesting thing I think I'd share with you. So, one year later, in 2024, there is another intern who continued to work on this project, and I think they have a publication on the open experiment setting. Give me one second, maybe you will be interested in this."}
{"speaker": "Interviewer", "text": "Yeah, I also saw a couple... I think the simulation now scaled up to even a million, if I'm not wrong. There's a paper on the number of agents."}
{"speaker": "Participant", "text": "Yeah, I'll... Oh, I saw it on the paper, but I have not taken a very close look at this. So this is definitely the continuation. This is definitely continued from my previous message. I put it in the chat box."}
{"speaker": "Interviewer", "text": "I see. Okay. Alright, we can go back to our interview."}
{"speaker": "Participant", "text": "Oh, okay."}
{"speaker": "Interviewer", "text": "Yeah, so... just a second, trying to locate the question. Okay, so you said that since your structure was fairly simple, and then just using LangChain."}
{"speaker": "Participant", "text": "Yeah, yeah. So, even without the framework, I believe we can manage. We can build this system even because we simply call the models' API and give the context. So we don't even need an agent framework for this. Yeah, so the agent is more specifically related to the first simulation. For another second debate once."}
{"speaker": "Interviewer", "text": "Yeah, okay. I guess LangChain or LangGraph mostly are helpful because of, like, tools and memory management, but you guys don't."}
{"speaker": "Participant", "text": "Yeah, even the..."}
{"speaker": "Interviewer", "text": "Excellent."}
{"speaker": "Participant", "text": "and wrote in, yeah."}
{"speaker": "Interviewer", "text": "Okay. And then, actually, I just have 3 more questions, maybe 2. And they're more like broad and abstract. So it doesn't have to be specific projects that you worked on, but for what types of problems do you think a multi-agent system works the best? Or what kind of tasks are they good at? Or maybe bad at."}
{"speaker": "Participant", "text": "So, I believe the multi-agent system can be best when they're trying to manage multiple goals at the same time. For example, they can work on their specific goals. I'm sorry, let me manage my words better. Maybe I can come up with an example. Yes. So, for example, if we are trying to ask one LLM agent with different tasks. For example, can I write on the whiteboard?"}
{"speaker": "Interviewer", "text": "Yeah."}
{"speaker": "Participant", "text": "Oh, okay, I can see your whiteboard. So, for example, there is a Task 1, Task 2, and Task 3. So, for example, if you want one agent to deal with these three tasks at the same time, and each task has subtasks. And, for example, each task needs an interval to complete. So, task 1 has such tests and tasks, and Task 2 has this, and Task 3 has this. So, if you are asking one agent to complete this, the agent cannot, maybe not be able to manage the multitask at the same time, because I believe that's because in their instruction tuning stage, they are not exposed to such a complicated setting. So, by the multi-agent system, we're able to split a complicated task into multiple parallel small tasks. And even though we need to give some shared content to the agent, we can achieve this by a shared memory or even conversations between the agents. So I think that's the first case where I believe the multi-agent system can work the best. And the second one is when a model is capable of multiple tools or specific actions. So that's pretty similar to this because if we are equipping one agent with multiple tools, that means even though we assume these tools are covered by a very simple API that can be learned in very simple instructions, the numerous tools will make the instruction super long in the prompt. So we need to teach how an agent should call the tool. And even some tools are not even necessary in one setting, but we hope that the agent can know there is a tool existing. So, by a multi-agent system, we can split the capabilities and tools into different agents and ask them only to perform the tools that are most frequently used. And as well as that, the second thing is task management. And the third one can be context management. Because after we split one agent into a multi-agent system, each agent's context will be much more manageable, so we don't have to actively compress or summarize their memory. So a model can be more robust in their behavior. Basically, that's three cases where I believe the multi-agent system can perform much better than single agents. It's from the perspective of a language model agent, because I'm not exposed to vision agents or others. So, yeah, that's my perspective."}
{"speaker": "Interviewer", "text": "Okay, so to summarize, the three scenarios are… so, first of all, when the tasks are inherently dividable into smaller subtasks. A second scenario is when there are numerous tools, and then we want to split the tools into multiple agents so that each agent will have a smaller range of tool selection."}
{"speaker": "Participant", "text": "Yes."}
{"speaker": "Interviewer", "text": "And then the third is for better context management."}
{"speaker": "Participant", "text": "Yeah, absolutely. Okay."}
{"speaker": "Interviewer", "text": "I think that's a pretty comprehensive response. I guess it's what a lot of people are saying, too. Okay, and then I just have a final question. I feel like this might not be too applicable to you, and I probably have already asked similar questions. So basically, what are the biggest challenges, do you think, in developing a multi-agent system? And then, is there anything—for example, if there's some add-on features you could add to LangChain—what would that be?"}
{"speaker": "Participant", "text": "Oh, that's actually a very big file."}
{"speaker": "Interviewer", "text": "It is a big question."}
{"speaker": "Participant", "text": "Now, let me think for a while. Yeah, so I believe one feature that I hope should be incorporated into LangChain is flexible task scheduling for a certain agent. So I'm not sure if that has already been incorporated into LangChain or LandGraph, but let me illustrate what kind of step it is. So let me use the whiteboard."}
{"speaker": "Interviewer", "text": "Yeah, go ahead."}
{"speaker": "Participant", "text": "Yeah, so for example, let's say this only applies to a very open problem, so there is no targeted answer that can be a trained model to do the optimization. We can only ask in order to do more free-form exploration. So for task scheduling, we wish the model to first do some planning like this. But this cannot—this plan can also be different from what's in the real scenario or real circumstance. So the model has to change their plan when they're doing the exploration. And so there can be multiple general goals, so if I spend much time on certain subtasks, maybe it should give up and go to the last add, or it may come up with more subtasks to do in certain steps. So maybe it will create some new steps to do. And it should assess how each step makes the agent closer to its final goal. So I believe that's the very rough structure of what I said—the task scheduling for an agent system. I'm not sure if someone has built a very similar feature, but I believe it's the most important feature."}
{"speaker": "Interviewer", "text": "Okay."}
{"speaker": "Participant", "text": "That the model that an agent can use in the freeform exploration, because first, it's an unseen scenario. And the experiment might not be fully visible to the agent in certain steps. So, whenever the agent is doing certain exploration, the environment will change. So, it needs to manage certain task scheduling in the exploration. So, yeah, I believe that's the feature I would like to add to the Lenshin package. But it's very rough, a rough Friday."}
{"speaker": "Interviewer", "text": "So you're saying, like, correct me if I'm wrong, you want the agent to have the ability to deal with unseen scenarios."}
{"speaker": "Participant", "text": "And open exploration of a task. Yeah."}
{"speaker": "Interviewer", "text": "Yeah, so I know that some of the frameworks do have…"}
{"speaker": "Participant", "text": "I notice the text you've provided is very short: \"Back to React, and…\"\n\nThis appears to be a fragment that is already transcribed correctly. There are no transcription errors, stutters, grammar errors, or spelling errors to fix. The ellipsis at the end suggests the speaker was interrupted or paused mid-thought, which is natural in conversational transcripts.\n\nHere is the text as-is:\n\nBack to React, and…"}
{"speaker": "Interviewer", "text": "Like LangChain. So I know that it has some mechanism called fallback, which is basically when it gets a certain input that is not expected, it goes to a different branch and then tries something else, or just throws an error. But it's…"}
{"speaker": "Participant", "text": "Yeah, it has been explored by multiple agent frameworks, and I think LangChain has also incorporated some of the frameworks, like ReAct. I'm not entirely sure about this, let me check. Yeah, so let me send that in the chat box. Oops, it's on the whiteboard. Okay, anyway. So it has already,"}
{"speaker": "Interviewer", "text": "Already incorporated some frameworks, but I wish that this framework could be more robust."}
{"speaker": "Participant", "text": "Flexible and applicable to unseen scenarios, or even though more free-form exploration and more dynamic environments. So ultimately, we wish that an agent can mimic human behavior."}
{"speaker": "Interviewer", "text": "Hey."}
{"speaker": "Participant", "text": "Human decision in a complicated scenario, but it cannot deal with those frameworks that are already incorporated in LensChain. They're actually very simple and cannot handle those complicated tasks. So, that's my idea, and I wish that LensChain can be approved."}
{"speaker": "Interviewer", "text": "Okay. Would it be correct if I say that you think that currently, no matter how these frameworks are working, they still follow the human-defined structure or pattern, but then it would be good if the agents or the model had the ability to even change the environment or change the structure, depending on the task."}
{"speaker": "Participant", "text": "This can be one fiscal interpretation of what I generally prefer. Yeah."}
{"speaker": "Interviewer", "text": "I see. That's interesting. Okay, that was actually my last question. Let me just stop the recording."}
