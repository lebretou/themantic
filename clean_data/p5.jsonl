{"speaker": "Interviewer", "text": "Okay. So I'll start with some pretty abstract questions. When you hear the term multi-agent system, what does it mean to you? And how do you see it different from, let's say, a single agent or a single LLM?"}
{"speaker": "Participant", "text": "So I think when we're working with multi-agents, a multi-agent system, it essentially means you're not using a single LLM that tries to do everything, but a team of specialized agents that can collaborate just like we do in a real workflow. A single agent is most of the time good for linear tasks, but it tends to hallucinate. It struggles with long reasoning chains, and most of the time, it can't scale across complex tool calls. On the other hand, a multi-agent system makes sure that it breaks the problem into different roles. For example, a planner that can decide the next steps, an analyzer that can execute those tasks, and a critic that can check for correctness, or like a validator that can enforce reliability so that our answers are what we're expecting them to be. So each agent is specifically optimized for a specific kind of behavior, and there's coordination between them, which makes it more accurate than a single agent."}
{"speaker": "Interviewer", "text": "I see, okay. It's a very comprehensive definition. Okay. And can you tell me about just one or two representative projects that you have done in the past that used multi-agent systems?"}
{"speaker": "Participant", "text": "So, okay. I would think, okay. So, in my internship at Amazon this last summer, I worked on creating a project where we were building an autonomous root cause analysis system built for very large-scale operational logs at Amazon. And so we had different types of agents in that which coordinated instead of just one model. So we had all of this information that I know about it through that itself, the planner agent, which decomposed the incident question into steps. There was a retriever agent which pulled the relevant log slices. There was an analyzer agent which generated—we generated domain-specific Python code to inspect patterns because the data was so huge that most of the time it would hallucinate, you know, pinpoint what was the root cause behind the failure of delivery at the right time. So we had to make sure that we were using just a subset of the dataset to get the pattern. And obviously, we understood the risk that there could be a possibility where the subset of data that we are using and the Python code that we are generating, and hoping that it would generalize to the entire dataset, there is a possibility that some root cause could be missed. But that's something that we need to improve on, of course. There was a critic agent that reviewed the code for correctness, a validation agent which performed the t-SNE checks and the SHAP attribution to ensure we have reliability. That was one of the projects that I worked on recently that used multi-agents."}
{"speaker": "Interviewer", "text": "I see. And in that project, what was your role? Did you focus on a specific component, like maybe writing system prompts, or were you doing the entire project?"}
{"speaker": "Participant", "text": "So I was doing the entire project. I was just given the problem statement that this is what we're currently facing, and the major, the most important problem that I kept facing was the amount of data set that we had, the number of columns—it was just way too large, you know?"}
{"speaker": "Interviewer", "text": "It's way too long."}
{"speaker": "Participant", "text": "So, way too large. It took a lot of time to come up with some form of way to make sure that the LLM doesn't hallucinate, and we can understand at least 90 to 95%—I think that was the threshold. I don't remember exactly, but 95% of the reasons why the package was missed. My responsibility was that I was just given the problem statement, so I had to do everything on my own. So that was one of the main things that I had come up with, and the other thing was that I proposed we use DSPy-based prompt signatures that created domain-specific Python analyzers instead of relying on fixed prompts every time."}
{"speaker": "Interviewer", "text": "I see. And for that project, do you remember what kind of frameworks did you use? Like, some popular ones, like LangChain, LangGraph?"}
{"speaker": "Participant", "text": "So, of course, we used the SPY for prompt engineering and agent signatures, and we did use Langchain in the beginning. I mean, I tried considering the possibility of integrating LangGraphs as well, but the time period was too short to understand both of those things. I was a little familiar with the workings of Langchain, so we used Langchain for orchestrating the Planner, Analyzer, and Critic workflows."}
{"speaker": "Interviewer", "text": "Okay, and then you said that you have a planner. Could you go over the high-level strategy?"}
{"speaker": "Participant", "text": "Second, I think my yoga."}
{"speaker": "Interviewer", "text": "Okay."}
{"speaker": "Participant", "text": "Not working that well. Let me just switch to normal audio."}
{"speaker": "Interviewer", "text": "Okay."}
{"speaker": "Participant", "text": "Can you owe me fine now?"}
{"speaker": "Interviewer", "text": "Yeah, okay, here."}
{"speaker": "Participant", "text": "You know."}
{"speaker": "Interviewer", "text": "Okay. And then, could you just briefly go over the high-level structure again? So you have a planner, and then…"}
{"speaker": "Participant", "text": "Hello?"}
{"speaker": "Interviewer", "text": "Hello?"}
{"speaker": "Participant", "text": "Yeah, sorry, I'm so sorry. There's just some network issue."}
{"speaker": "Interviewer", "text": "Oh, I guess, okay, network issue. No worries."}
{"speaker": "Participant", "text": "So, at a higher level, okay. We have a planner, then we have the worker, the critic, and finally the validator. That was the architecture. So the planner agent interprets the user query and breaks it into actionable steps. In this case, the user was the operators who had to manually write those reports explaining why the delivery was delayed. Then we have the retriever or the worker agent, which executed those steps. The data was stored on Redshift, so querying the Redshift data, analyzing the event logs, generating the Python code to run specific checks. Then I had the critic agent, which evaluated the outputs—the JSON structured outputs that I got—for correctness and logical consistency as well. And finally, I had the validator agent, which performed statistical validation using t-SNE and SHAP, and drift scores. There was some different kind of score that we came up with as part of the project, and schema checks to confirm that the results are reliable."}
{"speaker": "Interviewer", "text": "Okay, alright, thank you for clarifying. And then, was this your idea to use a multi-agent system for this specific task, since you mentioned that you were just given the task, or did they kind of say you have to do this?"}
{"speaker": "Participant", "text": "Not really. Again, they were just giving me the problem statement at hand, and it was a very open-ended discussion. They were open to discussing any ideas, so I proposed the multi-agent approach. Because when I analyzed the problem, I realized that a single LLM necessarily wouldn't work. So, again, the time period was very short, right?"}
{"speaker": "Interviewer", "text": "Okay."}
{"speaker": "Participant", "text": "So I had to, like, I did not have the time to test first starting with testing a single LLM, because from my experience working with agents, I thought that it wouldn't be reliable for multi-step reasoning, tool calling, and statistical validation at the same time."}
{"speaker": "Interviewer", "text": "Okay, I see. That's fair. And then, so I guess it was kind of intuitive to have this multi-agent system, but could you walk me through how you went from knowing that it's going to be a multi-agent system to knowing exactly how many agents you'll have, what each individual agent is, and what their role is going to be?"}
{"speaker": "Participant", "text": "I think so."}
{"speaker": "Interviewer", "text": "Or did the high-level structure change over time? Did you have, like, the critic agent at the very beginning, or did you kind of add that later?"}
{"speaker": "Participant", "text": "I mean, of course, the plan did change a couple of times as we went along, but I started from the end. Of course, I had regular discussions with my mentor as well. I would just put an image out that, okay, this is what I think we can move forward with. And of course, I had my co-team members as well, who really helped me. So I tried to break it down into first understanding the incident by manually going through each of those logs and defining the question, and then pulling the right slices of logs, then analyzing the patterns or writing the code. I tried doing all of this manually by myself for almost a month. That's what I was told, that for a month I need to do all of those things that I expect the LLM to do manually, the way operators would do it, and summarizing all of it using Jira. So that gave me."}
{"speaker": "Interviewer", "text": "Natural phase boundaries, I would say. I see."}
{"speaker": "Participant", "text": "Phase is, like, a candidate. You know, and so after that, I would like to turn those phases that I had in mind—okay, this is how it should be executed—into, like, capabilities or responsibilities. So the understand and plan phase would be the planner region. Then I would pull the logs and run the analysis. So that would be something that I would want an analyzer region to perform. So, the actions that I performed from start to end in the first month, I tried visualizing, okay, this is what I want, and for this particular purpose, for this particular phase, it's a little independent from the next or the previous phases, so I need a different agent for it. Or even if it's dependent on the first or the next phase, this is how I want the orchestration to be."}
{"speaker": "Interviewer", "text": "Okay, I see. And I guess in that same month, you also kind of figured out, like, how do you orchestrate among agents, like what kind of tools or how do you manage the memory also in that process?"}
{"speaker": "Participant", "text": "Yes, I see. Yeah, I think it was the first month itself. The first month was very challenging. I mean, just understanding how Amazon worked and how the delivery worked—which I was very surprised took me a really long time. And I had to think very carefully about the orchestration and the memory part as well, otherwise the system would just become a black box. You know, so yeah, I mean, they taught me a lot of things on the orchestration side. I remember we learned something about the planner-centric approach. There was some graph-style pattern, like defining each agent with a very clear contract input."}
{"speaker": "Interviewer", "text": "Totally."}
{"speaker": "Participant", "text": "and tools it can call using DSPy and LangChain. So, yeah."}
{"speaker": "Interviewer", "text": "Okay. And then, so they taught you, like, this general knowledge, and you kind of just apply it to your own project."}
{"speaker": "Participant", "text": "Yeah, the DSPY and Langchain part, of course, I knew a little bit about it and knew how to work with it, but planning the loops, tool calling, the agent patterns was my contribution. But my mentor and my buddy—I don't remember, but I think that's what he was called—he really helped me in figuring out how to translate those ideas into a real production-level system, because I'd never worked on such a large scale."}
{"speaker": "Interviewer", "text": "I see. And then, do you remember if we had to write the system prompts for the agents?"}
{"speaker": "Participant", "text": "So before we integrated DSPy, yes, I had to write those prompts manually by myself, and I honestly think by the end of 2.5 months itself, we'd been using the manual process. Like, you know, for each agent, I would define things like who you are, what your role and scope is, what things you're allowed to do, the tools, the data, the constraints, what you must output, like the schema or the form or the success criteria, and what you must not do. That was a very frustrating part of the prompt writing. I literally displayed my frustration in every problem, like, okay, you were not supposed to do this, how many times do I tell you? But yeah."}
{"speaker": "Interviewer", "text": "That's funny, okay. So I guess it was like a lot of trial and error. Sometimes they give you unexpected behavior, and you have to manually add that to the prompt."}
{"speaker": "Participant", "text": "Yeah, right."}
{"speaker": "Interviewer", "text": "Okay, that's frustrating, interesting. Okay. And then I think you talked a little bit about how you evaluated the system? You had some certain type of score. But could you briefly go over how you monitored the system? Or once you had the first prototype running, did you have any tools that helped to monitor the system? Maybe checking the traces, seeing intermediate outputs like that."}
{"speaker": "Participant", "text": "So, okay, talking about the score, the score was—we called it the thrift score, just to give it a name—but it was... So the data that we had, right, since the dataset is so huge and we're using Python code to create tools, you know, that could be generalized to the entire dataset. Now, I can't use the entire dataset for finding some pattern, because then it would hallucinate. So what I want to do is I want to select a subset of data, and the patterns that I get from those root causes, I use that subset data to create Python tools that can be generalized to the entire data."}
{"speaker": "Interviewer", "text": "Okay."}
{"speaker": "Participant", "text": "So, the main thing that we kept thinking about was: okay, I understand that I need a subset of data, but what would be the technique used to get that subset? So there are different ways, like very simple ones, maybe random or the centroid way, or all those different things. But again, if you're using a method like centroid, it would just give you the most common ones. So we thought that if my data can be converted into LSTM using embeddings, and if I use that dataset and then see how much the product is changing stages—let's say today it was ingested, then tomorrow it was staged, it was packed, all of these stages, right? So we observe that when a product is going through multiple stages back and forth, it most of the times leads to not being delivered on time. So we need those tracking IDs that are unusual. That's why we used LSTM embeddings to understand the packages that are showing the most unusual behavior compared to the most common ones, and then just use those selective packages. Based on that, we compared different techniques. It's not like we thought about this, implemented the math, and just chose that technique. No, we compared it with different techniques and we got the most unique root causes through that technique, so that's what we went ahead with. And for every run of the pipeline, we tried to produce a structured trace. So when the agent acted, of course, we had the planner, the analyzer, and all those things."}
{"speaker": "Interviewer", "text": "Yo."}
{"speaker": "Participant", "text": "What tools were called were Redshift, of course, Redshift Query, S3 Read, Python Sandbox, and Shab."}
{"speaker": "Interviewer", "text": "Was that through Amazon's, like, some AWS tools?"}
{"speaker": "Participant", "text": "Yes."}
{"speaker": "Interviewer", "text": "Like, the structured traces."}
{"speaker": "Participant", "text": "Yeah, AWS tools."}
{"speaker": "Interviewer", "text": "Oh, okay."}
{"speaker": "Participant", "text": "And so the way we tried to evaluate—again, the time period was so short that we, of course, couldn't really experiment with it that much—but we tried to compare the amount of time it takes operators to create the final report of root causes at a particular station on a particular date with the amount of time it takes the LLM to create the same report. We got those reports and shared them with the operators, and after we shared them, we needed, like, a thumbs up to confirm, okay, this looks good. And maybe this is something that can be worked on further ahead and in production as well. So that's something we did get. We did get thumbs up from the operators as well. They liked the results that the LLM was returning."}
{"speaker": "Interviewer", "text": "Nice."}
{"speaker": "Participant", "text": "But yeah, time, I think time and the operator's approval and their ability to trace back. Like, if they see some reason—let's say that a product was placed in a different bag than it was expected to, and that's the reason that the product delivery was delayed. Now, when the operator looks at this report, there should be some way that it can trace back to the data that was used to get that decision in the report. So that traceability was also possible. So overall, they liked it. Of course, more things could be done."}
{"speaker": "Interviewer", "text": "I see."}
{"speaker": "Participant", "text": "The time was short, so…"}
{"speaker": "Interviewer", "text": "Yeah, yeah, totally understand. Okay, just for clarification, so strictly talking about, like, just monitoring the pipeline, the system, you primarily just relied on the trace logs, the structured logs, and you're inspecting, like, the tool use and the outputs from that information."}
{"speaker": "Participant", "text": "Hmm."}
{"speaker": "Interviewer", "text": "Okay. And then, so I think you already talked a lot, but when you're developing, have you run into other issues? Like bugs that are more maybe like engineering?"}
{"speaker": "Participant", "text": "I did. So initially, since… okay, let me think… I was actually very much engrossed in the statistical part, but some of the issues were engineering-based issues. Like, yeah, there was one: the tool called tool calls, they made were failing a couple… a lot of times. There were schema mismatches sometimes. But most importantly, tool calls failed a lot of times, and the Python code timed out. That was one of the things that I felt was one of the hardest issues, because for a really long time, I couldn't figure out what was going wrong with it, because there were times when it was, you know, working from start to end, perfectly giving me the report, but there were also times when it just failed or something, and I did not understand. I would go over the code again and again, so it was an engineering-based problem."}
{"speaker": "Interviewer", "text": "Oh, okay. Did you figure out the reason eventually?"}
{"speaker": "Participant", "text": "So I mean, we spoke about this. I'm not sure, honestly."}
{"speaker": "Interviewer", "text": "Okay."}
{"speaker": "Participant", "text": "went wrong. I mean, my mentor and I, we tried understanding that, but we thought that so we were using Claude for that purpose, and we thought that like, sometimes even when you're using ChatGPT, right, it would just stop working, or something like that. So I thought that was something that was happening with the Claude model as well."}
{"speaker": "Interviewer", "text": "We couldn't figure it out. So it's probably more like an AI issue, LLM issue."}
{"speaker": "Participant", "text": "Yeah, I guess so."}
{"speaker": "Interviewer", "text": "Okay. And less engineering, okay. So let's talk about the same issue. Sometimes the model just doesn't behave. Like, LLMs can just fail sometimes. Do you think there's anything that can help with that? It's a big question. It's okay if you don't have an answer."}
{"speaker": "Participant", "text": "Yeah, I mean, by the end of the second month, I felt like my internship was not data science related, but just prompt engineering related. The LLM won't behave like I want it to, and I would just complain in front of my manager every day. It feels like I'm just doing prompting, and I'm just talking to the LLM every day. That's the only thing that I'm doing. Because they might hallucinate for a single column, they might choose the wrong tool, they might produce code that doesn't even make any sense at all."}
{"speaker": "Interviewer", "text": "I woke up."}
{"speaker": "Participant", "text": "Convincing. It is so convincing all the time, because when I start my day, I read it like one or two code bases, and I'm like, okay, my brain is working. By the third time, I get so convinced by what the LLM is saying that even if in my subconscious I know that what it is telling me is wrong, the way it tells it is so convincing that we could easily convince ourselves, okay, yeah, maybe it's right. So because we tried to log every agent step, you know, the tool calling and the intermediate output, we could immediately see where the breakdown happened. For example, if the planner chose the wrong next action, the trace can tell us, or if the analyzer generated code referencing a missing field, which is not even present, we can see it in the tool logs. So I think that is something that helped me understand better where exactly the LLM misbehaved, and where it's important that I pinpoint exactly where it's off track, and that makes it just much easier to adjust the prompts, or tighten the tool schemas, or redefine the agent responsibility, so that the model behaves consistently throughout."}
{"speaker": "Interviewer", "text": "And you talked about, like, intermediate outputs. So I guess at the time it was pretty painful that you have to look at the outputs manually and try to see where exactly it failed."}
{"speaker": "Participant", "text": "Yes, I had to do that. Like, for every step, I had to manually look at all the outputs. It was… I mean, now that I think about it, the project was not that difficult, but then each step took a lot of time because there was a lot of use of LLMs involved, multi-agents. It can be difficult because of the output that every agent gives you. You have to look at it and make sure that the output is what the next agent is expecting, and if not, then how does the next agent handle it? Even that, I felt, was very important and time-consuming, so I had to manually inspect the traces and the outputs to understand where things were breaking. But I think that phase is unavoidable in any multi-agent system, and before you."}
{"speaker": "Interviewer", "text": "Right."}
{"speaker": "Participant", "text": "To make the monitoring, you need to understand the failure modes yourself as well. So once I manually debugged a few runs, I started seeing patterns, like the planner misunderstood certain query types, or the analyzer hallucinated the column names. So after that, I automated most of the painful parts. I tried adding structured traces so that it's easier for me as well, and so I don't have to inspect the raw outputs anymore. Yeah, the first runs were very manual and messy, but that's actually exactly what informed the design of monitoring or checking the reliability that eventually made things a little smoother for me. But by that time, it was anyways pretty late, so…"}
{"speaker": "Interviewer", "text": "Okay. Yeah. I definitely agree with what you said. Like, that's just the problems that you described are pretty common with all kinds of multi-agent systems. And I think that's why most of the frameworks, like LangChain, they have some sort of error handling or a fallback strategy, where if it doesn't output a schema that you want, it just goes to a failure branch. Things like that. But yeah, it's very relatable. Okay, let me try to locate the next question. And then, okay, the next... maybe two or three questions? And they're the final two or three questions. They're going to be more abstract and high level. It doesn't have to be the specific topic or project that you just talked about. So, first of all, some people would describe how much they trust some AI systems. Like, they would describe their relationship with AI systems in terms of how much they trust or rely on them. Do you feel like that's relevant to your experience?"}
{"speaker": "Participant", "text": "How much they rely on their LLM system and how much they trust it. I do think that the trust part is relevant, but I see it differently than just how much I trust the system. I mean, it's like evaluating the predictability or the transparency, and of course, the verifiability. I don't know. But, like, to what extent can I verify it? So, I don't trust an AI system the way I trust a person, of course."}
{"speaker": "Interviewer", "text": "You wouldn't."}
{"speaker": "Participant", "text": "I trust the system to the extent that I understand its behaviors, its limits, or how it was trained, or the quality of its data. And when I design the multi-agent systems, I try to trust that when I give clear roles, or layers, or tool grounding, or consistency checks. So, yes, of course, trust matters, but not blind trust. It's structured trust that comes from making the system more auditable. Yeah, so I think I just trust to that extent, but not blind trust."}
{"speaker": "Interviewer", "text": "Okay. Since you mentioned transparency, let me ask: Do you think that if you built some pipeline, some MAS, some multi-agent systems, there would be signals or visual cues that would make you think, \"Okay, this is pretty transparent, and this system is predictable. I can expect the behaviors coming out of it.\" What kind of signals or indicators would make you think that the system is trustworthy and predictable?"}
{"speaker": "Participant", "text": "I... oh..."}
{"speaker": "Interviewer", "text": "It's okay if you don't know."}
{"speaker": "Participant", "text": "I think... I'm not sure, honestly."}
{"speaker": "Interviewer", "text": "Okay."}
{"speaker": "Participant", "text": "I think when I see patterns multiple times—like stable orchestration, consistent tool calls, or intermediate outputs that are more interpretable—if I see those patterns consistently, that's when I feel the system is transparent enough."}
{"speaker": "Interviewer", "text": "Or that I can..."}
{"speaker": "Participant", "text": "anticipated behavior, not perfectly, of course, but…"}
{"speaker": "Interviewer", "text": "Hmm."}
{"speaker": "Participant", "text": "A little, reliably."}
{"speaker": "Interviewer", "text": "I see."}
{"speaker": "Participant", "text": "I don't know if that's the answer."}
{"speaker": "Interviewer", "text": "Yeah, no, it's a pretty vague question too. And then, so you mentioned that a lot of times the system had, like, tool call issues. I'm a little bit interested about, like, are you talking about, so let's say you have the same input, same user query, whatever, and if you run the pipeline like 10 different times. Is there going to be a situation where, like, 8 out of the 10 times it was doing it correctly, and then the two times it failed or failed to call the right tool? Is that what it is, or is it just, like, different testing?"}
{"speaker": "Participant", "text": "So, like, yeah, of course it did happen a lot of times that the output was just not what I was expecting. Like, especially early on, I did see cases where the same input would sometimes route slightly differently or call the wrong tool. Oh, I'm sorry, I got a little carried away with what I was thinking about. Could you repeat your question once?"}
{"speaker": "Interviewer", "text": "Yeah, so I was just wondering about the tool called failure that we talked about. Was that the case? Or, what was it like? If you just run the same system without changing any code—let's say you run the system 5 times using the exact same input and exact same task. Is it going to be like, okay, it worked 4 out of 5 times, but then it failed that one time? So you said it was also an LLM issue, like sometimes LLMs work, but then very rarely they just fail completely."}
{"speaker": "Participant", "text": "Honestly, like, 80-90% of the time, if the output that I was expecting, I did not get that. That was most of the times because of LLM failure. The other times, it was okay, I got the report, I looked at the report, and then I manually looked at the dataset to understand. And of course, for the data that I'd been using, every day the operators are writing the reports, right? So I have a view of what the operators have written, and comparing it with what my LLM has given me as well. Right. So I would compare it with that, and sometimes I felt that it pinpointed to some root cause that even the operators did not catch, or did not dive that deep, because the LLM—I realized that, for example, if there was a very common issue, like something called container hierarchy mismatch. So the LLM would really capture that very well. But it needs context of what that meant. Like, the container hierarchy mismatch, what does it even mean? The operator looks at it, they would understand it, but if, you know, it needs to—the way an operator would write it, they won't in their reports write something like, okay, these many products failed delivery because of container hierarchy mismatch. They would write it in a format that's more human-readable and understandable."}
{"speaker": "Interviewer", "text": "Very little, okay."}
{"speaker": "Participant", "text": "So, that was something that I would feel was an issue. Sometimes the LLM would see, okay, container hierarchy mismatch. It would try to get from its own understanding what it meant, and then, you know, rephrase that in the actual report. So I feel that we cannot really be that reliable on the LLM to just look at those words based on just its knowledge, understand, and rephrase it in the actual report, because then it might lead to hallucination as well. So I think that's another failure in the kind of report that we were expecting, that we did not get when we encountered this kind of issue, but mostly it was LLM failure 80-90% of the time. But otherwise, this was also a good enough problem that we were thinking about."}
{"speaker": "Interviewer", "text": "I see. Okay. Yeah, that was actually all the questions I have. Okay, 45 minutes. But yeah. Okay, I can stop the recording."}
