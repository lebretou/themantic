{"speaker": "Interviewer", "text": "Okay. Alright, let's go ahead and start. I'm going to start off with a pretty abstract question. So, what is your definition of a multi-agent system?"}
{"speaker": "Participant", "text": "So, to me, the multi-agent system is a system where we input a task that we want the system to do for us. And the system will do that autonomously through the interaction between multiple agents."}
{"speaker": "Interviewer", "text": "Okay. And can you tell me about one or two representative projects that have used multi-agent systems in the past?"}
{"speaker": "Participant", "text": "Okay, the very recent project that I have done with multi-agents is a system—an agent that I wanted to create because I am a PhD student. What I want is to create a system that can help me with the literature review given a topic that I want to explore. So I developed a system that does that. Given the topic, it will autonomously crawl all of the papers that are relevant on arXiv and analyze those papers and generate a literature review so that I can base my own literature review on that and work from there."}
{"speaker": "Interviewer", "text": "I see. So how many agents are there? You said that there's a web crawling agent, probably like another summary agent?"}
{"speaker": "Participant", "text": "Yep. So, like, I will start with a web agent to crawl all of the papers. That is the first agent that I need. Second one, I will use the LLM Power agent to read all of the abstracts, because we know that I asked them to read all of the abstracts and I asked them to generate some tags. And from those tasks, I will decide whether I will keep this paper or not. So after all of that, I will have, sort of, papers that might be relevant to the topic that I know. But of course, because that filtering is just based on the abstract, so it's not—we cannot be 100% sure that it's going to be relevant. So after having a list of the potentially relevant papers, I will try to let another agent read all of the content in the paper and then decide if we still want to keep this paper or not. And after all of that, I will need another agent to verify all of the information that I already have."}
{"speaker": "Interviewer", "text": "Okay."}
{"speaker": "Participant", "text": "Yeah, like, ultimately, we still need a human to verify all of this information, so I maybe call it, like, a human agent in the very final step."}
{"speaker": "Interviewer", "text": "I see. And then you'll be, like, the human that's doing the check-in."}
{"speaker": "Participant", "text": "Yep."}
{"speaker": "Interviewer", "text": "Okay."}
{"speaker": "Participant", "text": "Me and my teammate."}
{"speaker": "Interviewer", "text": "I see. Did you use any framework for this pipeline that you just described? Like, LangChain or LangSmith?"}
{"speaker": "Participant", "text": "Yeah, I initially tried to use LangChain, but at the end of the day, I feel like it's too simple to use LangChain, so I just built up from scratch."}
{"speaker": "Interviewer", "text": "Just built that from scratch. I see. And then… so, when you…"}
{"speaker": "Participant", "text": "Oh, by the way, I wanted to let you know that from the code, from the coding perspective, I built that from scratch. But for the database, I use a very simple thing. I use Notion database to save everything because Notion is very intuitive. I use Notion because it's simple to set up, and it's very intuitive. It can also have very good visualization for the human verification step later."}
{"speaker": "Interviewer", "text": "Hmm. I actually haven't used Notion, so… the database means that you store all the papers in Notion?"}
{"speaker": "Participant", "text": "Yep, a store of, like, information, and store all of the steps, right? Like I just told you before, there are multiple steps, right? The abstract analysis and then the full paper analysis and human verification. So for each step, I need to save everything, and that information will be saved in the Notion database with multiple columns too."}
{"speaker": "Interviewer", "text": "Yep."}
{"speaker": "Participant", "text": "To let you know, we've already completed step one, and right now we are in steps two and three. That would make it easier for us to verify later, because I think human verification is very important in this project. We know that LLMs or multi-agent systems can do this very well. That can save a lot of time for us, but at the end of the day, it's still research. We need to verify, because we know that recently there is a lot of literature reviews out there, but they can generate some fake citations. So human verification is very important. And Notion Database is one of the best ways for us to help with human verification in our project."}
{"speaker": "Interviewer", "text": "So, for this human verification, did you have this idea at the very beginning, or did you not have this initially, try it, and see that LLMs can still generate fake content, so you decided to add this?"}
{"speaker": "Participant", "text": "So actually, we kept that in mind from a very first step when we built this project because the ultimate goal in this project is to create an LLM-based, powerful agent that can help us generate literature reviews."}
{"speaker": "Interviewer", "text": "Yep."}
{"speaker": "Participant", "text": "That is reliable. But we know that a lot of our imaging can be hallucination. So that's why we keep that in mind and build that from scratch with human verification."}
{"speaker": "Interviewer", "text": "So when you were developing, what were the challenges you encountered, or what was the part where you had to trial and error? For example, like the system prompts or just communication between agents?"}
{"speaker": "Participant", "text": "Okay, so because we were trying to make it as transparent as possible in our process, we want to…"}
{"speaker": "Interviewer", "text": "We want the reliable generated literature review."}
{"speaker": "Participant", "text": "You cannot leave. So, that is… we have to make it transparent enough, but when we develop it, the thing that we need—like, we feel like—is the most difficult is about how to design the threshold for each step. So, for example, as I told you before, the very first step is to use web crawling to crawl all of the papers that are relevant. So, for the very first step, like, we need to define what keywords we need to input to the web crawler. And even though, like, we set a lot of them, or maybe if we just keep the—for example, like, the topic that I worked on is scalable oversight. So, if I just import the keyword \"Scalable Oversight,\" it will give me very relevant papers. But it might not be enough, like, so the precision is very high, but the recall is very low, right? So that's why, like, we want to include a lot of keywords, but then the recall will be very high, but the precision is very low. So that's a trade-off that we need to work on. And also, another thing is, like, when we also use LLMs to analyze the abstract or even the full paper. So, when we analyze those, we need—so because we know that LLMs can hallucinate."}
{"speaker": "Interviewer", "text": "Right."}
{"speaker": "Participant", "text": "So, we need to develop a way to remove, or at least reduce, hallucination through engineering and so on. And that's the last thing that we faced in this project."}
{"speaker": "Interviewer", "text": "I'm ready to correct the transcribed text, but the text provided (\"Sorry, Owen. 13.\") appears to be either incomplete or a fragment. Could you please provide the full interview text that needs to be transcribed and corrected?"}
{"speaker": "Participant", "text": "I appreciate you sharing this, but the text provided only contains \"Divia.\" which appears to be a name or label rather than interview content that needs correction.\n\nIf you have the full interview transcript you'd like me to correct, please share that and I'll be happy to help fix any transcription errors, grammar issues, spelling mistakes, and stutters while preserving the original meaning and content."}
{"speaker": "Interviewer", "text": "So you said prompt engineering. Could you also explain what type of engineering that you did?"}
{"speaker": "Participant", "text": "Okay, I think it's very simple. It's just based on our observation. So, what we did is, like, in our process, what we're trying to do is use an LLM to read, say, the abstract."}
{"speaker": "Interviewer", "text": "That's it."}
{"speaker": "Participant", "text": "And give a score. Like, give a relevant score to the topic. So, for example, I work on scalable oversight. I want to decide whether this paper will be included or not, like instead of just a yes or no question."}
{"speaker": "Interviewer", "text": "Yep."}
{"speaker": "Participant", "text": "And it's going to be very, very hard for us to verify later. We will ask the LLM to generate a score. And yeah, in the very first step, we just asked them to generate a score from 0 to 10, whether it's relevant to the topic. I thought that it works, but it's still not very clear for our verification step later. So what we did is, like, we give them a rubric for each round of scoring. We give them the criteria for this score. For example, if you generate a 10, it's going to be perfectly fit. If it's a 5, it's fit, but somewhat not very relevant. But if zero is completely off."}
{"speaker": "Interviewer", "text": "So, we have a..."}
{"speaker": "Participant", "text": "rubric. Maybe we call it a rubric so that we can…"}
{"speaker": "Interviewer", "text": "Berkeley."}
{"speaker": "Participant", "text": "So that we can interpret the score later, so that it's easier for humans to verify later, that is one of our strategies that we use for prompt engineering. Another way that we try to improve the prompt is because we know that, like, in our group, most of us are not native speakers in English. So what we have done here is a bit tricky, but we were trying to design the problem ourselves first."}
{"speaker": "Interviewer", "text": "And we will."}
{"speaker": "Participant", "text": "And we will ask ChatGPT to improve the problem, yeah."}
{"speaker": "Interviewer", "text": "Yeah."}
{"speaker": "Participant", "text": "Yeah, so that's probably some tricks or some way that we can use to improve the prompt."}
{"speaker": "Interviewer", "text": "I see. And that works pretty well."}
{"speaker": "Participant", "text": "As of now, it's worked pretty well in terms of giving us the literature, the generated literature review, and also it gave us a very intuitive way to verify each step of the pipeline."}
{"speaker": "Interviewer", "text": "I see. So, you said that for each intermediate output from the agents—for example, like the web crawler and the abstract reader—those intermediate outputs are also stored in Notion, right?"}
{"speaker": "Participant", "text": "Yes."}
{"speaker": "Interviewer", "text": "Okay, so Notion was sort of like the place that you monitor the entire system."}
{"speaker": "Participant", "text": "Yep, true."}
{"speaker": "Interviewer", "text": "Got it. Could you brief me, walk me through, or explain what a structure looks like? Because I haven't used Notion myself, I don't know the interface, or just describe it."}
{"speaker": "Participant", "text": "You want me to walk you through, like, the structure of the database, or the way that I set it up?"}
{"speaker": "Interviewer", "text": "Or could you just screen share, if that's..."}
{"speaker": "Participant", "text": "Oh, sure."}
{"speaker": "Interviewer", "text": "Yeah, I'll give you the permission."}
{"speaker": "Participant", "text": "If you want, I think I can share with you the whole structure of this."}
{"speaker": "Interviewer", "text": "Oh yeah, that would be perfect. Okay."}
{"speaker": "Participant", "text": "Okay, can you see the screen now?"}
{"speaker": "Interviewer", "text": "Yep."}
{"speaker": "Participant", "text": "Okay, so it's going to be this. Let me show you. So, like, the reason why I use Notion is because I also use Notion for taking notes. Yeah, so it's very convenient for me and very familiar to me, and also with my teammates. So, for the global oversight, this is going to be the Notion tab that we use for the whole project. So, for example, here we have the meeting notes. So, for example, for meeting notes here. In this section, it's going to be the literature review that we want to have."}
{"speaker": "Interviewer", "text": "Yeah."}
{"speaker": "Participant", "text": "So, you see here, we have a structure. This is from humans—like, we design all of this first. So we redesign all of this. But what we want to do here is, like, we want the LLM to autonomously generate a literature review for us to work from, instead of like, working from scratch. So, what we've done here is divide it into two topics that we will work on."}
{"speaker": "Interviewer", "text": "It's going to."}
{"speaker": "Participant", "text": "The agent, monitoring, and the scalable oversight method. So for each topic, we will have multiple people to work on this. And for each topic, for example, this is the workflow that we used, as I showed you just now. Here. This is the whole pipeline for the multi-agent system that I told you about before. Yep. So what we really need here is, because it's autonomous, so…"}
{"speaker": "Interviewer", "text": "Yeah."}
{"speaker": "Participant", "text": "What we need is just, like, for each topic I showed you here—there are two topics, right? So for each topic, what I need is just some input, some configuration."}
{"speaker": "Interviewer", "text": "Right."}
{"speaker": "Participant", "text": "And then the whole steps here, from step 1 to step 8, will be saved and will be done autonomously. So, for example, for step zero here, you need to define the archive query, main query, and relevance score. It's like a threshold to cut off some papers."}
{"speaker": "Interviewer", "text": "Yeah, and some LLM system prompts."}
{"speaker": "Participant", "text": "Some LLM abstract analysis form, so I told you here, like, this is the rubric that I use."}
{"speaker": "Interviewer", "text": "Yeah, I see."}
{"speaker": "Participant", "text": "Yeah, yeah. And then all of this, I will work on the JSON format because it's going to be easier to interact between the agents. And then we'll do it. We have a perplexity summarization. Don't care about perplexity. It's autonomous, it's not used in inference. But what we hear, like, as I told you before, after we analyze a lot of abstracts and find a shortened list of papers, we will have a prompt to analyze the whole paper. And then, given all of the papers that have been saved, we were trying to put that into a taxonomy generation so that it's going to generate a whole taxonomy."}
{"speaker": "Interviewer", "text": "I appreciate your submission, but the text provided (\"Tectonomy.\") appears to be either a single word or term that may be a proper noun, brand name, or potentially a transcription error itself. Without additional context about what was actually said in the interview, I cannot reliably correct or transcribe this.\n\nCould you please provide the full interview text that needs to be corrected?"}
{"speaker": "Participant", "text": "Yeah. And then finally, we will ask them to generate a writing sample. Like, this is not the one that we will use, because at the end of the day, we still want humans to write the paper."}
{"speaker": "Interviewer", "text": "Yep."}
{"speaker": "Participant", "text": "This writing is sort of like giving you the whole story of what's going on in this research topic, so that we can have a sense of what's going on, and then we will write that later. And for the database, as I took here, this is the whole pipeline. But for the database here, for each topic, we will have our own database. So, for example, I work on this one, so I can walk you through this one first. So, for each database, in the very first step, when you use the web crawler to crawl the paper, we will have the title."}
{"speaker": "Interviewer", "text": "Right."}
{"speaker": "Participant", "text": "All of this excluded or further considered will be none here, right, at that step. But we will have the text generated by the LLM after you read the abstract. You have a URI, you will not have this one. This will be none at this time."}
{"speaker": "Interviewer", "text": "Okay."}
{"speaker": "Participant", "text": "This will be these categories will be the archive category, so you will have this one. But at this time, the key contribution will be none too. Man fighting will be none too. Methodology will be none too. And this one will be yeah, you will have this one. You will also have the relevant score generated by you reading the abstract. The source is from archive and unique ID, so this unique ID will be used throughout our process to identify the paper, yeah. And author, too. So after that, when you have all of this, the thing that you will have after this step will be the relevant score. So after you have the relevant score, you will know that, like, it's based on the threshold that you set. If, say, if you set the threshold is 9, so everything above 9 will be included, but everything below 9 will be excluded. But if you see here, after this process, we will have a verification step. So the very second step here is like you have to look at all of the excluded papers. Yeah, like, for example, you can just filter here from the filter option, like if it's checked or unchecked. So here, I want it to be checked because I want to investigate all of the."}
{"speaker": "Interviewer", "text": "Don't agree with us."}
{"speaker": "Participant", "text": "Yeah, so what humans need to do right now is just filter all of this, have the excluded papers, and you're gonna decide whether this is a true excluded paper. If it's not, you just click it here. It's gonna become the included paper. But I would say, like, this is a very rare case, because if it's already, like… I think the reason why I want to have this human verification is, like, I don't really trust LLM analysis. But I think at the end of the day, at least for this step, it's worked pretty well. Yeah. So after that, like, after we have started the first human verification on the excluded papers, I will probably regenerate all of this, use another LLM to generate all of the keywords, findings, and methodology. For us to have, by doing this one, the next human verification step is gonna be more high-demanding. The reason is, right now, we will check all of the included papers, so we have to look at here. Right now, we check all of the included papers. So for all the included papers, we need to look at the tags here. We need to look at the key contribution here, look at key findings and methodology. So from here, what we do is, like, we will have to carefully check if this paper is a true included paper."}
{"speaker": "Interviewer", "text": "Hmm."}
{"speaker": "Participant", "text": "And if it's yes, then we just leave it there. Otherwise, we have to check. If you really want to just remove it, we just click it here. Otherwise, if you're like 50-50, you don't know, you're not very sure about that, you can check the further consider here. So the reason why we decided for the consider here is, like, we want to have a consensus among the teammates. So if someone is not sure about this one, the other one can have a check later."}
{"speaker": "Interviewer", "text": "I see."}
{"speaker": "Participant", "text": "Yep, so that is how we decide that. And after all of that, we will decide whether we have included a paper or excluded a paper, and whether it's further considered or it's not further considered. And after all of this process, we will sit together and decide which paper will be included. Based on these criteria, and after we have all of the included papers, which we already have here, we will also exclude the further consider papers. Then, based on all of this writing, we will ask the LLM to generate the taxonomy. From the taxonomy, the structure of the taxonomy is JSON, where you have the main topic and multiple citations, and then we will map back to this database, so everything will work on the database to generate these categories. So at the end of the day, we have a few—if I'm not wrong, we have 10 categories here. And for each category, we'll map back to each paper. So if you see here, we only include the papers that we want to include, and then we generate this back. And that is how it's worked in this pipeline. As for how Notion can be used here, I think Notion just… we use Notion for the sake of simplicity, and also it's very intuitive for us to do the human verification."}
{"speaker": "Interviewer", "text": "Yeah, I see."}
{"speaker": "Participant", "text": "Yep."}
{"speaker": "Interviewer", "text": "And?"}
{"speaker": "Participant", "text": "Have you?"}
{"speaker": "Interviewer", "text": "Have you used this pipeline multiple times, or just for this project?"}
{"speaker": "Participant", "text": "I actually just use this for this project, but for two databases, like this topic and the other topic."}
{"speaker": "Interviewer", "text": "Any other topic, I see. Okay. And then... Sorry, I'm trying to locate the question."}
{"speaker": "Participant", "text": "Yeah, no worries."}
{"speaker": "Interviewer", "text": "You did say that you didn't really trust LLMs, so you needed human verification."}
{"speaker": "Participant", "text": "Oh."}
{"speaker": "Interviewer", "text": "And then, so, do you feel like trust towards LLMs, or like, the reliability is a kind of an issue?"}
{"speaker": "Participant", "text": "Okay, I would say that would be the issue. So, let me tell you, the reason why I really want to do this process is I don't trust the literature review generated by some LLM agent out there."}
{"speaker": "Interviewer", "text": "Mm."}
{"speaker": "Participant", "text": "The reason why is, like, because I do research previously, so I'm somewhat aware of the literature review. And when I try to work on another project, like a different topic, I am entirely new to it—a new person. So when I'm trying, the very first thing that I need to do is know the literature review. So I try to ask Perplexity, Grok, and OpenAI to generate a taxonomy. I just type it like, \"I want to work on this one, can you generate a taxonomy or generate a literature review for me?\" So it generates something, but because I'm not an expert and I haven't worked on that before, I cannot verify it. So that's the first thing. So how do I verify that? I map back to what I've already done, where I have some sense of the literature review, and I ask them to generate a literature review for me to verify. And a lot of times I feel like ChatGPT, Perplexity, and OpenAI—that's why they're very good at communication, at coding, or something like that. But in terms of literature review, they miss a lot of important works out there. Also, a lot of the time, especially Perplexity—I would say Perplexity is good. It's not generally hallucinating. But it has very high precision. However, the recall is very low. On the other hand, OpenAI can generate very good writing for a literature review. But I'm not very sure about the very new version. But for the O1 version that I used before, I think it generated a lot of hallucinated papers, which is a very serious concern."}
{"speaker": "Interviewer", "text": "I see."}
{"speaker": "Participant", "text": "Yep."}
{"speaker": "Interviewer", "text": "Okay, so besides this project, for any sort of multi-agent system, do you feel like human verification is always needed, or can there be some transparency signals or design that would help with this?"}
{"speaker": "Participant", "text": "I would say at the end of the day, human verification is still the most important step when we work with the LLM. But I think, as you will see in the future, or maybe now, I think LLMs will be soon, like, better than humans. Okay. At least for some tasks, maybe like math or something. I don't think a normal human, like an average human, can match an LLM in terms of math problems or coding problems. So it's very, very hard for a human to verify the LLM's output in the future. So I would say we need a different approach. But I still believe human verification is a need. But we might need to have a better pipeline for the human verification instead of just having human verification from scratch. So one of the examples that I know people are doing right now, and I strongly believe that will be the future, is through debate. So instead of assuming that evaluating is easier than generation. What you have done here is, instead of having a human generate the verification from scratch based on, say, what you asked the LLM."}
{"speaker": "Interviewer", "text": "To generate the…"}
{"speaker": "Participant", "text": "To generate a math answer from a question, instead of just asking the human to start from scratch."}
{"speaker": "Interviewer", "text": "Read from."}
{"speaker": "Participant", "text": "And verify that. So rather, you will ask the outline to generate an answer, and you will add another agent, like multiple agents, to debate about that. For one, we'll be trying to debate that this answer is correct. Another agent will also debate that this is going to be the wrong answer. And due to the debate process, they will only highlight the most important components in the answer so that it can lean towards the correct answer or the incorrect answer. So that humans, instead of reading from scratch through the whole answer, they can just only look at the important parts and verify that later."}
{"speaker": "Interviewer", "text": "I see. Yeah, I actually had another interviewee who specifically worked on something you just described. He had this debate multi-agent system where each agent is also assigned to, or they also ask each agent to output some confidence for, I think, each of their arguments."}
{"speaker": "Participant", "text": "Okay."}
{"speaker": "Interviewer", "text": "And then they showed that, in the end, it had higher accuracy than just having a single LLM."}
{"speaker": "Participant", "text": "Yep."}
{"speaker": "Interviewer", "text": "Yeah, okay. I have two more questions, which are a little bit more broad and general, so you don't have to think about this project specifically. And okay, first, what do you think the biggest challenge is in developing a multi-agent system? I know it's a big question, so I guess just talk about anything that's in your head. Like, the biggest challenge in developing."}
{"speaker": "Participant", "text": "The biggest challenge in developing it is really very broad."}
{"speaker": "Interviewer", "text": "I guess I can give some examples to make it less broad. So, for example, I have... I had other interviewees saying that system prompts are something that they really struggled with. I had this one person who worked at... She did an internship at Amazon, and then the internship was three months long, right? And she was developing this multi-agent system from scratch. The structure or the architecture itself wasn't too complicated—like, she had a few agents—but she really struggled with writing system prompts. I think she ended up spending two months just trying to refine her system prompts because at first, she was just writing the system from scratch. And then she had to run the system and wait for some unexpected behavior, and if that happened, she would have to manually include that into the system herself. And so it's just a lot of trial and error, which took a lot of time. I think other people also mentioned the communication between agents, like defining inputs and outputs, and how to make the entire orchestration work."}
{"speaker": "Participant", "text": "Oh, okay, so it's really about the tech. Okay, if that is the case, then I think I have an answer in my head now. This might not be verified, because I haven't really faced a concrete challenge on that. But just from my background, because I previously worked on the robustness of the model, and I think I also see that quite a lot of the time when I use the LLM. So I would say one of the challenges is about the safety of the..."}
{"speaker": "Interviewer", "text": "Of the airline."}
{"speaker": "Participant", "text": "Communication. So, what I mean by safety here is that I know there are some people trying to develop multi-agents. Specifically, they're trying to develop multi-agents for web agents. So when we're done with that, the problem is the behavior of the agent system. We cannot control the behavior of the agent system. For example, if there is a web shop agent with multiple agents that we will interact with, each agent will be responsible for one store in the webshop. So there are multiple webshops, right?"}
{"speaker": "Interviewer", "text": "Yep."}
{"speaker": "Participant", "text": "The question is, like, when you're trying to—so, as a human, if you have multiple options for use, but there are multiple stores in the workshop, you will chat with one system, and then based on the conversation, will you decide to move to another system."}
{"speaker": "Interviewer", "text": "Yeah."}
{"speaker": "Participant", "text": "Yeah, so that will be the case for how humans will interact with that. So right now, what we want is for another agent to have done that for us. But the question is, like, it's going to... it can raise the bias. Because I actually have an interview with Visa for an internship. So in that interview, like, we know that Visa has an issue about bias in communication. What I mean by the bias is, like, instead of when you chat with one agent, instead of it referring to a better store and you working with the agent in that store, so it just has a loop where you chat with that chatbot and it's the next suggested agent. This rule, just that one. So it's going to be a loop, yeah. So I think one of the ways that Visa solved this—I just got this during my interview—is that they were trying to generate some... they would introduce some randomness. Instead of just 100% believing in the next decision of the LLM, they just introduce some randomness so that it can refer to another agent during the interaction."}
{"speaker": "Interviewer", "text": "But can't they just be hard-engineered to, like, have the agent not be able to choose..."}
{"speaker": "Participant", "text": "Yeah."}
{"speaker": "Interviewer", "text": "I'm ready to transcribe and correct text about LLM-based multi-agent systems. However, the text you've provided only contains \"self.\" which appears to be incomplete or a formatting error.\n\nCould you please provide the actual interview text that needs to be corrected?"}
{"speaker": "Participant", "text": "Yeah, I think that is one of the answers that I already answered during the interview, and they say, like, because it's going to have a lot of things there, especially, like, prompt injection. So it still happens, even though we really designed a very careful system prompt."}
{"speaker": "Interviewer", "text": "I see."}
{"speaker": "Participant", "text": "Yep. So that's the first thing. Basically, the bias during communication is the first thing that I already know in Visa. Another one is very relevant to what I have. Like, my research interest is about prompt injection."}
{"speaker": "Interviewer", "text": "I see."}
{"speaker": "Participant", "text": "The problem is prompt injection. Like, we know that it's a very challenging vulnerability of the agent model. And I feel like the more agents that we have—so previously, we only had one LLM, one agent, and just prompt injection there, and we can prevent that. But the more you know that when we have multiple agents, we will have communication. And the communication is not just between the agents, but rather we still have all the external data during the interaction. So the surface for prompt injection will be increased a lot. So that it will be another issue compared to the single agent system before."}
{"speaker": "Interviewer", "text": "Yeah, yeah, I guess I'm not too familiar with safety, but since there are more sources, there are, like, more places to attack, kind of. Yeah, I see."}
{"speaker": "Participant", "text": "Yep. Actually, in my group we have one paper called, like, to compare the vulnerabilities between a single agent and a multi-agent system."}
{"speaker": "Interviewer", "text": "Especially when comparing LLMs."}
{"speaker": "Participant", "text": "And the LLM deployed in the web environment. And we showed that the web agents will have more than 40% vulnerability in the prompt injection compared to the single LLM agent alone."}
{"speaker": "Interviewer", "text": "I see."}
{"speaker": "Participant", "text": "Wow."}
{"speaker": "Interviewer", "text": "Could you share the paper?"}
{"speaker": "Participant", "text": "Yeah, sure. Let me share two things with you. Just give me a few minutes."}
{"speaker": "Interviewer", "text": "Also, that was the last question I had, so I can… stick to your time. I'll stop the recording."}
{"speaker": "Participant", "text": "Yeah, thanks."}
