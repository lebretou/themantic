{"speaker": "Interviewer", "text": "Okay, let's go ahead and start. So I'll start with a pretty abstract question. What is your definition of a multi-agent system? And when you hear the term multi-agent system, what does it mean to you? How would you define it?"}
{"speaker": "Participant", "text": "So, in my understanding, a multi-agent system is a place where it involves different agents. But agents are not anymore just a tool. It's like an LLM-based service provider, which has certain capabilities and a definition within the system. So, the definition should be in a way that it's compatible with other agents in the multi-agent system, and they can collaborate in a meaningful way."}
{"speaker": "Interviewer", "text": "I see. And could you tell me about one or two representative projects that you have worked on in the past that used multi-agent systems?"}
{"speaker": "Participant", "text": "Yeah, so I'll take one example in Amazon Rufus. I've done like multiple multi-agent systems. One example is in Amazon Rufus, we built a multi-agent system to provide product recommendations. Right. Product recommendations, it looks simple, but it comes with different pieces where you need to retrieve the information from a database, making sure the products are legitimate, and then also you need to find some deals. For example, when customers ask, \"Can you show me some products which are on sale?\" you should be able to find it. So there are different components inside of it. Also, for sure, they come with customer service pieces as well, because when customers say, \"Oh, well, the product I bought this time is not great, can you help me return this one, or can you help me update the mailing address?\" So for that, you need to be able to handle it. So I can tell you at a high level, but I don't think I'm supposed to go into too much detail because..."}
{"speaker": "Interviewer", "text": "Yeah."}
{"speaker": "Participant", "text": "Business, regulation, yeah. So in the entire system, we have like four different agents, which is the initial design. And then one of the agents is… because, you know, in business, like a lot of things you need to be very mindful of, especially legal-related. For example, when customers ask me, \"Can you recommend some stock that I can investigate?\" It's definitely outside of the scope or responsibility of a shopping agent. That's supposed to answer you, so you need to have a fallback agent, which is used to deal with those questions you are not supposed to answer. And that one has like a very heavy business requirements and guardrail system you need to process, so you definitely have one of them. And there are three other agents. I wouldn't go into too much detail, but they like collaborate together to get products. Also, there are contexts we'll be sharing with each other within the collaboration process. There are definitely some more advanced ways to deal with it. For example, you only choose to share specific context that is relevant to other agents' scope, but the thing is, like, something we're exploring right now, yeah."}
{"speaker": "Interviewer", "text": "Okay. So, what was your main responsibility in the project that you just described? Did you work on the entire thing from scratch, or did you focus on a specific part, for example, like writing system prompts, or working on the higher-level architecture?"}
{"speaker": "Participant", "text": "I was everywhere, so at the beginning, it was like two or three scientists who started with it. One of them also came from memory, other than me."}
{"speaker": "Interviewer", "text": "Sure."}
{"speaker": "Participant", "text": "So we got started at the beginning, we built a demo, and after the demo, like, in business, the way it works is you build a demo and prove it works, and if leaders think it's a place where it's worth investigating, then more people will flood in and help you ship to production. So, at the beginning, it was like 3 of us to design everything. It's more of a local Python proof of concept. But later on, when it gets funded, our product managers will come to help with designing, and our UX designers also come to help improve the customer experience. And then engineers come to, for example, one of the biggest problems we were facing is the latency. Because it injects a lot of product information, and also the prompt is super big, so you need to do a lot of prompt optimizations, where what they did is prompt caching and also context summarization. So it's a lot of engineering challenges as well. So it's a collaboration."}
{"speaker": "Interviewer", "text": "Okay, and so currently you have just shown the prototype; you haven't really gone into production."}
{"speaker": "Participant", "text": "Oh, it's already in production, 100%."}
{"speaker": "Interviewer", "text": "It's in production, okay. And then, did you use any framework, like LangChain, LangGraph, or some Amazon tool?"}
{"speaker": "Participant", "text": "We use Amazon's internal tool called Agent Core."}
{"speaker": "Interviewer", "text": "Agent Core. Yeah, Agent Core, yeah, because it is."}
{"speaker": "Participant", "text": "It's Amazon's own service, so it makes more sense to use it directly rather than other MCP services."}
{"speaker": "Interviewer", "text": "Okay, and could you talk more about the prompt optimization process? Was it a lot of trial and error, or was there a systematic way of doing that?"}
{"speaker": "Participant", "text": "So, prompt optimization, in a sense, I think it's a very broad area. I think the biggest technique they use is prompt caching. It's a pure engineering problem where the agent—when the agent's LLM is being called, the prompting can be cached, so they avoid overcalling. But actually, it wasn't just prompt optimization. I actually did both. So, we did prompt optimization, and we also did context optimization. It depends on what your question is, like, which part are you asking about?"}
{"speaker": "Interviewer", "text": "You said that you had, like, a super long prompt,"}
{"speaker": "Participant", "text": "Yeah, yeah."}
{"speaker": "Interviewer", "text": "In the end, so I was wondering about how… what steps did you take to eventually get to that super long prompt? Was it super long in the very beginning, or did you…"}
{"speaker": "Participant", "text": "Yeah, it's like an incremental thing. In business, you need to incrementally add different customer experiences. For example, product comparison, product recommendation—a lot of experiences you need to add on top of it. So it's like an incremental process. At the beginning, it can be very short. Later on, when more and more collaborators come in, they're saying, \"Oh, can you also support our service or feature?\" So that's the process—how the prompt is getting longer and longer. And it definitely comes with a lot of conflicts that you need to pay attention to and solve."}
{"speaker": "Interviewer", "text": "Okay. Do you mind if I ask, like, what is the overall structure of the prompt? Are there some things that you define, for example, like what kind of input will the agent get? What kind of output should the agent have, or how should it communicate with other agents? And some context, for sure."}
{"speaker": "Participant", "text": "I don't think I can get into super detail, but the way I can say it is it depends on the prompt and the multi-agent system design at the very beginning. So you need to, for example, have one agent ingest another agent's output."}
{"speaker": "Interviewer", "text": "Right. So, you need to make sure that you tell this agent, like, you need to have a router, right? An orchestrator. So, with the orchestrator, you need to understand what the workflow is and also what the input and output from each agent are."}
{"speaker": "Participant", "text": "And then within the sub-agent, you also need to tell the sub-agent, \"Hey, you need to make sure your output format can be ingested by another agent, which is taking your output as input.\" Yeah."}
{"speaker": "Interviewer", "text": "That's it. So for this project, did you initially... so, like, given the goal, given the task that you want to solve or the problem that you want to solve, did you initially think that a multi-agent system is the best solution, or did you try with just a single agent, single LLM?"}
{"speaker": "Participant", "text": "Yeah, this is a very good question. So, at the very beginning, because we were a bunch of folks who just got started to work within one or two years. We are not super senior, so we didn't know that there would be so many collaborations in the future. So at the very beginning, we actually started with a single agent."}
{"speaker": "Interviewer", "text": "But the thing is, like, when the…"}
{"speaker": "Participant", "text": "The problem is getting larger and larger, and your prompt can easily exceed 1,000 lines."}
{"speaker": "Interviewer", "text": "At that point."}
{"speaker": "Participant", "text": "You start to realize it's not going to last. It's not viable because, like, the cost is very heavy. The latency is, like, super long. Sorry, one second. So, Friday, November 20th… Yeah, so the latency was super long, and also it started to hallucinate because it's injected."}
{"speaker": "Interviewer", "text": "Sorry about that."}
{"speaker": "Participant", "text": "Too much information, and they're following too many guidelines. So, this is the point. It becomes like a design problem—a design problem that you start to understand. Because at Amazon, like, it's a business company, there are a lot of policies you need to follow."}
{"speaker": "Interviewer", "text": "And also, there are a lot of components, APIs, and databases."}
{"speaker": "Participant", "text": "Tools you can use. So it increased the difficulty of the multi-agent. So as time goes by, I like that the multi-agent is a very natural way because it helps with scaling up and increases the speed of onboarding different customer experiences. I see. Okay."}
{"speaker": "Interviewer", "text": "And then, so you said that at first you tried a single LLM, and then it naturally became a multi-agent system."}
{"speaker": "Participant", "text": "Boom."}
{"speaker": "Interviewer", "text": "For the high-level structure, like what agents are there going to be, what is the role of each agent—did the high-level structure of your system also change over time?"}
{"speaker": "Participant", "text": "It changes. So, yeah, there are a lot of non-technical discussions. Because in the company, everybody wants to make sure their product is successful, so the service provider or customer experience provider from other teams, they want to put their service into Rufus so that they can benefit from the Rufus traffic. So people are fighting to send things in and out. So every time, like, this sort of non-technical discussion happens, like, an independent contributor doesn't care."}
{"speaker": "Interviewer", "text": "Excellent."}
{"speaker": "Participant", "text": "But every time there's updates, we need to update the system. It's a challenging thing, but we built something. So there is one paper from last year's ICML. I say ICML, I forgot. It's like an oral paper. An oral paper, in a sense, it's like the best paper, sort of thing. So it's called AFLO. I can share you the paper link later on. It's like a workflow. Automatic workflow generation, where you can have reward modeling. For example, when you delete or increase some feature of one of the sub-agents, and then use and build some pipeline, get inspired from AFLO, and use it to let AFLO itself automatically update each prompt scope. Each prompt has our responsibility, so that the updated multi-agent system can still function and they're compatible with each other. Yeah, so this is some research idea we tried. It somehow worked, but there's no guarantee that it's the best way to automate the entire process, because…"}
{"speaker": "Interviewer", "text": "So, okay. I'm understanding this correctly—it's generating prompts, generating even agents on the fly."}
{"speaker": "Participant", "text": "It's a modification, it's not generating."}
{"speaker": "Interviewer", "text": "Oh, it's a modification. Okay. Yeah. It's modifying the current…"}
{"speaker": "Participant", "text": "existing agents' scope? Because if you think about agents, essentially it's like a prompt that connects those different tools and APIs, if necessary."}
{"speaker": "Interviewer", "text": "Okay."}
{"speaker": "Participant", "text": "Yeah."}
{"speaker": "Interviewer", "text": "So, again, I guess the things that you experimented with the most for that project were the context management and then the system prompts."}
{"speaker": "Participant", "text": "So, it was the tool calling. Especially, we started this project with some—like, I won't be able to tell you the model—but it's like a very early version of some model."}
{"speaker": "Interviewer", "text": "Oh."}
{"speaker": "Participant", "text": "We're trying a different type of model from different companies. At the very beginning, it's very challenging to have the two calling to be accurate. So it became a challenge. But later on, when the model is getting better and better, because so many models got released this year."}
{"speaker": "Interviewer", "text": "Yeah."}
{"speaker": "Participant", "text": "This problem, like, better. So, tool calling and the RAC system, Rich, are you familiar with RAC?"}
{"speaker": "Interviewer", "text": "Yeah."}
{"speaker": "Participant", "text": "Yeah, so the tool calling mechanism was the very first challenge we had. But then the summarization optimization, it's a problem that we'll encounter later on, yeah."}
{"speaker": "Interviewer", "text": "Okay. So, could you also tell me how you monitored your system? When you have a prototype and then you want to make sure that it's working correctly—each agent is doing the job that they should be doing—and then the output is also correct?"}
{"speaker": "Participant", "text": "Yeah, so basically there's no way to do this monitoring because it's interactive—like, people interact with it, and everything is on the fly. So there's no way to be aware of how good my system is or not. But you can still do sanity checks, right? Before launching, you need to have some offline evaluation to make sure what you're getting is correct before you're sending it to production. And this offline evaluation uses data, like, different employee test cases to work on. It's like making sure it's as close as possible to actual customer input. So we're using the numbers generated by this offline dataset as a proxy for how good it is."}
{"speaker": "Interviewer", "text": "Okay. So I guess that's just, like, more evaluation, making sure the system is behaving the way that it should."}
{"speaker": "Participant", "text": "Evaluation is a very important problem, but it's quite challenging, yeah."}
{"speaker": "Interviewer", "text": "Okay. Yeah, so I was actually asking more about, like, when you were developing—was there any, so like, how did you check, let's say, the intermediate output of each agent?"}
{"speaker": "Participant", "text": "This is more of an engineering problem. Like, the output of each agent will be put into some service, and this service will be connected to the next one. Sometimes errors happen, but the thing is, the models are trying to be better at following the output syntax, for example, JSON or MD. Yeah, like, different types of formats. So it's really hard to define if it's correct or incorrect, as long as the output format aligns with whatever the next agent expects."}
{"speaker": "Interviewer", "text": "What else?"}
{"speaker": "Participant", "text": "It will work. It's just a matter of how good it works or how bad it works. So there's basically no errors you can detect. If there's an error, it will be sent to retry the next time, and the next turn, the error problem wouldn't be there."}
{"speaker": "Interviewer", "text": "Okay. So what was, like, the most common error or bug that you ran into? I think you mentioned tool calling and hallucinating at the beginning."}
{"speaker": "Participant", "text": "So I would still say it's a hallucination, yeah."}
{"speaker": "Interviewer", "text": "Hallucination."}
{"speaker": "Participant", "text": "Yeah, it's still the hallucination. It's not even a bug. It wouldn't show you any bug."}
{"speaker": "Interviewer", "text": "It's not a bug."}
{"speaker": "Participant", "text": "It's just that you realize what it says, like, sometimes it's changing, especially under pressure. For example, if you ask a question that it does not know, it will say, \"Oh, I don't know\" at the beginning, which is normal. But if you keep applying pressure, saying you must know, you're a chatbot, you're supposed to be so smart, you must tell me this and that. When customers repeat this sort of high-pressure query, it starts to hallucinate."}
{"speaker": "Interviewer", "text": "So do you think that this hallucination problem can definitely be reduced by having a stronger model, but it doesn't go away entirely? So from an engineering perspective, what are some actions that you can take to try to minimize hallucination as much as possible?"}
{"speaker": "Participant", "text": "Yeah, like, it really depends on how you view this problem. I think it can be improved, I would say. I don't think it can be completely solved. It can be improved in two ways. For example, if you have a small model or an open source model, you can fine-tune it, like with DPO or GIPO, to make sure it has stronger regulation for the actuality. It will help, but it wouldn't help you solve the problem 100%. The second thing is you can have some basic guardrails. For example, prevent the model from saying things that violate legal requirements, right? But for hallucination, I think guardrails can help you, but guardrails wouldn't be able to solve 100% of the problem. This is very much the biggest pain point, because if you think about a multi-agent system, it's actually a cascaded process. Even though your model has like 99% accuracy, it looks great, but if you cascade the error rate at the end, it's like 99 to the power of 5 or 6. It won't fail, yeah."}
{"speaker": "Interviewer", "text": "Yeah, right, okay. And then so you also said that at industry you first have, like, a working prototype, you present it to your manager. I wanted to ask, so at what point did you think, okay, this prototype is pretty ready, this pipeline is pretty reliable, pretty consistent, I think it's okay now for me to share with my manager? Or even for production, like, how do you eventually decide that this pipeline or this multi-agent system can actually be ready?"}
{"speaker": "Participant", "text": "Actually, this is not determined by me. So, like, prototype—it's rather like I have more control over it because with a prototype, I can think through things. As long as the functions I envision, like the 3 or 4 most important customer experience features, if I'm able to deliver that, I was confident to talk to my manager and my director, which is fine."}
{"speaker": "Interviewer", "text": "Okay."}
{"speaker": "Participant", "text": "But even launching to production, it's a decision that the director or VP needs to make, so that's sort of out of my control."}
{"speaker": "Interviewer", "text": "I see. Okay, I have another question. For developing the multi-agent system, do you think there are any features or interface that could speed up that process? Or what are some features that would be nice to have?"}
{"speaker": "Participant", "text": "Yeah, this is a very good question. If you look back into the interactive machine learning domain, which is the problem space, I briefly touched it a little bit back in my PhD. So, later on, in deep learning, you need to have different layers, like fully connected layers, dense layers, whatever, pooling layers. And there have been some people who did interactive machine learning, where you have a UI with different layers that come with LEGO blocks. You can drag them into the space and then use a line to connect them, like specify what the input and output size are, for example, a 16 pooling layer will be connected to, like, 64 densely or whatever. So that's the interactive way to do it. And then the machine will use translation to translate this sort of idea into implementations and help you to do the implementation. So building a machine learning, deep learning model became an interactive way. By the way, did you record this meeting?"}
{"speaker": "Interviewer", "text": "Yeah, yeah."}
{"speaker": "Participant", "text": "Okay, sounds good. Yeah, so that's actually a good idea for multi-agent, because multi-agent basically comes with three things. First thing is multi-agent design—structure design, right? And then, other than structure design, within the Orchestrator, you need to let the Orchestrator know what are the other sub-agents with their definitions. That's the first thing. Second thing, for the sub-agent, you need to specify which agent can access what kind of external database, because RAG is a very important thing."}
{"speaker": "Interviewer", "text": "Right now, because of the model, there is a lot of domain knowledge that the model was not able to access."}
{"speaker": "Participant", "text": "For example, if you're at UMD, let's say you want to access your UMD internal database for student and employee information. That's something that the LLM doesn't know, because it's not pre-trained on that. So, you need to... the second LEGO block is you need to let each sub-agent know what kind of tools it can access, and give the tool definition. What kind of APIs? By API, you can actually call it a tool, because essentially it's tool calling, right? So, for each tool, what is each tool's definition? What is the exemplary input and output?"}
{"speaker": "Interviewer", "text": "For each one, this is something you need to have."}
{"speaker": "Participant", "text": "And third thing, if you put everything into integrity, you need to make sure that the MCP and the connections between sub-agents and communication between sub-agents to the orchestrator—yeah, this is something that I envision we can use, like some sort of interactive UI to build this sort of system. I don't think there are too challenging things. But it really depends on the complexity, you know? Yeah. But it definitely improved from a research perspective. It improves the accessibility, it improves the generalizability, and making sure that people who are not that technical can also have access to model aging. I don't see… so I was thinking about this research idea a while back, and I did some literature review where nobody was doing it."}
{"speaker": "Interviewer", "text": "But it's a very engineering-heavy project."}
{"speaker": "Participant", "text": "Being new at Amazon doesn't allow me to have a lot of bandwidth to work on it, yeah."}
{"speaker": "Interviewer", "text": "I think there actually are some interfaces like that."}
{"speaker": "Participant", "text": "Oops."}
{"speaker": "Interviewer", "text": "I can share a screen real quick. I know people have been trying it. There's also a couple of tools available out there, but I think the problem with those tools is that they don't… hmm… So this is one of the tools. They have…"}
{"speaker": "Participant", "text": "It's already there."}
{"speaker": "Interviewer", "text": "Started there, yeah. They have, like, agents. You can also define, like, the tools. Also, RAG database, I think."}
{"speaker": "Participant", "text": "I see, yes. Okay, yeah."}
{"speaker": "Interviewer", "text": "But from what I've gathered so far, from the interviews I've done, I feel like not many people are using it. I'm not sure why."}
{"speaker": "Participant", "text": "So, yeah, I can tell you the reason. I'll use one example: prompt tuning. Very easy and straightforward thing, right?"}
{"speaker": "Interviewer", "text": "Yeah."}
{"speaker": "Participant", "text": "At the very beginning, like, the engineers and PMs at Amazon, they're not there to tune a prompt. They think it's a very technical thing. So they send it to scientists, but scientists don't want to tune a prompt. It's like there's no sense for a code there. So there is a misunderstanding, or a learning curve, where people start to think, \"Oh, aging\"—like, super complicated, like a lot of things need to be done. Any mistake made can be easily propagated. So I'm viewing this as one of the reasons why not a lot of people are using it, because, first of all, people who work on agents, engineers and scientists, they don't need this tool. They know how to make…"}
{"speaker": "Interviewer", "text": "You can just use LendChain."}
{"speaker": "Participant", "text": "Yeah, nothing is needed by PMs, PMs and designers, UX designers. But those people are still intimidated by this, like, oh, this complicated thing is very technical, I cannot use it."}
{"speaker": "Interviewer", "text": "And yeah, if you're just designing or envisioning, then you don't need something that actually works. You can just use PowerPoint, right? Yeah, I also had the same thought. I think it's just kind of awkward. I feel like it's not easy enough for people who don't have any experience to build."}
{"speaker": "Participant", "text": "A working pipeline."}
{"speaker": "Interviewer", "text": "But it's also, like, kind of a toy to actual engineers."}
{"speaker": "Participant", "text": "Yeah, I agree."}
{"speaker": "Interviewer", "text": "Okay. And then so we're trying to locate the question. Okay, and then I'm going to go back to being a little bit more general. So, for what type of problems do you think a multi-agent system works the best? I think you already talked about it a little bit."}
{"speaker": "Participant", "text": "I see. So, hmm… to be honest, I still don't believe multi-agent systems can be that reliable for very complicated problems. For example, a lot of agents need to send information back and forth. It's an iteration, so I don't think it's the best use case. I believe it's a use case where, so abstract my idea here. I believe it's quite complicated work that can be done by a single agent, but when it's done by a single agent, you need to take some trade-offs—either hallucination, latency, or usability. So I think multi-agent systems are a way to scale up. But functionality-wise, I still think the multi-agent system is just as good as a single agent system if you want it to do something. It's not like a superhero or superpower that could be advanced in a lot of performance-wise. It's just a way to scale up and to split the workload so that the model will hallucinate less."}
{"speaker": "Interviewer", "text": "I see. Okay, that was actually the last question from what I've prepared, but before we end, I actually have a very specific question. So for the system prompts, when you're developing, how did you edit it? Did you keep it in the code editor as a super long string, or did you have it somewhere else?"}
{"speaker": "Participant", "text": "Oh no, it's like the Python console, like VS Code, Python file. It's like prompt equals comma. Like, we're getting a comma, it's like a lot of prompts, like…"}
{"speaker": "Interviewer", "text": "So it's just a super long string in Python. Okay. So the reason why I ask that is because I have interviewed probably like 7 or 8 people now, and one of the problems—the most common problem is probably writing system prompts. And so my mentor and I are thinking about doing something about it. I actually interviewed another person who interned at Amazon, and she had this 3-month internship to build some sort of multi-agent system, but she spent almost 2 months just writing system prompts. It was very tedious because she was just traveling and erring. She would have a prompt, and then she would run the system, just wait for unexpected behavior to occur, and then manually include that unexpected behavior into the prompt. Like, \"oh, do not do this, do not do that.\" But then they eventually turned into this framework. I don't know if you have heard..."}
{"speaker": "Participant", "text": "I'm ready to help transcribe and correct interview text about LLM-based multi-agent systems. However, the text you've provided only contains \"Auto prompt.\" which appears to be incomplete or a placeholder.\n\nPlease provide the actual transcribed interview text that needs to be corrected, and I'll apply the corrections according to your guidelines."}
{"speaker": "Interviewer", "text": "It's a framework, and it uses some prompt optimization technique. So basically, instead of writing the system prompt yourself, you just define the input and output, and you provide some in-context learning examples, and then it tries to determine the best route for that set of input and output."}
{"speaker": "Participant", "text": "Yeah, I think, for example, Claude itself has, like, prompt optimization."}
{"speaker": "Interviewer", "text": "Yeah."}
{"speaker": "Participant", "text": "Yeah."}
{"speaker": "Interviewer", "text": "So we're currently brainstorming, but we feel like we wanted to do something, maybe like a visual… something visual about editing your prompt. Because, at least I think that system prompts for multi-agent systems usually follow some structure. They always have some inputs and outputs, and like you said, like tool definitions, also the context. So we're thinking maybe do something about that. For example, like visualize the structure—inputs and outputs, things like that."}
{"speaker": "Participant", "text": "Yeah, so I think there's something that can be done here. It's like an interpretation tool, in the sense that, for example, it's not about building a multi-agent system, it's more so about understanding a multi-agent system, and debugging and problem-shooting. For example, if I gave you a super long multi-agent system, can you give me a tool so that this tool can help me visualize in a diagram that shows how the workflow of this multi-agent system functions? And what is the input-output of this multi-agent system? And then tell me, what is the gap? Gap, meaning there is some functionality that the orchestrator thinks the sub-agent can do, but the sub-agent itself, or the collaboration itself, has some gap. So this tool can help to diagnose the problem and help for future refinement. It's also a tool that helps with interpretation and transparency, improving the multi-agent transparency. It will be useful, yeah."}
{"speaker": "Interviewer", "text": "So, I'm not sure if this is, but I think AWS…"}
{"speaker": "Participant", "text": "Already has it."}
{"speaker": "Interviewer", "text": "Already has it. It's called AWS Observability."}
{"speaker": "Participant", "text": "Observability in multi-agent systems."}
{"speaker": "Interviewer", "text": "I'm ready to help correct the transcribed text, but it appears only the word \"Aging…\" was provided. Could you please share the full interview transcript that needs to be corrected?"}
{"speaker": "Participant", "text": "Really?"}
{"speaker": "Interviewer", "text": "I'm not sure if it's…"}
{"speaker": "Participant", "text": "Skywalk Aging Corps of the Office of Observability. It's called AWS Bedrock Agentic Core Observability."}
{"speaker": "Interviewer", "text": "Yeah. Provides detailed visualization of the aging workflow."}
{"speaker": "Participant", "text": "Yeah, it's already provided."}
{"speaker": "Interviewer", "text": "The reason I know this is I took the AWS AI Practitioner certificate."}
{"speaker": "Participant", "text": "Okay, yeah. The aging world is, like, developing too fast."}
{"speaker": "Interviewer", "text": "Yeah, I think it's about time. I'll stop recording."}
