{"speaker": "Interviewer", "text": "Okay, so I'll start off with a pretty abstract question—a pretty big question. So, what is your definition of a multi-agent system? And what does it mean to you? So, like, how do you see it as different from a single agent or a single LLM?"}
{"speaker": "Participant", "text": "So, if we have energetic systems with multi-agents, and usually these days we put it into the space of LLMs or multi-modal language models that can do all sorts of tasks, then these systems have aspects like autonomy. To a certain degree, they should be deciding by themselves. They are dynamic in a way that they can handle all sorts of tasks. They're not restricted, not hard-coded, not hardwired to a certain set of problems, but they should be problem-solving. We should be able to do problem-solving tasks by themselves without having a predefined set of problems to work on. Then there's much more to it. There's a very big topic all about communication—in a way, how these agents communicate with the user, but also with each other. How do you structure the communication? This leads to questions on how to structure the software. Yeah, what kind of flows do you want to integrate? And then there's much more to it. Multi-agent systems are usually also designed in a way that they are specialized, in a way. So there are certain agents focusing on certain tasks. And this is very interesting these days when there are lots of models coming up with certain specialities. So there are models that are good at reading tables, then there are other models that you use for image generation, and this way you can have very complex tasks being distributed among agents. But also, it comes with a lot of problems and a lot of questions. I try to be as vague as possible because there are people that say, look, we should not have them too autonomous, we should have them automating things and still have the user decide because as of now there are questions on who is taking responsibility. Of course, in the end it has to be a human, but the question is, like, how do we design a system? How do we abstract it in a way to have the best separation of concerns, to have the best abstraction level where the user can still stay in the loop? Yes, so this is how I would try to roughly describe it. This is not necessarily a definition, but I hope it helps."}
{"speaker": "Interviewer", "text": "No, it's fine, it's just your thoughts. Okay, then I'll move on. Could you tell me about one or two projects that have used multi-agent systems that you worked on in the past?"}
{"speaker": "Participant", "text": "Yes. I worked on a system with multi-agents where I was generating research reports. Let me think... We were using a database on a technical site. Right. And then we were integrating it with a Python server running LangGraph, and this LangGraph was orchestrating different agents for different tasks."}
{"speaker": "Interviewer", "text": "Yeah."}
{"speaker": "Participant", "text": "And then we had, in this case, organized them in a kind of deterministic way. And if you want, I can go a little bit into the technical details on what we decided and how we implemented it, if that's interesting."}
{"speaker": "Interviewer", "text": "Yeah, please go ahead. Also, like, what are the agents? What are their roles?"}
{"speaker": "Participant", "text": "Yes, okay. So in the end, it came from the idea that we want to have a very rough user's description of a certain problem and result in a sophisticated report."}
{"speaker": "Interviewer", "text": "But at the same time, it's also manageable by a user because."}
{"speaker": "Participant", "text": "What often happens is you put something in. If you just use an LLM for a certain type of interaction, you say, \"Hey, look, I have these questions and give me the response,\" then what comes out is a very sophisticated answer, but it's not very… It's very stateful, in a way. So we wanted to have all the information, all the assets, the artifacts that are being generated, to be persisted separately in a way that we can manage it, so that we can say for a certain artifact that the user can still manipulate certain things without having to re-run the whole conversation. How did we do this, or what is it about? So, consider our report generation pipeline, where we had the initial query, which was then going through APIs for initial research on existing literature. In this case, I can give you a very precise description. It was a Semantic Scholar API. And this was something which was very helpful for this one agent, which was overall having the role. We didn't talk so much in roles; we broke it down more into tasks and actions. And in this particular case, it was more about getting the related work. It was doing it by having this query and translating this human user query into a search string, because often cases you have databases that have a certain syntax in order for their querying, and agents are very helpful in translating, in general, but also translating human language into a search query string for this Semantic Scholar database. We were also connecting then some other databases. We use different agents that were, like, specifically strong with these certain respective system prompts in the end with the respective documentation on the database's query string, which is very easy to set up."}
{"speaker": "Interviewer", "text": "Yeah. This was a very easy, small agent, which was…"}
{"speaker": "Participant", "text": "Basically taking over this search. And then, when this search was going on, we persisted all this data that was coming out into our database to have our agentic system being kind of operational. So when it breaks down at a certain point, you know where it stopped and where we want to go on. And we kind of hardwired it into our workflow in a way that we persisted. Like, we were tracking all the agents with tasks and actions. So we said this is our overall task, and this consists of different actions, and each action is persisted into the database, including state—pending. Like, if it's pending, if it's active, or whatever—also with the results. So we knew at any time what was happening in terms of the process flow. And then we were going on. After this first initial screening, we had an overall agent which was giving an overall strategy for the report, I would say. So it was more of a strategic approach, where this one agent said, \"Look, based on this information, we want to have a report which goes a little bit in this or that direction.\" It was kind of broad. But then we stopped, and then we had a different agent and gave him the task in this particular direction with this set of background information: \"Please generate a set of stories that we should...\" Like, it was going a little bit more on a layer below. It was going more operational, more tactical, let's say. To make more precise storylines for this report. And then finally, we were going one step further to have actual data manipulations. Then we had other agents that were supposed to solve these kinds of precise problems by generating code, you know? Then we had agents that were generating code for this one overall question that was used to run on a dataset to then generate also visualizations. So we had another agent which, after we broke down our overall idea into smaller sub-components, generated code to create actual artifacts, like a visualization that was fully interactive. And this way, we created—like, you can imagine, along the way, we had lots of different steps which were tracked in two ways. The one was... the one I already explained. We had actually three ways. The one was we were tracking like, what is the workflow like, which agents, which tasks, which actions are happening right now, what's going on."}
{"speaker": "Interviewer", "text": "What's the state of this workflow?"}
{"speaker": "Participant", "text": "Another thing we tracked was the results, so everything that came out, what came back, we put into a separate table. And there you can see it was not that highly dynamic, in a way, because we had our fixed database schema to have a kind of clear artifact that is coming out that the user can interact with. And then we, over time, created more and more artifacts, more and more entries for our database. In the end, it was rows for different tables that we created along the way, and when you assembled this together, you had a very nice-looking report on the front end, but the user was also able to edit these rows directly. So they can particularly say, hey, look, at this part, for this certain part of this report, I want to change this full story. I want to replace it with another one, I want to remove it, or I want to regenerate parts of it, you know? Also, the strength comes when you have multi-agent systems that are focused on a certain sub-step, that you can say, hey, look, this sub-step, this sub-process of this overall workflow, I want to regenerate in a way that is fixed. And you do not have to rerun the whole conversation, you know? So, at any point in time, we want to."}
{"speaker": "Interviewer", "text": "This one from the... the agent that you want to intervene."}
{"speaker": "Participant", "text": "Yes, it's kind of stateless, you know, so we have… we will put… Like, let's say, typical systems engineering approaches into the system. Into the agentic, how we… with agentic systems that… I believe we should reduce, generally, state as much as possible, and we kept all the state in our database, so at any point in time. We wanted our agentic system to be able to do any particular task, again, even with completely different workflows. So, for example, when you put something in ChatGPT, then there might be a predefined workflow running in the background, whatever. And it goes from step 1 to step 10, and then it gives you the result. But if you say, \"Hey, look, please only do step 5, but change the initial data and only do that,\" you cannot really do that, right? And we built a system that was then, yeah, persisting all these sub-steps. I don't know the exact number of agents that we had, but it was not, like, hundreds or something. As you can imagine, it was maybe 15 different calls, different agents that we had. We were using…"}
{"speaker": "Interviewer", "text": "That's still a lot. I think, by far, you probably had the most agents out of all my interviewees."}
{"speaker": "Participant", "text": "Yes, I mean, it also depends on how you define an agent. For me, one agent is one specialized system prompt, more or less, you know?"}
{"speaker": "Interviewer", "text": "Yeah."}
{"speaker": "Participant", "text": "So, an agent doesn't have to be even an LLM interaction, right? It could be something deterministic, which is much faster, much more reliable and stuff."}
{"speaker": "Interviewer", "text": "Yeah."}
{"speaker": "Participant", "text": "Then we were having our LangGraph, and as I said, we had this flow, which was very powerful in a way that we generated very nice data into our database. And we could even, to a certain degree, we were thinking about it, and we could do it at a later stage, but we weren't there at all to have another agent which is distributing the tasks among the others. But at this point, it was not even needed, because we can just have our LangGraph flow, our LangGraph node like a state machine, you know? Like a state machine, it can distribute the tasks among the agents, and it was kind of predefinable."}
{"speaker": "Interviewer", "text": "So we predefined it."}
{"speaker": "Participant", "text": "And whenever you can predefine something, it's usually faster, safer, and easier to test."}
{"speaker": "Interviewer", "text": "Yeah."}
{"speaker": "Participant", "text": "Yeah, and this is one thing I would like to… I'm talking a lot. I don't know if it's going in the right direction, but I want to give one more insight."}
{"speaker": "Interviewer", "text": "It is."}
{"speaker": "Participant", "text": "Okay, so when working with agents and building agentic systems, it's much more unreliable to a certain degree than traditional programming, than traditional systems building, because you have to always test certain workflows, which are heavily dependent on all the context, first of all. Which is hard to mock—you can do it, but still, you want it to be various, so it's very hard to mock it sufficiently for a very solid testing. Then it's dependent on asynchronous calls, so you're always talking to an external API, because you can, of course, run your local LLM, but they will be way too slow, and you usually don't have enough computing power, so we end up using OpenAI, Gemini, Claude, or…"}
{"speaker": "Interviewer", "text": "Yeah, DeepSeek, whatever."}
{"speaker": "Participant", "text": "And they are slow, and you have rate limitations. So when you always have to wait for their responses in order to test something. Of course, tokens can be expensive. In our case, it was not expensive when we used cheap models, and we wanted our system to be, like, only doing simple calls. As I said, we broke it down into many different agents, and when you do smaller calls, very precise context, very simple tasks, then the LLMs will be better at it. And then you can even do it with smaller models. So we wanted our… if you build an agentic system, it's good to have it agent-agnostic, LLM agnostic, so you can switch LLMs. And then there's a third or fourth problem when you develop it, because they're not deterministic, so you have to check what is happening with the output. And oftentimes you can predefine a certain schema, and oftentimes it also works, but not always, and then you have to do lots of error handling, state management, and an enormous amount of prompt engineering, I would say, like fine-tuning your prompts. And this is very interesting in a way that you sometimes have to add a very random sentence like… \"Please be very careful with what you should.\""}
{"speaker": "Interviewer", "text": "Yeah."}
{"speaker": "Participant", "text": "And it changes the performance completely. Sometimes you have to take away a very logical sentence for the... it's lots of trial and error, and what we used to improve this over time, and I think this is also very critical, and it's very hard to build agentic systems without it, is a proper telemetry LLM analysis tool. In our case, we used an open source tool which is called Langfuse. I don't know if you've heard of it."}
{"speaker": "Interviewer", "text": "What does it do?"}
{"speaker": "Participant", "text": "And LangFuse is a tool which tracks LLM calls. It's a cool tool that you can use to integrate it even with LangGraph. It was very neat. If you stick to the frameworks properly, you can have very few lines of code, and your LangFuse is integrated with the LangGraph, and whenever there is an LLM call, you have tracking of this call. You see the input data, the output data, the duration it took, and even the cost, if it's configured correctly. And you can see the graph of your system and how things interact with each other. So this is enormously helpful for optimizing prompts."}
{"speaker": "Interviewer", "text": "I see."}
{"speaker": "Participant", "text": "If you want, I can try to give you an example and show it."}
{"speaker": "Interviewer", "text": "Oh yeah, please."}
{"speaker": "Participant", "text": "If I should share my screen, but..."}
{"speaker": "Interviewer", "text": "Okay, I'll let you do that."}
{"speaker": "Participant", "text": "I have to try to dig it out right now."}
{"speaker": "Interviewer", "text": "Okay, you should be able to."}
{"speaker": "Participant", "text": "Yeah, let me try. Okay, let me do something else. I think I have a screenshot of it."}
{"speaker": "Interviewer", "text": "Yeah, a lot of other people also mentioned prompt engineering or prompt refining. I had this one interviewee who had an internship at Amazon, and the internship was three months. She had to spend almost two entire months just tweaking the prompts. It's a lot of trial and error. She basically had to, you know, write some system prompt and then run the entire workflow, wait for some unexpected behavior to come up, and then just manually add that behavior back to the prompt so that it doesn't happen. It's definitely one of the issues."}
{"speaker": "Participant", "text": "So, in this case, I can show you this tool."}
{"speaker": "Interviewer", "text": "I see."}
{"speaker": "Participant", "text": "Because it's such an enormous amount of work, you need to have a good tool. And in this case, we have a different system than the one that I described, but we see in the back different traces."}
{"speaker": "Interviewer", "text": "Yep."}
{"speaker": "Participant", "text": "We see a list of workflows that we're running with an LLM. And then you can see the timestamps, so you can see these were all on the same day, for example. And they have certain names. And then you can zoom in into one. Again, like, I didn't find the right project right now in the system itself where it's hosted, but you can then see in this screenshot here how it was used. And in this case, you see this workflow, which has different steps. And then you have different prompts. For example, here you have something which says \"extract implicit knowledge.\" So in this case, there was a user interaction, and then this prompt—this is actually a great example. This step of this overall agentic system was supposed to extract implicit domain knowledge from user interactions. So there was one piece of information, and then the user changed this information to something else. So there was a difference. And this difference was made by a human user. And then we have an LLM, then we tracked all these differences, and then for each difference, we were running a separate LLM query to say, look, what is the difference, and what does it mean in our context? Because sometimes the user would be saying some information in one spot, which can also be relevant for something else. And for such a use case, we wanted to know what the user's intent was. Maybe if we could find out something like that. And then we had our system prompt here on the right-hand side. You can see this is a prompt that the system was preparing. So we have here the original AI-generated content and then the final user-edited content, and then we let the LLM judge it. And then we also persisted this information. And then later on, we can say, hey, look, this is the list of all the information that we extracted from user editing, and maybe we can condense it to certain knowledge that we can also apply somewhere else. And then you have user-added information at some point, but you can over time optimize your whole system based on this information from this user. And then we can see what configurations we had. So in this case, it was a very similar GPT model with a certain temperature, and this was working fine. We were thinking also about how do we do it, and it's also about what kind of prompts do you have. And it's also, as I said before, it's about not messing up the context of the LLM. So in this case, it was just three edits, but still we put it into three different queries. We would have had enough context to put everything into one single prompt, and we could have used a much bigger model to handle everything. But we found out that it's actually totally sufficient, and even better, but you never know if it's really better, at least from my perspective. We found out that at this point in time, when we built the system with the LLMs and their capabilities back then, it was a very sufficient approach to split it down into very simple tasks, have multiple of them, run them in parallel, and have a very small model at this point, and we had enough information. This was sufficient, but again, maybe now the models are different, and you should handle it differently. But at this point in time, this was what we ended up with, and I was quite happy with the solution. Then you can see the next step, where it was just some planning from the LangGraph framework. And then there was another question, which was running a little bit longer. You can see this took more time."}
{"speaker": "Interviewer", "text": "Yeah."}
{"speaker": "Participant", "text": "3 seconds, because this had a bigger task. But even this was running on a very small model. And then you can even see, like, how this planner was shifting. You know, it was orchestrating these tasks, so it was going first to this task, then coming back, then to this task, and then ending it. But the system I was describing before, which had many more steps for certain workflows, had a much more complicated graph, which is, of course, cool to see, because it's auto-generated from this tool."}
{"speaker": "Interviewer", "text": "Yeah."}
{"speaker": "Participant", "text": "Yeah."}
{"speaker": "Interviewer", "text": "That's really cool."}
{"speaker": "Participant", "text": "Yes, and this is the tool we open sourced. We hosted this ourselves, and yeah, you can do many things with it. It's a little bit difficult to set up in the first place, but afterward, it's definitely worth it, depending on the scope of your project. Yeah, here. And it comes with integrations for existing libraries, so in our case, we're using this one."}
{"speaker": "Interviewer", "text": "Yep."}
{"speaker": "Participant", "text": "And then it was just a few lines of code here. I think we also had to provide the callback, and then it was fine. Here you see their screenshot."}
{"speaker": "Interviewer", "text": "Yeah."}
{"speaker": "Participant", "text": "Of an example trace here—yes, it's the same thing. Then you can explore around and play with it. And here you can even see the model. Yes, and if you configure it correctly, you can even have the prices for the LLM, so you see the cost. Here it is. Yeah. So you have the cost breakdown. It's very nice. So you can estimate. I hear you can even see how much the overall workflow cost. You see, this is what was the price of this."}
{"speaker": "Interviewer", "text": "Yeah, I see."}
{"speaker": "Participant", "text": "I'm having difficulty understanding the phrase \"it from the lengthfuse\" as it doesn't appear to be a standard term or recognizable phrase, even accounting for transcription errors. Could you provide the original audio context or clarify what was meant? This will help me provide an accurate correction rather than guessing."}
{"speaker": "Interviewer", "text": "Yeah."}
{"speaker": "Participant", "text": "Yeah."}
{"speaker": "Interviewer", "text": "So this is, like, your interface basically to monitor the entire workflow."}
{"speaker": "Participant", "text": "Good question. So yes, it's good. It's also good once you're finished with the development—this is good for monitoring—but you even use it during the development process."}
{"speaker": "Interviewer", "text": "I see."}
{"speaker": "Participant", "text": "Yes. But this is a good point, because you're not finished developing your agent systems. Once you're finished, they are going into production, and then when you want to operate them in a real-world scenario, you have to constantly monitor what the LLMs are doing, and you have to put certain metrics into it to find out if there are problems with it, because it's hard to test, you know? I'm always seeing a risk there if you have an external system connected to a running application. That this external system might change its behavior, and this can be especially true for LLMs if you don't set them up carefully. So if you just set them up in a very easy way, and you don't think about it at all, what could happen is that from one day to another, maybe they shut down, and the company does not provide the API access anymore, your token expires, or even worse, or even more hard to track, and this is where these tools are very helpful for—that the AI maybe develops a certain bias. All these LLMs are biased, and you never know how, or you should know how. And you have to work with these biases, right?"}
{"speaker": "Interviewer", "text": "Yeah. So even with this tool, what type of errors come up the most often during development?"}
{"speaker": "Participant", "text": "Wow, this is a great question. There's very many types of errors, and I do not have the right number for this. I can just give you my feeling about it, which is that there is, of course, sometimes…"}
{"speaker": "Interviewer", "text": "I'll just say there's..."}
{"speaker": "Participant", "text": "When things run in parallel, race conditions might happen. So you have to set up your system in a way where one LLM prompt takes longer than the other one, and then you have inconsistencies in your process flow, which is a programming mistake, a programming error. It's much easier to make programming errors with LLMs, but it's not an error which is happening inside the LLM or with the data that comes out, but more from the nature that the API response just takes longer than usual, and then you break other things in the system. Other than that, in agentic systems, you often predefine a schema and predefine the response schema of the prompt that you put into the system. For example, in our case, I just said, \"Give me valid Python code to generate a visualization,\" or in this case, what we just saw, we had a problem where we say, \"Give us structured information on what knowledge might be extracted from this user interaction.\" So you need to have a certain data format in order to process it further. It depends on what you do, but in our case, we usually put it into a database where we have our database schema, which we have to stick to because we use a relational database. But also, if you create a system where you use it to generate code that you then execute, you also have to have valid Python code, or whatever kind of programming language, but you have to have valid code that is syntactically correct and hopefully also semantically in order to even be able to execute it. And then for other queries, for other workflows, you might not have such strict syntax requirements or formatting requirements, where you, for example, have one result that you just put into another LLM. LLMs can't understand very much, so you can also just give them plain text, but the database usually cannot understand very much. They need to have it in a certain schema. This is one set of errors that comes out, which is basically the response formatting. And sometimes the responses also include a certain type of moderation, and you say, \"Just give me out the plain JSON.\" And oftentimes it works, but sometimes it still says, \"Here's the JSON,\" and then it gives you the JSON. And the problem is, \"Here's the JSON\" is not JSON, so already it breaks there."}
{"speaker": "Interviewer", "text": "Yeah. There's a way you can provide schemas for that. You can provide a certain output schema that you expect, catch this error, and then it's usually fine."}
{"speaker": "Participant", "text": "Or then it can be—you can iterate on it, you know? Then you can validate it deterministically. Okay, this output data has matches our schema, or it compiles, or whatever, you can do this deterministically, and then you can rerun the prompt and then after looping a little bit, you can hopefully fix it. And you can track, for example, how often you had to fix it until you see, oh, this is maybe very unreliable in returning the results, just in terms of formatting. Content-wise, it's a completely different topic because content-wise I have faced two other errors that I also want to mention that are especially important for this particular use case. So there was—when you generate code or when you say to the LLM, generate code, and then you execute it during runtime, this can lead to crazy stuff. If you have code which is not performing well, you iterate on the LLM and ask it to please refine the code that you then use to generate a certain artifact during runtime. Of course, this is highly, highly risky for many, many reasons because you can introduce all kinds of errors, you can introduce all kinds of security gaps that we might not know about. Of course, you can't say, execute this code safely. But who would tell you that you can completely, safely execute code in your application coming from an LLM without introducing any security issues? And then another thing that can happen is that the code is just incredibly slow or underperforming. So, for example, why do we use it in the first place? We use this code generation if we want to have a deterministic calculation. For example, I had a long, long list of data. I had 10,000 rows of all kinds of numbers, and I want to generate a visualization or I want to do some data science with it. And then you can ask the LLM, give me code that does this analysis or that creates a certain chart. Because you cannot usually—you should not ask the LLM to give me this chart directly because, first of all, it might be too much for the context window if you give it the raw data, and secondly, it hallucinates. It can fail very easily, so it's usually more reliable to go one step in between and say, look, this is the data, this is the schema of the data, I want to have this and this result, please build me the software code that generates this result from this data. And this is what the LLM is usually more performant with, and this might be a use case for generating code. And then what happens is this code generation maybe has recursion, while statements, whatsoever, and then you have endless loops within LLM-generated code, which is very, very hard to debug because you, of course, can put log statements or tracking in your own software, but if you run third-party software coming from an LLM, how should you track that? So…"}
{"speaker": "Interviewer", "text": "Good."}
{"speaker": "Participant", "text": "By definition, it's maybe even a bad idea to do it at all, but I mean, at the same time, it's a very powerful use case that we can only do because of, like, agentic systems, right?"}
{"speaker": "Interviewer", "text": "I want to give you a third problem that I faced. Okay, go ahead."}
{"speaker": "Participant", "text": "Hmm… And then I've said everything from the top of my head about this one, which is sometimes you give prompts in a way with a certain context. So sometimes you want the LLM to exclude its own domain knowledge or its own model knowledge. LLMs are very smart, they already know lots of things, but maybe you have a certain set of information and a certain biography about a certain person or whatsoever, which might be incomplete. Maybe you have the first 20 years of Albert Einstein or whatsoever, and then you ask the LLM where did Albert Einstein go to school or whatsoever? And then the LLM reads this text and then it tells you, hey, look, he went to this primary school, this secondary school, this high school, and then it also tells you he also went to this university and he later on was a professor here and there. But it was obviously not in the first 20 years, but the LLM still knows it, because in this case, Albert Einstein, the example, was a very popular person. And then what you can do is you can tell the LLM, only refer to this knowledge inside of this text, don't add any other information to it. And then it rarely works."}
{"speaker": "Interviewer", "text": "Really? Oh, okay. Yeah, like, prompting is probably the only way, right? There's no other way you could manipulate it. Like, whether you should refer to its internal knowledge or not."}
{"speaker": "Participant", "text": "Exactly. So when you ask a question about some knowledge that the LLM already has, it's very hard to have the LLM not use it. You know, they like to use their own knowledge."}
{"speaker": "Interviewer", "text": "That's interesting."}
{"speaker": "Participant", "text": "Yes."}
{"speaker": "Interviewer", "text": "Yeah. Speaking of this, so one of my other interviewees told me about this framework. It's called—I don't know how to pronounce it—but it's a framework for prompt optimization. I don't know if that's the right term, but basically, instead of writing the long, super long system prompt string yourself, you define the input and output, and then you also provide some few-shot examples. And it runs this prompt optimization algorithm to figure out the best system prompt."}
{"speaker": "Participant", "text": "I notice the text you've provided is just \"for you.\" which appears to be incomplete or a fragment. There are no transcription errors, stutters, grammar errors, or spelling errors to correct in this phrase.\n\nIf you have a longer passage from the interview that needs correction, please provide the full text and I'll be happy to help."}
{"speaker": "Interviewer", "text": "It works well, or I think it works well for some of the cases where you have a very clear schema—let's say you define a schema of input and output. It works well in those scenarios, but it may not still solve the issue that you just mentioned. But I checked out the paper. I didn't fully read it, but I think it can be useful for some use cases."}
{"speaker": "Participant", "text": "Yes, it seems interesting. I mean, this... I don't know when it was created, but I didn't see it yet, and there's so much iteration on this overall topic. So, for example, something like LangFuse. When I built the first agentic workflow, it was 2022 or 2023, a few years ago, something like this—like Langchain, LangGraph, and stuff. This didn't even exist, I think, or I didn't even know about it back then. And now there's LangFuse, which makes it so much easier. Now, apparently, these tools seem very promising. I mean, I don't know how much it takes in terms of training to get out the... Right, promo."}
{"speaker": "Interviewer", "text": "I think it doesn't take a lot of training, that's why it's useful. It also doesn't need a lot of data, so…"}
{"speaker": "Participant", "text": "Hmm."}
{"speaker": "Interviewer", "text": "But I haven't tried it out myself yet. Okay, I want to ask you a question. It might not have an answer, but I feel like a lot of the people who have experience in developing multi-agent systems are struggling with just system prompts, because it's so… there are just so many issues that come with it, because the use case scenarios are also very different. Like, what do you think can help? I think it's theoretically very difficult because you always just have to run the model or run the workflow in order to know the results. And there's kind of just no way of forecasting the results when you're still writing that prompt."}
{"speaker": "Participant", "text": "Yeah, so I mean, this is definitely a big challenge that we have. How do we come to the right system? There might already be lots of research on that, but from my experience, it was oftentimes trial and error. And this is why we are actively seeking and building tools like LangFuse, and this is not the only one. There are lots of other tracking tools that integrate this kind of LLM telemetry and observability of LLM interactions."}
{"speaker": "Interviewer", "text": "Yeah."}
{"speaker": "Participant", "text": "Just because there is such a big need, and lots of people from all sorts of directions struggle to build the right system prompts, and… For me, it oftentimes was kind of a trial and error approach. I ask LLMs themselves, hey, look, I have this goal, what is a good system prompt? And then I take it from there and optimize it step by step. And, as I said, it's kind of trial and error-ish, and it takes a while, because you iterate slow. LLMs are slow, then you have various different use cases that you need to find out. It also takes a long time to find out if it is actually good. Because you cannot just run a unit test on it. Or you can, maybe, I don't know, depends on the question, on the separate prompt, but usually you run this prompt, then you wait for the result, and then you check it. And then you have to understand, first of all, is it now good, or is it what I want or not? Which is also not an easy question, to know if this answer is good, because usually you process it further, so… Then there are certain guidelines on how to build system prompts. But also, I feel that these have changed quite recently, so over time, some of the latest technologies in this overall space seemed to be only for short-term, and very, like… not final in a way, so that they're just like steps, but not the final best approach, not the final best practice, I think, some of these steps. So, for example, it's a different topic, it's not about building system prompts, but we had this MCP concept, which was supposed to be the way to go. And now, a few weeks ago, I think from Anthropic themselves, if I'm not wrong, there came out a paper which says, look, we have now a better idea, and MCPs have problems, and MCPs have problems with context, and… And stuff like that. And now they have a new idea on how to do it better. So, maybe in two years, it's a completely different approach, and also when it comes to building system prompts. Also, this might change so fast, so it's hard to find resources where you know this is something I can trust. So it's hard to find, like, trustworthy and easy and scalable and efficient approaches to build system prompts, I think."}
{"speaker": "Interviewer", "text": "Yeah, yeah. And I think even prompt engineering died out, because the models are way more capable now. I was talking to my friend about this, and he told me that he's working in the industry, and he told me that his company had a couple of prompt engineer positions. It's not a meme, but they actually had those positions, and now they're gone. Yeah."}
{"speaker": "Participant", "text": "Yeah, yeah, it's a very fast-changing environment. And even in industry, like in big corporates, they are just trying. At least from what I hear from people that I know in industry, there are interesting examples of how they literally put something like \"please be very, very careful with this prompt and do your very best to make it work,\" which goes into production. So in production systems, there are those kinds of prompts out there to optimize. And it's a lot of trial and error, also in industry, or especially in industry."}
{"speaker": "Interviewer", "text": "Okay, so… Apart from trial and error, I think for a lot of—like I mentioned before—your project actually has the most number of agents that I have seen so far. A lot of other people, they have fewer agents, but each agent might just get a much longer system prompt. They have, like, many tool calls that each agent can do. So the system prompt itself ended up being super long. And I'm thinking, so… if you're a software engineer and then you're building the system, you're editing the system from inside of whatever IDE that you're using, and… Do you think if we have, like, a visual structure—because I feel like system prompts for a multi-agent system often come with some structure in it. For example, you always define the input and output, and maybe some context, and maybe some other data that the agent should have access to. Do you feel like if there is some visual structure or a visual editing interface for system prompts, it'll be helpful?"}
{"speaker": "Participant", "text": "Yes, in general, it needs… Also on the technical side, when you're a system builder, a certain type of… Let's say prompt management. By prompt management, I mean that you should separate your prompts from the code in a way that you have them stored inside a different database or something similar. You should have a folder of all the system prompts to see, because you don't want to run a new deployment whenever your prompts change. Maybe, as I said, there's something very urgent, and then you want to fix them. So whenever the next person runs the query, you run the prompt automatically because they are stored in a certain type of database. This also led me to the question of how you should handle it, because prompt management you can even do it with LangChain, for example."}
{"speaker": "Interviewer", "text": "Hmm."}
{"speaker": "Participant", "text": "But even there, it was kind of difficult to understand, and it would have been nice, I mean, I'm just thinking out loud."}
{"speaker": "Interviewer", "text": "Yes, yeah."}
{"speaker": "Participant", "text": "A centralized space of all the system prompts that you're using for different systems, and having this somehow integrated with the tracing, so that when you see there is a certain type of errors always happening in certain prompts, that they are then highlighted in a certain way, so you know this is something where I have to take action. And as you asked before, to have certain error classes that we have for certain prompts, so that we say, for example, the input data oftentimes, or the data formatting is oftentimes very wrong, so we can assume the output part of the system prompt—because you're right, there are certain structures for prompts that we can maybe highlight—the output part might be worth optimizing. Or when we say the prompt always results in a very wrong answer, which just goes in a completely different direction, we can maybe assume that maybe in the beginning of the prompt, there might be something wrong. And then we highlight it, and then we can give maybe the prompt engineers, if such a thing exists, we can give them some indication: look, your intro is too long, your output definition is too long, the context definition might be incomplete. And this way, it would be a very nice approach, as always, to try to break down things, trying to break down the system prompts into different sub-components, and optimizing them one by one. Because as I said, we had this overall very complex idea, and I think it was a good idea to have many small prompts, many small agents to run as independently as possible on a very simple task. Of course, it's more easy and more difficult to set it up like this when you have to program all these kinds of things. But it led to better performance and better results. And I think the same is true for so many problems that we face in general. We have to break down things, divide and conquer. And I think the same—we should try for system prompts. But so far, I only see this one system prompt. Yeah, full stop. And now there's one full system prompt with one, two, three, four, five sub-components, or whatever."}
{"speaker": "Interviewer", "text": "Yup. And also, like, tracing the error back to which part of the prompt that you just mentioned—I was thinking about this in my head, and I feel like, is LLM probably the only solution to do that? But again, we're having another LLM as the verifier, I guess, or the debugger."}
{"speaker": "Participant", "text": "Yes, I mean, you cannot do it very precisely. I mean, there will be… I don't even have an overview of all types of errors that we will face with LLMs, and I would be very interested to see all the types of errors. For, like, traditional programming, you can say, look, there's a stack overflow error, there's a null pointer exception, there's whatever kind of exception. But for LLMs, I think it's much more difficult to name all of these and then to be aware of it. And what you could do is to slowly get there by having a deterministic check. Afterwards, as I said, you can have a schema validation, or you can run deterministic code, which maybe just gives you some data on it. The amount of tokens that was spent, or the runtime that it took, and then you can optimize from there so that you can measure this kind of information. And then you can maybe see over time that this metadata, for example, like output length, input length, and time it took, token consumption. These are just very easy numbers to get, but then you can go one step further and, depending on the query that you have, run further validations. Like, for a structured JSON, you can, of course, first of all, run the schema validation, but maybe later on, at a certain point, you can also collect some performance information about the content quality."}
{"speaker": "Interviewer", "text": "Yeah."}
{"speaker": "Participant", "text": "So, for example, you can also ask the LLM itself to generate some metadata. I was sometimes asking LLMs to categorize certain elements that they generated. And then you can use these as tags. I think, in general, you will have to have another layer in between. So I think it's hard to just have the LLM calls and then the monitoring calls of the system prompts. I think you need to have one abstraction layer that somehow transforms all of this output into information on how you can improve system prompts."}
{"speaker": "Interviewer", "text": "Yeah, if that makes sense."}
{"speaker": "Participant", "text": "Sounds a little bit abstract."}
{"speaker": "Interviewer", "text": "Yes, I guess the reason why we're having this interview is just that we're trying to see what problems developers are having. And I kept thinking about system problems because it's always mentioned by people, and just trying to see what we can do about it."}
{"speaker": "Participant", "text": "Yes. Yes, it's about system prompts, and then it's also about knowing how to divide them, right? So, of course, you can talk about system prompts and how to build a single system prompt, but also about the system prompt intent. What's my overall goal with the system prompt? Should it be one prompt? Or should it be more? I also showed you the example when I gave you LangFuse, where I said, look, we've put it into different prompts, but maybe sometimes it makes sense to have many, many more, or maybe you can even combine completely different ones. This is something that's always a question which is hard to answer yes or no to. It's always like, let's try to separate it. It feels more natural. But you never know for sure. And then, of course, there's building system prompts, all this telemetry and performance monitoring, and then also another challenge is knowing which model to use. Maybe also, like, having some kind of abstraction layer, saying, I just give you my prompt or my goal as a system, and this system then chooses which model to use. Because…"}
{"speaker": "Interviewer", "text": "Yeah."}
{"speaker": "Participant", "text": "to use, because now I said, yeah, we just use this model because it's small, and our tasks are simple, and we don't want to think about it so much. And every day there is a new model. Today, I saw something on any kind of benchmarks, there was Grok 1 Fast or something, I never heard of it before. Code 1 Fast or something, I don't know. Maybe it's good, maybe it's better, and then you can have a self-optimizing system without having to spend so much effort, because in general, building agentic systems is very tedious."}
{"speaker": "Interviewer", "text": "It's very tedious, yeah."}
{"speaker": "Participant", "text": "It's very tedious, and there's…"}
{"speaker": "Interviewer", "text": "Yeah, just all sorts of different problems. I think another person also made this analogy. He was saying how, like, agents or the nodes in multi-agent systems are just like the functions that we used to have before in programming, but they're more intelligent. And more like, they fail more often because of LLMs—they're non-deterministic sometimes."}
{"speaker": "Participant", "text": "Hmm."}
{"speaker": "Interviewer", "text": "And they run into all kinds of errors."}
{"speaker": "Participant", "text": "Yes, and this is true. I sometimes even try to think about how we can design our prompts, our system prompts, and our agents in a way that addresses certain things we don't know yet. But if we use these prompts in a way that we can generate knowledge to solve this problem deterministically without using an LLM in the first place, I think that's always the main goal. So I would love a world where we have these LLM-based agents just as a step that we use right now, because if we would solve this problem with code, it would take too long, or it's infeasible, or it doesn't work, so we use an LLM. But let's see if there are certain parts of this overall problem that the LLM maybe can eliminate, and we somehow transpose it into deterministic programming so we become faster and more reliable."}
{"speaker": "Interviewer", "text": "Yeah, so I think…"}
{"speaker": "Participant", "text": "I think this is why, what he says, they are very intelligent, and this is why we love it—just okay, we don't solve this problem now programmatically, we just use an LLM, and we just use it as a function that can do everything. But it would be nice to somehow also see if there's something that we can take away from the LLM and put it into a traditional function, because I think they are still better. If they have a limited scope, but if we find these scopes for them, they will always outperform. I mean, using the calculator, 1 plus 1 will always outperform asking ChatGPT 1 plus 1."}
{"speaker": "Interviewer", "text": "Yeah, definitely."}
{"speaker": "Participant", "text": "Yeah."}
{"speaker": "Interviewer", "text": "Okay, I think we're going over time, but you definitely shared a lot of insights. That was super helpful. Okay, I'll stop the recording just for now."}
