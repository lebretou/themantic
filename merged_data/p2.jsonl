{"speaker": "Interviewer", "text": "Alright, we can go ahead and start. So… I'll first start from some more abstract question. So, when you hear the term multi-agent system, like, what does it mean to you? And, for example, like, how do you see it different from, let's say, a single agent or a single LLM model?"}
{"speaker": "Participant", "text": "So for the multi-agent system, I believe, so each agent should have different goals and, and equipped it with different tools or capabilities. For example, they're accessing But they can access different sources or, interact with the, environment in a more diverse way, so each agent can, Do what… what they can do the best, and therefore, they can… they can collaborate in a more efficient way, and achieve, And, complete more, complicated tasks, so I think that's the, That's the strength of building the multi-agent system to me, yeah."}
{"speaker": "Interviewer", "text": "So just, first of all, like, each agent should have slightly different, or just different, task or goal, and then they have access to different tools, or, share… some shared context."}
{"speaker": "Participant", "text": "Yeah, yeah, I think, I think yes."}
{"speaker": "Interviewer", "text": "Okay, okay, and, can you all… can you talk about your, experience with, using or developing, multi-agent systems?"}
{"speaker": "Participant", "text": "Yeah, so I think I can share, show my screen? Yeah."}
{"speaker": "Interviewer", "text": "outlet."}
{"speaker": "Participant", "text": "It has been, many… two years ago, I participated in two different projects involving multi-agent, system. Let me share my browser… I can see that? Can you see my screen?"}
{"speaker": "Interviewer", "text": "Yeah, yeah."}
{"speaker": "Participant", "text": "Yeah, there's basically two, two projects. So let me talk about the first one. So, So… so this one is somehow, interesting, I believe. It's two years… two years, in the summer in 2023, I, built such a simulation platform, which involves different, different agents, always diverse personnel. They, travel… they can travel around the map. As you can see to the right, and, when… and so this… this is also geo… geograph… geograph-grounded, so, they can know which country they're in, and, they… when… when there are, agents that… that are, geographically close to each other, a conversation could be triggered, so we can see in the call-in in the middle, so it starts conversation, and some participants, who were involved, and they started to talk… talk to each other. And, so here, all the agents, they're all, at that time, I think they're all GPT 3.5, based, and the only thing different is they have different personas. So you can see, I think. the sale in the center. You can see, their friend… they, they are… They're initialized with, different ages, different place of birth, and different, different habits, something like that. So, we can build, we can build, a very high-level girl, for example, they should share where they have gone."}
{"speaker": "Interviewer", "text": "I have to."}
{"speaker": "Participant", "text": "into, or, what's, what's interesting, what's interesting, they, see in different locations. But, on this one, that's a very, prototype, because Because, after, one month or two months of developing, we found there is an open source project which is developing, unless, you know. faster way than ours, so we just aborted this project. So this is my very first-hand experience about the multi-agent system, but it's very simple, as you can see. And the second one, it's also in 2023, so we built, systems that, that we wish to, leverage the power of multi-agent system to, give a more calibrated. Confidence estimation for a certain question. So, So first, maybe I can… talk about more… talk a bit more, in detail. So, for example, so we want to give a… give an agent one question, right? So, so, so an agent can give a, so agent can give a, give an answer, and it can… it can say, this… this… answer is 80% to be correct, so it's… it's the, confidence. But some… that, we wish that the confidence can be more calibrated. So, for example, if we select a bunch of questions, a bunch of answers, that the agent has 80% of confidence. We hope, The 80% of these answers can be correct."}
{"speaker": "Interviewer", "text": "Match the accuracy."}
{"speaker": "Participant", "text": "Yeah, so that's… that means more calibrated. So, here, we want… we want to, collaborate the… we want to involve multiple agents in the… in a debate system. So, agent debate was a very, popular topic in 2023. So, we did that. We involved the agents, so we asked them to do the second… to two phases debate, and the agents can revise their Revise their initial answer, get… revise their initial answer based on other, other models' critiques. So you can see these agents are in different models. They can be, ChatGPT, the, Mistral, or, Cohere. So, they have different, different internal knowledge, so they may be able to assist other agents to have a more comprehend… comprehensively, consider answer. So… So basically, that's… We were doing, and finally we can see that it did help the model in their confidence estimation in multiple datasets. Yeah, so that's the two, two projects I… have the first, first-hand experience with motivation systems. So it's not… so they're all… they're both in very controlled, they're both in very counter experiment settings, so they couldn't do something, free-form exploration. So, Yeah, so I think that's a limitation… I assume. …in my experience. Yeah."}
{"speaker": "Interviewer", "text": "Oh, but I think this is deviating away from our interview, but it's okay. I've already read, quite a bit of literature on confidence collaboration, and then I see that it's, the results are much better compared to self-verbalized, or just, sampling. sampling methods."}
{"speaker": "Participant", "text": "Yeah, yeah."}
{"speaker": "Interviewer", "text": "That's good. Okay. But then the… the accuracy… Or not accuracy, the confidence, And then also the way that models update their confidence is still more so just using prompt and then their self-correction, right?"}
{"speaker": "Participant", "text": "That's correct. We… we prompt them to do the update."}
{"speaker": "Interviewer", "text": "I see. Okay, and then, I can… proceed in the interview. So, could you talk about a little bit About the development, so, for example, like, what, framework did you use? I saw Lane Chance."}
{"speaker": "Participant", "text": "Yeah, of course, we only use LenChain. So, we, we just… we… We consider the length graph also, but for such simple functionality, the lynching is, good to, perform some, simple agent, agent, development."}
{"speaker": "Interviewer", "text": "Okay. And then, so… I think one of the… Hmm… Challenges you guys have is probably, developing the system prompts, right?"}
{"speaker": "Participant", "text": "Yeah, yeah, so, we actually did not, try… so you mean the second one, right? calibration. Yeah, that's correct. So, we… actually, we didn't, optimize the prompt in, in certain, high-level methods. We, we just, tried, a few, a few candidates, for the prompt, and we simply chose the, One with… with the most calibrated, confidence. So… Yeah, yes."}
{"speaker": "Interviewer", "text": "Okay. So… I guess, let's keep talking about the second project."}
{"speaker": "Participant", "text": "Yeah."}
{"speaker": "Interviewer", "text": "So it seems to me that, like, the… the structure of the entire system is pretty, It's pretty, how do you put it? Like, it's pretty intuitive, since you're just having the agents, doing debates. Were there any changes to your initial idea of the structure to the final, Architect, or the structure of the system."}
{"speaker": "Participant", "text": "I think it's, the final one is pretty similar… pretty close to, the initial, initial… scope, of the framework? Yeah, because, there is actually only two stages. So, the first stage, there's only one, agent, involved, and they give the initial, initial, result, and the second stage is to have a group discussion and change the first stage result. So… Yeah, that's because the framework is quite simple, so we didn't update the structure many times. And the initial result is, good enough, so… Yeah, so we didn't, update, much time."}
{"speaker": "Interviewer", "text": "Okay. I see. And then… I don't know if this one is… relevant to, your specific projects. But, as you may know, like, a lot of people use multi-agent system for automation, and then something that they have to deal with is, like, the context management, or, like. the memory, among agents. For example, like, what are some shared variables, shared data, and what are something that should be specific to an individual agent? Also, the tool management, like, what tool… an agent should have access to. Did you have to deal with anything that I mentioned?"}
{"speaker": "Participant", "text": "So because our, so for the ex… for the exp… For the project I have been involved in, we didn't face this challenge because, the context of the whole conversation is, is very manageable. Because there will not be… In terms of the environment variable that, That, that's fed to the agent. yep."}
{"speaker": "Interviewer", "text": "Okay. Yeah. totally okay. And okay, and then… let's talk about a little bit on, like, implement… implementation of the system and, back to when you were still developing them. So just talking about like, coding. So when you're using LaneChain, and then, let's say you have a first… a prototype of the system, how did you interact with, and then, like, sort of monitor the behavior of the system?"}
{"speaker": "Participant", "text": "I should give more detail about how you, To act or monitor the system."}
{"speaker": "Interviewer", "text": "Yeah, so for example, let's talk about the debate project, I guess. Yeah. Okay, I guess for that project, there's a couple things you should probably care about, which is, the collaborations, whether it's good or not, or, like, whether the result is correct. Did you also pay attention to, let's say, like, the intermediate, outputs from the agents? Like, what are some of the arguments?"}
{"speaker": "Participant", "text": "So… So, in this project, actually, there is, There's no… nothing we call the intermediate, So, I believe you're referring to some interactions between agents, that's not very relevant to the final result, or should be visible to the users in the intermediate stage, is that correct?"}
{"speaker": "Interviewer", "text": "It's more so just… How did you make sure that the, like, each agent is doing what it should be doing, I guess?"}
{"speaker": "Participant", "text": "Yeah, because, because, because, We also, Give me one second to, organize my words. So, I think the… I think the interaction between agents are actually, data we'll be, actively collecting. So, and we can… because, this experiment setting is pretty, pretty simple, I believe, because they're only doing, limited terms of interaction. And, what their, what their, And their interaction can, can make a difference in their, in the final result. So, we will collect them all, and, and… investigate, out there actively, contributing in the conversation. So… Yeah, so maybe this doesn't… doesn't address your concern, but that's because the, the Experiment setting is pretty simple. We don't simply read them all. We read the, we can, we can, we read their intermediate, output, Manually, and we can… we can see what's… what's going on there."}
{"speaker": "Interviewer", "text": "Yeah, that's good. No, that's, that's kind of the response I was looking for. Did you also, like, so… Still the debate, our system. did you observe any pattern? Like, for example, if, for example, something like… okay, the LLM… the LLMs are initially, overall, pretty overconfident, and then they see some arguments, and then they drop their confidence."}
{"speaker": "Participant", "text": "I think, yes, but I couldn't, Let me see if I can, find some data… it's just two… two men… two years ago."}
{"speaker": "Interviewer", "text": "Okay."}
{"speaker": "Participant", "text": "Let me, let me try if we collect some data here. Hmm, give me one second… Yes, I think, best… we didn't, I didn't keep a copy of data, but it's mentioned in the appendix. Let me share my screen again."}
{"speaker": "Interviewer", "text": "Okay. Give me one second."}
{"speaker": "Participant", "text": "Yeah, so, basically it's mentioning the figure 4, so it shows, before, before and post, after the second stage of the, distribution of models captains, change. So, so we… So first is the average accuracy, so you can see there, The, the bins in the, 2.2 and 0.5, they, almost disappeared, and, and, and the confidence around 0.9, it, it's, It, it increases a lot, and, this is for the, average accuracy… average confidence, so generally increases. And for the calibration, so a desired calibration is more close to the line. line in the graph. So, so we can see, both, increase, increasement in both, Indose. accuracy and, calibration."}
{"speaker": "Interviewer", "text": "I see. Okay. And then… So, when you're still just, implementing the system, say, using LaneChain. Were there any, like, common bugs or, issues that you run into? Sorry, I know it's a long time ago."}
{"speaker": "Participant", "text": "Long time. It's a so long time, yeah. But, I couldn't remember, actually, because, I don't see, other users, having such… having many problems when they are trying to incorporate, a long tree, sorry, a long list of actions. But here, because the agents in these settings, they are basically doing the conversation. They don't even incorporate the tools that, the… The tour in the… Yeah. that's building, land chain. So, what we're doing is mostly about the, how the prompt, the memory, and, the conversation. So, So, yeah, we didn't incorporate many advanced,"}
{"speaker": "Interviewer", "text": "Yeah. Was memory… Was memory hard to manage?"}
{"speaker": "Participant", "text": "It's not… not hard to manage, because, actually, this, Actually, these two projects, they use one codebase. So, so the memory is actually more, involved in the simulation part."}
{"speaker": "Interviewer", "text": "Okay."}
{"speaker": "Participant", "text": "Yeah."}
{"speaker": "Interviewer", "text": "How did you, how did you sort of manage the memory in this one?"}
{"speaker": "Participant", "text": "Basically, we… we only… though I mentioned the memory, so actually, we're, so most, most of the time, we, keep the four conversations, because… Right. Because for, GPT 3.5, the context window is pretty, is, is, pretty long. And, and when the, context, When the… when the count… oh, sorry. When they, Contact goes longer, and beyond the, con- contacts window that we can manage, we will use a, we'll use another agent to summarize the past experience, and then, and we only keep the summarized, experience in the memory. So we simply truncated the long-term memory and only keep a summarized version."}
{"speaker": "Interviewer", "text": "That's it. And then, I guess, for… And some of, are… So basically, in this project, each agent has sort of their own memory."}
{"speaker": "Participant", "text": "Yes."}
{"speaker": "Interviewer", "text": "They have some similar… System prompt, I guess."}
{"speaker": "Participant", "text": "Yes, that exact thing."}
{"speaker": "Interviewer", "text": "Okay. I see. And then… since… okay, again, still talking about this project, I feel like… The longer you run this… Simulation, Were, like, were there any unexpected behaviors you saw from the agents?"}
{"speaker": "Participant", "text": "Yeah, so…"}
{"speaker": "Interviewer", "text": "Fascinating."}
{"speaker": "Participant", "text": "So we, we also tried different models, like some, I think the Llama 2, or, some even smaller model, in 2023, and we did see some, Patterns that the agents are They try to repeat their… repeat some words, and, yeah, yeah, so, like. The repeated output, or… I'm sorry, I couldn't, recall, the most of the,"}
{"speaker": "Interviewer", "text": "Okay. It's okay."}
{"speaker": "Participant", "text": "No behaviors. So, but, but I, I think they're, they're mostly, similar to those, those, behaviors that the, L image are given a very long context. Especially, we are dealing, we are using a somehow smaller model. For example, they will output the Sorry, for example, they will… sometimes, the row flipping is another And those are, phenomenon I observed, so it will forget, the person that we gave it at first, which is also kept in the memory and the system prompt, and after a certain interaction with another agent, the agent will, Believe, he's the… another agent, my mistake. Yeah, yeah. So, I think that's also, investigated in another paper I co-authored, but, But it's, it's not about, it's not focused on the multi-agent system."}
{"speaker": "Interviewer", "text": "hyphen."}
{"speaker": "Participant", "text": "Yeah, this… just a, repeated… repeated output, or the row flipping. There are the two, abnormal behavior, I can't recall."}
{"speaker": "Interviewer", "text": "Okay. That's pretty interesting. So what was… what was, like, the initial goal of this project? Just to."}
{"speaker": "Participant", "text": "So, so, for this project, so, at the begin- at, So at the beginning, I was, asked to collaborate, in this project, because someone is doing the… doing the, debate."}
{"speaker": "Interviewer", "text": "And, that was another intern in my,"}
{"speaker": "Participant", "text": "in my undergraduate… the lab, I… was in, in the undergrad study. And I, and because, the, the intern cannot, manage both the, both the debate and the simulation, so, my advisor asked me to mainly develop the simulation project and, develop the backbone. I mean, the multi-agent system interaction, code. So, yes. So… so we didn't set a very specific goal for this. My advisor has multiple very opened ideas, like… like, he wants to do a simulation, like, in… with, more than 100. More than 100 agents. But, just I met, like what I mentioned, we… This project has, has been… aborted… aborted."}
{"speaker": "Interviewer", "text": "Yeah."}
{"speaker": "Participant", "text": "There's one interesting thing I think I'd share with you. So… One year later, in 2024, I… There is another intern who, take, who, continue to work, work on this project, and I think they have, They have a publication. on the… Open this experiment setting. Give me one second, maybe you will be interested in this."}
{"speaker": "Interviewer", "text": "Yeah, I also saw a couple… I think the simulation now scaled up to even, a million, if I'm not wrong. the number of agents, I think there's a paper."}
{"speaker": "Participant", "text": "Yeah, I'll… Oh, I saw on the paper, but I have not, take a very close look at this. So this is definitely the, continuation… this… this is, this definitely… is continued from my password. I put in the chat box."}
{"speaker": "Interviewer", "text": "I see. Okay. Alright, we can go back to our interview."}
{"speaker": "Participant", "text": "Oh, okay."}
{"speaker": "Interviewer", "text": "Yeah, so… Just a second, trying to locate the question. Okay, so you said that since your, structure was fairly simple, and then just using LaneChain."}
{"speaker": "Participant", "text": "Yeah, yeah. So, even with… I believe even without the lenshin, so we can manage… we can, build this system, even, because we simply, call the models API and give the context. So we don't even need an agent framework for this. Yeah, so… so the agent is more… is more specifically, Related to the first simulation. For another second debate once."}
{"speaker": "Interviewer", "text": "Yeah, okay. I guess LaneChain or the LaneGraph mostly are helpful because of, like, tools and, like, memory management, but you guys don't."}
{"speaker": "Participant", "text": "Yeah, yeah, even the…"}
{"speaker": "Interviewer", "text": "Excellent."}
{"speaker": "Participant", "text": "and wrote in, yeah."}
{"speaker": "Interviewer", "text": "Okay. And then… actually, I just have 3 more questions, maybe 2. And they're more, like, broad and abstract. So, it doesn't have to be the specific projects that you worked on, but for what types of problems do you think a multi-agent system works the best? Or what kind of tasks they're good at? Or maybe bad at."}
{"speaker": "Participant", "text": "So, I believe the multi-agent system can be the best, when they're trying to manage their, they want to achieve their multiple girls in the same time. For example, But they can… they can work on their, specific goal, I'm sorry, I think let me manage my words. Maybe I can get, come up with an example. Yes. So, so for example, if we gave… if we are trying to, ask one, one LM agent with different tasks. For example, can I write on the whiteboard?"}
{"speaker": "Interviewer", "text": "Yeah, yeah."}
{"speaker": "Participant", "text": "Oh, okay, I can see your whiteboard. So, for example, it tried to… for example, there is a task wand, Task 2? and Task 3. So, for example, if you want to one agent to deal with These three tasks at the same time, and each task, it has, subtasks. And, for example, each task, needs an interval to complete. So, so for example, task 1 has such tests and tasks, and it says task queue has different… have this, Task 3 have this. So, if you are asking one, one agent to complete this, the agent can be, cannot… maybe not, able to. manage the multitask at the same time, because, because in their, I believe that's because, their instruction tuning stage, they are not exposed to such a complicated setting. So, by the, multi-agent system, we're able to split, most… a complicated task into, into multiple paralleled Small tasks, and even though we need the, need to, need to… give some shared content to the agent. We can achieve this by a shared memory, or even the conversations between the agents. So I think that's the first, first, cases, I believe the motivation system can work the best. And the second one is, when they're trying to When a model is trying… is, is capable of multiple, I mean, the tools or specific actions, so… That's similar, it's pretty similar to this, because, if, if, if, we are, we are feeding, Sorry, if we are equipping one agent with multiple, tools, that means we… even though we… we assume these, tools are caught by a very simple API that can be learned in, very simple instructions, but the numerous, tools will make the instruction, super long in the prompt, so we need to teach how, how an agent should, call the, call the, call the tool. So this will… And even some tools are not even necessary in one setting, but we hope that the agent can can know, there is a tool existing. So, by multi-agent system, we can split, Split… split the agent with, Split, sorry. We can split, the capabilities, or that, through, through, Sorry. So, sorry. We can split the tools into different agents and ask them only to do, what are… to perform the… which tools are most frequently used. And as well as, so after we call the, task management, the two call in, also, the sec… third one can be the contacts management. Because after we, splitting one agent into multi-agent systems, each agent's context, context will be much more manageable, so we don't have to actively, compress or summarize their memory. So, a model can Be more robust in their behavior. basically, that's the three, that's three cases. I believe the multi-agent system can perform much better than single agents. It's from the perspective of a language… language model agent, because I'm not exposed to the, vision… vision… agents, or… other. So, yeah, that's my perspective."}
{"speaker": "Interviewer", "text": "Okay, so to summarize, the three scenarios are… so, first of all, when the tasks are inherently dividable into smaller subtasks, and then also when there's A second scenario is, like, when there's numerous tools, and then we want to split the tools into multiple agents so that each agent will have, a smaller range of tool selection."}
{"speaker": "Participant", "text": "Yes."}
{"speaker": "Interviewer", "text": "And then the third is for better, context management."}
{"speaker": "Participant", "text": "Yeah, yeah, absolutely. Okay."}
{"speaker": "Interviewer", "text": "I think that's a… that's a pretty comprehensive response. I guess it's what a lot of people are saying, too. Okay, and then I just have a final question. I feel like this might not be too applicable to you. And I probably have asked… already asked similar questions. So basically, what are the biggest challenges, do you think, developing a multi-agent system is? And then, is there anything, for example, if there's some add-on features you can add to LaneChain. What would that be?"}
{"speaker": "Participant", "text": "Oh, that's a… that's actually a very big file."}
{"speaker": "Interviewer", "text": "It is a big question."}
{"speaker": "Participant", "text": "Now, let me think for a while. Yeah, so I believe, one… One feature that I hope that should be incorporated into Lynching is the task scheduling, a very flexible task scheduling for a certain agent. So I'm not sure if that has already been incorporated into LendChain or LandGraph, but let me illustrate what kind of step it is. So… Let me use the whiteboard."}
{"speaker": "Interviewer", "text": "Yeah, go ahead."}
{"speaker": "Participant", "text": "Yeah, so, so for, for example. Let's say, this only applies to, a very open problem, so there is no targeted answer, that can be and be trained model to, do the optimization. We can only… Asked in order to do more free-form exploration. So… So, so for task scheduling, so for task scheduling, we wish the model to first do some planning. like this. So… But this cannot… this also a plan, can be different from, what's… different from the real scenario… real circumstance. So, the model have to, Changed the… changed their plan, When they're doing the exploration. And so there can be multiple general goals, so if I spend much time on certain, on certain subtasks, maybe it should give up, give it up, and go to the… last, last add, or it may come up with more subtasks to do in certain in certain… In certain steps, so maybe it will create some new… New steps to do. So, and it should… Should assess how each stat makes the agent closer to its final goal. So… I believe that's the very rough structure of, of the… what I said, the task scheduling. for… for… For an agent system. I'm not sure if they… someone has… has, It has built a very… such a similar feature, but I believe it's the most important. feature…"}
{"speaker": "Interviewer", "text": "Okay."}
{"speaker": "Participant", "text": "That the model… that an agent can use in the freeform exploration, because, Because, first, it's… first, it's an unseen scenario. And… And the, and the experiment might not be fully, visible to the agent in certain… in certain steps. So, whenever… The more the agent is doing certain exploration, the… The environment will change. So, it will… so it needs to, manage certain… task scheduling. in the exploration. So, yeah, so I believe that's the… feature I would like to add to the lenshin. package. But it's very rough, a rough, rough Friday."}
{"speaker": "Interviewer", "text": "So you're saying, like, correct me if I'm wrong, so you're saying that you want to have You want the agent to have the ability of dealing with, like, unseen scenario."}
{"speaker": "Participant", "text": "And open in the, open exploration. a task. Yeah."}
{"speaker": "Interviewer", "text": "Yeah, so, I know that some of… some of the frameworks do have… ."}
{"speaker": "Participant", "text": "Back to React, and…"}
{"speaker": "Interviewer", "text": "like, like Lanking. So I know that it has some… A mechanism called, fallback, which is basically when… when it gets a certain input that is not expected, it goes to a different branch. And then try something else, or just throw in the error. But it's…"}
{"speaker": "Participant", "text": "Yeah, yeah. It has been explored by multiple, multiple… agent frameworks, and I, I think, I think the lane chain has also incorporated some of the framework, like the React. I'm not pretty sure about this, let me check. Yeah, so, so… Let me, let me send that in the toolbox, the chat box. Oops, it's on the, it's on whiteboard. Okay, anyway. So, it has already,"}
{"speaker": "Interviewer", "text": "already incorporated some frameworks, but I wish that this framework can be more."}
{"speaker": "Participant", "text": "flexible and apply to the unseen scenario, or N, even though the More, free-form exploration and more dynamic environment. So, ultimately, we wish that an agent can mimic human,"}
{"speaker": "Interviewer", "text": "Yo."}
{"speaker": "Participant", "text": "human, decision in a complicated scenario, but it cannot… but… That… those frameworks that's already incorporated in, lenshin, they're actually very… very simple and cannot deal with in those, complicated tasks. So, Yeah, so that's my… my idea, and I wish that the lenshin can… Approved."}
{"speaker": "Interviewer", "text": "Okay. Would it be correct if I say that so, you… you think that… Currently, No matter how these frameworks are working. They still follow the human-defined structure or pattern, but then it would be good if the agents or the model has the ability to even change the environment or change the structure, depending on, the task."}
{"speaker": "Participant", "text": "This can be one, one fiscal interpretation of what I generally prefer. Yeah."}
{"speaker": "Interviewer", "text": "I see. That's interesting, that's interesting. Okay. That was actually my last question, and then… Let me just stop the recording."}
