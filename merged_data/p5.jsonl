{"speaker": "Interviewer", "text": "Okay… And… Okay, we can go ahead and start. So, I will first start with some pretty abstract questions. So, when you hear the term multi-agent system, what does it mean to you? And, for example, like, how do you see it different from, like, let's say, a single agent or a single LLM?"}
{"speaker": "Participant", "text": "So… I think when we're working with multi-agents, a multi-agent system, it… Essentially means, you're not using a single L&M that tries to do everything to a team of specialized agents, or that can collaborate just like we do in a real workflow. So a single agent It's most of the times good for linear tasks, but it… tends to hallucinate. It struggles with long reasoning chains, and and most of the times, it can't scale across complex, call… complex tool calls. And on the other hand, a multi-agent system, it makes sure that it breaks the problem into different roles. For example, a planner, that, you know, that can decide the next steps, an analyzer that can execute those tasks, those tasks. And a critic that can check, for correctness, or, like, a validator that can enforce reliability, so that our answers are what we're expecting it to be. So each agent is specifically optimized for a specific kind of behavior, and there's coordination between them, which makes it more accurate, than a single agent."}
{"speaker": "Interviewer", "text": "I see, okay. It's a very comprehensive definition. Okay. And can you tell me about, just one or two representative projects that you have done in the past, that used multi-agent systems?"}
{"speaker": "Participant", "text": "So… Oh, okay. I would think. Okay. So, in my internship at Amazon, this last summer, I worked on creating a project where we were building an autonomous root cause analysis system. It was built for very large-scale operational logs. At Amazon. And, so, We had different types of agents in that, which coordinated Instead of just one model. So we had the… all… all of this information that I know about it is through that itself, the planner region, which decomposed the incident question into steps. There was a retriever agent, which pulled the relevant log slices. There was an analyzer agent, which generated… We generated the domain, specific Python code to inspect patents, because the data was so huge that most of the times it would hallucinate to, you know, pinpoint that what was the root cause behind the failure of delivery at the right time. So we had to make sure that we are, you know, using just a subset of the dataset to get the pattern, And obviously, we understood the risk that there could be a possibility where the subset of data that we are using and the Python code that we are generating and hoping that it would generalize to the entire dataset, there is a possibility that some root cause agent would be missed. But… that's something that, we need to improve on, of course. There was a critic agent that reviewed the code for correctness, a validation agent which performed the TSNE checks and the SHAP attribution to ensure we have reliability. That was one of the projects that I worked on recently that used multi-agents."}
{"speaker": "Interviewer", "text": "I see, I see. And in that project, did you… Like, what was your role? Did you focus on A specific component, like maybe writing system prompts, or you're kind of doing the entire project."}
{"speaker": "Participant", "text": "So I was doing the entire project. I was just given the problem statement that this is what… this is the problem that we're currently facing, and The major, the most… important problem that I kept facing was the amount of data set that we had, the number of columns, it was just way too large, you know?"}
{"speaker": "Interviewer", "text": "It's way too long."}
{"speaker": "Participant", "text": "So, way too large. So, it took a lot of time to come up with some form of way to make sure that the LN doesn't hallucinate, and we can, we can… understand at least 90 to… I think, the threshold was 95%, I think. I don't remember, but 95% of the, reasons why the package was missed. So… my responsibility was… I mean, I was just given the problem statement, so I had to do everything on my own, and Yeah, so that was one of the main things that I had come up with, and the other thing was the… I proposed that we used, DSPY-based prompt signatures, that created the domain-specific Python analyzers instead of relying on, the fixed prompts. Every time."}
{"speaker": "Interviewer", "text": "I see. And for that project, do you remember, like, what kind of frameworks did you use? Like, some popular ones, like, LaneChain, LaneGraph?"}
{"speaker": "Participant", "text": "So, we, of course, used the SPY for prompt, prompt programming, prompt engineering, and the agent signatures, and, we did use, in the beginning, we did use LAMChain for, we… I mean, I tried considering the possibility of integrating land graphs as well, but the time period was too short to, you know, understand. both of those things, I was a little familiar with the working of Langchain, so we used Langchain for orchestrating the Planner, analyzer, and critic workflows."}
{"speaker": "Interviewer", "text": "Okay, and then, so you said that you have a planner, and then… or could you, Can I have you go over, like, the high-level strategy?"}
{"speaker": "Participant", "text": "Second, I… I think I… my yoga."}
{"speaker": "Interviewer", "text": "Okay."}
{"speaker": "Participant", "text": "Not working that well. Let me just switch to normal audio."}
{"speaker": "Interviewer", "text": "Okay."}
{"speaker": "Participant", "text": "Can you owe me fine now?"}
{"speaker": "Interviewer", "text": "Yeah, yeah, I can. Okay, here."}
{"speaker": "Participant", "text": "you know."}
{"speaker": "Interviewer", "text": "Okay. Okay. And then, could you just briefly go over the high-level structure again? So you have a planner, and then…"}
{"speaker": "Participant", "text": "Hello?"}
{"speaker": "Interviewer", "text": "Hello?"}
{"speaker": "Participant", "text": "Yeah, sorry, I'm so sorry. There's just some network issue."}
{"speaker": "Interviewer", "text": "Oh, I guess, okay, network issue. No worries."}
{"speaker": "Participant", "text": "So, at higher level, okay. So the system followed, like. we have a planner, we… next, then we have the worker, the critic, and finally the validator. That was the architecture. So the planner agent interprets the user query and breaks it into actionable steps. So in this case, the user was the operators who had to manually write those reports behind why the delivery was delayed. Then we have the retriever or the worker agent, which… which, which executed those steps, like, you know, the data was stored on Redshift, so querying the Redshift data, analyzing the event logs, generating the Python code to run specific checks. Then I had the critic agent, which evaluated the outputs, the JSON structured outputs that I got for correctness, or, like, logical consistency as well, so… and finally, I had the validator agent, which performed statistical validation. Using TSNE and SHAP, and drift scores, there was, again, some different kind of score that we came up with, as a part of the project, and schema checks to confirm that the results are reliable. So… Yeah."}
{"speaker": "Interviewer", "text": "Okay, alright, thank you for clarifying. And then… Was this… Your, idea to, like, use a multi-agent system for this specific task, since you mentioned that you were just given the task, or did they kind of say, you have to do this?"}
{"speaker": "Participant", "text": "Not really. Again, they… I… I was just given the problem statement at hand, and it was a very open-ended discussion that I… I was… they were open to discussing any ideas, so I proposed the multi-Asian approach. Because when I analyzed the problem, I realized that a single LLM necessarily… I… So, again, the time period was very short, right? It's just."}
{"speaker": "Interviewer", "text": "Okay."}
{"speaker": "Participant", "text": "So I had to, like… I did not have the time to test, like, first start with testing a single LLM, because from what… from how much experience I had working with agents, I thought that… that it wouldn't be reliable for multi-step reasoning, or tool calling, and statistical validation at the same time."}
{"speaker": "Interviewer", "text": "Okay, I see, I see. That's fair. And then… So… So I guess it was kind of intuitive to have this multi-agent system, but could you walk me through of, like, how did you go from okay, I know that it's going to be a multi-agent system, to knowing exactly, how many, like, how many agents that you'll have, what are every single individual agents, and kind of, like, what their role is going to be."}
{"speaker": "Participant", "text": "I think… So…"}
{"speaker": "Interviewer", "text": "Or did the high-level structure change over time? Did you have, like, the critic agent at the very beginning, or you kind of added that later?"}
{"speaker": "Participant", "text": "I mean, of course, the plan did change a couple of times as we went along, but I… so… because… So first, like, I started from, like, the end… I had… of course, I had regular discussions with my mentor as well. And I, I would just, you know, like, put an image out that, okay, this is what I think, we can, you know, move forward with. And, of course, I had my co-team members as well, who really helped me. So I… I tried to, like, break it down into, like, first understanding the incident, by manually going through each of those logs and defining the question, you know, and then… pull the right slices of logs, then analyzing the patterns, or writing the code. I tried doing all of this manually by myself for, like, almost a month. That's what I was told, that for a month, I need to do all of those things that I expect the LLM to do manually, the way operators would do it. And, summarizing all of it using Jira. So, that… that gave me, like."}
{"speaker": "Interviewer", "text": "natural phase boundaries, I would say. I see."}
{"speaker": "Participant", "text": "phase, is, like, as a candidate. You know, and so after that, I would, like, turn those phases that I had, that I had in mind, that, okay, this is how it should be executed. into, like, capabilities or responsibilities. So the understand and plan phase would be the planner region. Then I would pull the logs and run the analysis. So that would be… that is something that I would want an analyzer region to perform. So, the actions that I performed, from start to end in the first month, I tried visualizing, okay, this is what I want, and for this particular purpose, it's… for this particular phase, it's a little independent from the next or the previous phases, so I need a different agent for it. Or if it… or even if it's dependent on the first or the next phase, this is how I want the orchestration to be."}
{"speaker": "Interviewer", "text": "Okay, I see. And I guess in that same, month, you also kind of figured out, like, how do you, how do you, Orchestrate among agents, like, what kind of tools, or, like, how do you manage the memory also in that process?"}
{"speaker": "Participant", "text": "Yes. I see. Yeah, I think it was the first month itself. The first month was very… was a little… I mean, just understanding how Amazon worked and how the delivery worked, which… which… I was very surprised that it took me a really long time. And I had to think very carefully about the orchestration and the memory part as well, otherwise, because the system would just become a black box. You know, so… Yeah, I mean, they just taught me a lot of things, like, on the orchestration side, I remember I… We learned something about the planner-centric, there was some graph-style pattern, like, defining each agent with a very clear contract input."}
{"speaker": "Interviewer", "text": "Totally."}
{"speaker": "Participant", "text": "and tools it can call using DSPY and Langchain. So, yeah."}
{"speaker": "Interviewer", "text": "Okay. And then… So, they taught you, like. This general knowledge, and, you kind of just apply to your own project."}
{"speaker": "Participant", "text": "Yeah, the… the DSPY and Langchain part, of course, I knew a little bit about it and knew how to work with it, but, like. planning the loops, tool calling, the agent patents, was my contribution, but my mentor, and my buddy, I don't remember, but I think that's what he was called, but he really helped me in figuring out how to translate those ideas into a real production-level system, because I'd never worked on, like, such a large scale."}
{"speaker": "Interviewer", "text": "I see, I see. And then, do you remember, like, if we had to… Write the system prompts? For, like, agents."}
{"speaker": "Participant", "text": "So before we, integrated DSPY, yes, I had to write those, prompts manually by myself, and I… honestly, I think, by the, end of… 2.5 months itself, we'd been using the manually written… The manual process itself. Like, you know, for each agent, I would define things like who you are, what your role and your scope is, what things are you allowed to do, or the tools, the data, the constraints, or what… what you must output, like the schema, or the form, or the success criteria, and what you must not do. That was a very frustrating part of the prompt writing, like, I had to… I literally displayed my frustration in every problem, like, okay, you were not supposed to do this, how many times do I tell you? But yeah."}
{"speaker": "Interviewer", "text": "That's, that's funny, okay. So, I guess it was, like, a lot of trial and error, sometimes they give you unexpected behavior, and you have to manually add that to the prompt."}
{"speaker": "Participant", "text": "Yeah, right."}
{"speaker": "Interviewer", "text": "Okay, that's frustrating, interesting. Okay. And then I think you talked a little bit about how did you, like, evaluate the system? You have some certain type of score. But, could you briefly go over, like, how did you, how did you monitor the system, or once, let's say, you had the first prototype, once it's running, did you have… Any tools that help to, like, monitor the system? Maybe checking, like, the traces. Seeing, like, intermediate outputs like that."}
{"speaker": "Participant", "text": "So, okay, talking about the score, the score was, we called it the thrift score, just… to give it a name, but, it was… So the data that we had, right, since, the data set is so huge, and we're using Python code to… Create tools. you know, that could be generalized to the entire dataset. Now, I can't use the entire dataset, for finding some pattern, because then it would hallucinate. So what I want to do is, I want to select a subset of data, and the patterns that I get from those, for the root causes, I use that, those, that subset data to create, Python tools. tools that can be generalized to the entire, data."}
{"speaker": "Interviewer", "text": "Okay. Okay."}
{"speaker": "Participant", "text": "So, but what happens is that, now, the main thing that we kept thinking about was, okay, I understand that I need a subset of data, but what would be the technique used to get that subset? So there are different ways, like, you know, very simple ones, like, maybe random, or, the centroid way, or, you know, all those different… things. But again, like, if you're using a method like Centroid, it would just give you the most common ones, so we, we… we thought that, okay, if my data can be converted into LSTM using embeddings, and if I use that data set, and then I see how much the product is changing stages. like, let's say today it was ingested, then tomorrow it was, sorry, I don't remember the terms, what the workflow that a product goes through. But, it was staged, it was packed, all of these stages, right? So, we observe that when a prop… when a product is going through multiple stages, back and forth. you know, some… it does most of the times lead to not being delivered at time. So, we need… we need those tracking IDs that are unusual. So, that's… that's why you used LSTM embeddings for… to understand, The packages that are showing the most unusual behavior compared to the most common ones, and then just use those selective, packages. And, based on that, we compared, like, different techniques. So it's not like that, okay, we thought about this. We implemented the math, and we just chose that technique. No, we… we compared it with different techniques, and the, we got the most unique root causes through that technique, so that's what we went ahead with. And, for every run of the pipeline, we… We tried to produce a structured trace, So when the agent acted, of course, we had the planner, the analyzer, and all those things."}
{"speaker": "Interviewer", "text": "Yo."}
{"speaker": "Participant", "text": "What tools were called were the Redshift, of course, Redshift query, S3 Read, Python Sandbox, Shab."}
{"speaker": "Interviewer", "text": "Was that through, Amazon's, some, like, some Amazon tools? AWS tools?"}
{"speaker": "Participant", "text": "Yes."}
{"speaker": "Interviewer", "text": "Like, the structured, traces."}
{"speaker": "Participant", "text": "Yeah, AWS tools."}
{"speaker": "Interviewer", "text": "Oh, okay, okay."}
{"speaker": "Participant", "text": "And… So… I, the way we tried to evaluate, so, again, the time period was so, so short that we, of course, couldn't really experiment with it that much, but we tried to compare it with the amount of time that it takes the operators compared to the amount of time it takes the LLM to create the final report behind the root causes in a particular station on a particular date. And, get those reports, we share it with the operators, and, after we share it with the operators, we needed, like, you know, like a thumbs up, that, okay, this looks good. And, maybe this is something that can be… worked on further ahead, and in production as well. So, that's something that we did get. We did get, like, thumbs up from the operators as well. They liked the kind of results that their LLM was returning."}
{"speaker": "Interviewer", "text": "Nice, nice."}
{"speaker": "Participant", "text": "But yeah, time, I think time, and the… operator's approval and their ability to trace back, like, if they see some reason, like, okay, the main reason was, let's say that a product was placed in a different bag than it was expected to, and that's the reason that it was… that the product delivery was delayed. Now, when the operator looks at this report, there should be some way that it can trace back Oh… trace back to the data that was used to, get that decision, in the report. So that traceability was also possible, so, overall, they liked it. So, of course, more things could be done."}
{"speaker": "Interviewer", "text": "I see."}
{"speaker": "Participant", "text": "The time was short, so…"}
{"speaker": "Interviewer", "text": "Yeah, yeah, totally understand. Okay, just for clarification, so, strictly talking about, like, just monitoring the pipeline, the system, you… Primarily just relied on the, like, the trace logs, the structured logs, and you're inspecting, like, the tool use, the outputs. those information."}
{"speaker": "Participant", "text": "Hmm."}
{"speaker": "Interviewer", "text": "Okay. And then… so, I think you already talked a lot, but, when you're developing, have you run into, like, other, issues? Like, bugs, that is more, maybe, like, engineering?"}
{"speaker": "Participant", "text": "I did. So initially, since… Okay, let me think… I was actually very much engrossed in the statistical part, but, some of the issues Engineering-based issues. Like, oh yeah, there was one, the tool called… tool calls, made were failing a couple… a lot of times. There were schema mismatches, sometimes. But most importantly, tool call… tool calls failed a lot of times, and the… or the Python code, timed out. that was one of the things that, I felt… was one of the hardest issues, because for a really long time, I couldn't figure out what was going wrong about it, because there were times when it's, you know, working from start to end, perfectly giving me the report, but there were also times when we just… I don't remember the exact error that I would get, but just failed or something, and I did not understand. I would go over the code again and again, so it was an engineering-based problem."}
{"speaker": "Interviewer", "text": "Oh, okay. Did you figure out the reason eventually?"}
{"speaker": "Participant", "text": "So… I mean, we spoke about… so… I'm not sure, honestly."}
{"speaker": "Interviewer", "text": "Okay."}
{"speaker": "Participant", "text": "went wrong. I mean, me and my mentor, we, like, we tried understanding that, but we thought that… so we were… we were using Claude for that purpose, and we thought that, like, sometimes even when you're using ChatGPT, right, it would just stop working, or… Something like that. So, I thought that that's something that's happening… that's happening with the clot model as well."}
{"speaker": "Interviewer", "text": "We couldn't figure it out. So it's probably more like an AI issue, LLM issue."}
{"speaker": "Participant", "text": "Yeah, I guess so."}
{"speaker": "Interviewer", "text": "Okay. And less engineering, okay. And, okay, so… Okay, let's talk about, like, the same issue. Like, if sometimes the model just doesn't behave. Like, LLM can just fail sometimes. Do you think there's anything that can help with that? It's a big… it's a big question. It's okay if you don't have an answer."}
{"speaker": "Participant", "text": "Yeah, I mean, does… I've been. Honestly, that was the… by the end of second month, I felt like my internship was not data science related, but just prompt engineering related. won't behave like I want it to, and I would just crib in front of my manager every day. It feels like I'm just doing prompting, and I'm just talking to, the LLM every day. That's the only thing that I'm talking to. Because they might hallucinate for a single column, they might choose the wrong tool, they might produce code that doesn't even make any sense at all."}
{"speaker": "Interviewer", "text": "I woke up."}
{"speaker": "Participant", "text": "convincing. It is so convinvincible all the time, because I, I… see. when I start my day, I read it, like, one or two, like, one or two, code bases, and I'm like, okay, I… I… my brain is working. By the third time, I get so convinced by what the LLM is saying, that even if in my subconscious, I know that what it is telling me is wrong. The way it tells it is so convincing that We could easily convince her, okay, yeah, maybe it's right. So, because we tried to log every agent step, you know, the tool calling and the intermediate output, so we could immediately see where the breakdown happened. For example, if the planner chose the wrong next action. the trace can tell us, or if the analyzer generated code, referencing a missing field, which is not even present. So we can see it in the tool logs. So, I think that is something that helped me, you know. understand better where exactly the LLM misbehaved, and, where it's important that I, you know. Like, pinpoint exactly where it's off track, and that makes it just much easier to adjust the prompts, or tighten the tool schemas, or redefine the agent responsibility, so that the model behaves consistently throughout."}
{"speaker": "Interviewer", "text": "And, you talked about, like, intermediate outputs. So, I guess at the time, it was pretty painful that I guess you have to look at the outputs manually and try to… See where exactly it failed."}
{"speaker": "Participant", "text": "Yes, I had to, I had to do that. Like, for every step, I had to manually look at all the outputs. It was… I mean, now that I think about it, the project was not that difficult, but then each step took a lot of time, because there was a lot of use of LLMs involved, multi-agents. Can be direct, because of the output that every agent gets you, gives you, you have to look at it and make sure that that is the output that the next agent is expecting, and if not, then how does the next agent handle it? Even that, I felt, was, very important and time-consuming, so I had to manually inspect the traces, the outputs, to understand where the things were breaking. But I think that phase is unavoidable in any multi-agent system, and before you."}
{"speaker": "Interviewer", "text": "Right."}
{"speaker": "Participant", "text": "To make the monitoring, you need to understand the failure modes yourself as well. So once I manually debugged a few runs, I started seeing patterns, like, or the planner misunderstood certain query types, or the analyzer, I don't know, some… hallucinated the column names. So, after that, I just… I automated most of the painful parts, so I tried adding structured traces so that it's easier for me as well. And, you know, Yeah, I mean, so I… so that I don't have to inspect the raw outputs anymore. So the… yeah, I mean, the first runs were very manual and messy, but… That's actually exactly what informed the design of, you know, monitoring, or checking the reliability that eventually made things a little smoother for me, but I mean, by that time, it was anyways pretty late, so…"}
{"speaker": "Interviewer", "text": "Okay. Yeah. I… I definitely agree with what you said, like, that's just the… like, the problems that you described are pretty common with just all kinds, every multi-agent system. And I think that's why, I think Most of the frameworks, like LaneChain, they have some sort of error handling or, like, a fallback, strategy, where if it doesn't output a schema that you want, it just goes to, like, a failure branch. Things like that. But yeah, that's… It's very relatable. Okay, let me try to locate the next question. And then, okay, I'm… the next… Maybe 2 or 3 questions? And they're the final two or three questions. They're going to be, like, more, abstract and high level. It doesn't have to be the specific top, project that you just talked about. So, first of all, So, some people would describe as how much they trust some AI systems. like, they would describe their relationship with AI systems in terms of how much they trust or rely on them. Do you feel like that's relevant? To your experience."}
{"speaker": "Participant", "text": "How much they rely on their LLM system. and how much they trust it. I do think that the trust part is relevant, but I see it… you know, I think I see it in a diff… differently than just how much I trust the system. I mean, it's like… evaluating… The predictability or the transparency, and of course, the… That's a term, verifiability? I don't know. But, like, to what extent can I verify it? So, I don't trust an AI system the way I trust a person, of course."}
{"speaker": "Interviewer", "text": "You wouldn't."}
{"speaker": "Participant", "text": "I trust the system to the extent that I understand its behaviors. Its limits, or how it was trained, or the quality of its data, or… You know… And, like, when I design the multi-agent systems, I try to… trust, like, that when I give clear rolls, or layers, or tool grounding, or the consistency checks. So, yes, of course, trust matters, but not blind trust. It's structured, that comes from making the system more, auditable. Yeah, so I think I just trusted to that extent, but not blind trust."}
{"speaker": "Interviewer", "text": "Okay. Since you mentioned, like, transparency, and then… Do you think… So let's say, you built… some pipeline, some MAS, some, multi-agent systems. Do you think there's going to be, like, signals or visual cues that would make you think, okay, this is pretty transparent, and this system is predictable, I can expect the behaviors coming out from it. Like, what kind of signals or indicators, that would make you think that, okay, this is… the system is trustworthy, I can, it's pretty predictable."}
{"speaker": "Participant", "text": "I… Oh…"}
{"speaker": "Interviewer", "text": "It's okay if you don't know."}
{"speaker": "Participant", "text": "I think… I'm not sure, honestly."}
{"speaker": "Interviewer", "text": "Okay, okay."}
{"speaker": "Participant", "text": "I think when I… when I see… patterns multiple times, like, stable orchestration, or, consistent tool calls, or, the intermediate outputs that I'm getting are more interpretable. If I see those patterns consistently, I think that's when I feel that the system is transparent enough."}
{"speaker": "Interviewer", "text": "Or that I can…"}
{"speaker": "Participant", "text": "anticipated behavior. Not perfectly, of course, but…"}
{"speaker": "Interviewer", "text": "Hmm."}
{"speaker": "Participant", "text": "A little reliably."}
{"speaker": "Interviewer", "text": "I see. I see."}
{"speaker": "Participant", "text": "I don't know if that's the answer."}
{"speaker": "Interviewer", "text": "Yeah, no, no, it's a pretty vague question, too. And then… So, you mentioned that, a lot of times the system had, like, tool calls, issues. I'm… I'm a little bit interested about, like. Are you talking about, so let's say you have the same input, same user query, whatever, and if you run the pipeline, like. 10 different times. Is there going to be a situation where, like. 8 out of the 10 times, it was doing… Correctly. And then the two times it failed, or failed to call the right tool. Is that what it is, or is just, like, different testing?"}
{"speaker": "Participant", "text": "So, like, yeah, of course it did happen, like, a lot of times that… the output was just not what I was expecting. Like, especially early on, I did see cases where the same, input would sometimes, route slightly differently, or call the wrong tool. So… Oh. I'm sorry, I got a little carried away with, the… what I was thinking about. Could you repeat your question once?"}
{"speaker": "Interviewer", "text": "Yeah, yeah, so I was just wondering if, the, like, the tool called, failure that we talked about. was… So… Was that the case? Or, wha- was it, like… If you just run the same system without changing any code. Let's say you run the system 5 times, using exact same input, exact same task. Is it going to be like, okay, it worked 4 out of 5 times, but then it failed, that one time? So you said it was a, it was also, like, an LLM issue. Like, sometimes LLM works, but then, like, very rarely they just fail completely."}
{"speaker": "Participant", "text": "Honestly, like, 80-90% of the time, if the… if… if… I mean, Yeah, if the output that I was expecting, I did not get that. That was… most of the times, because of LLM fail. The other times, it was… Okay, I got the report, I looked at the report, and then I manually looked at the dataset to understand… and of course, for the data that I'd been using, for those I mean, every day the operators are writing the reports, right? So I have a view of what the operators have written, and comparing it with what my LLM has given me as well. Right. So I would compare it with that, and sometimes I felt that it, it… pinpointed to some root cause that even the operators did not catch. …you know, catch, or, did not go dive that deep, because the LLM, I… I realized that, for example, if, there was a very, common issue, like, something called container… container hierarchy mismatch. So the… the LLM would really capture that very well. it needs context of what that meant. Like, the container hierarchy mismatch, what does it even mean? The operator looks at it. they would understand it, but if, like, you know, it needs to… the way an operator would write it, they won't, in their reports, they won't write something like, okay, these many products failed delivery because of container hierarchy mismatched. They would write it in a format that's more human-readable and understandable."}
{"speaker": "Interviewer", "text": "Very little, okay."}
{"speaker": "Participant", "text": "So, that was something that I would… I feel was, like. an issue. Sometimes the LLM would, see, okay, container hierarchy mismatch. It would try to, get from its own understanding what it meant, and then, you know, rephrase that, in the actual report. So I feel that we cannot really, be that reliable on the LLM to just, you know. look at those words based on just its knowledge, understand, and rephrase it in the actual report, because then it might lead to hallucination as well. So I think that's, another failure, in the kind of report that we were expecting, that we did not get, when… when we encountered this kind of issue, but mostly it was LLM failure, 80-90% of the time. But otherwise, this was also, like, a good enough problem that we were thinking about."}
{"speaker": "Interviewer", "text": "I see. Okay. Yeah, that was actually all the questions I have. Okay, 45 minutes. But yeah. Okay, I can stop the recording."}
