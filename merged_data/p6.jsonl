{"speaker": "Interviewer", "text": "Alright, so… I will start with something that's pretty abstract. So, when you hear, the term multi-agent system, what does it mean to you? Like, what's your definition of."}
{"speaker": "Participant", "text": "Hello?"}
{"speaker": "Interviewer", "text": "Hello?"}
{"speaker": "Participant", "text": "Sorry, I think we lost traction for a while."}
{"speaker": "Interviewer", "text": "Okay, okay, sorry. So, what is your definition of a multi-agent system? Or, to be specific, LLM-based multi-agent system?"}
{"speaker": "Participant", "text": "Okay. So, I understand there is, an overload of definition on agents, specifically. So, nowadays, especially in tech, in industry, people refer to agents as having the ability to make tool calls. You, do web search, do, like, use one of the tools, which is different from… But we have been calling agents in HCI, like, dates dating back to, like, 20, 30 years ago. So, typically, in HCI, we have been calling agents as autonomous any autonomous… software or program that does something automatically for the user. So, any kind of recommendation system can be seen as an agent. A lot of universities, specifically any of the, Automatic functionalities we do for the user, like suggesting data patterns, suggesting data operations can be seen as agents. Now, of course, if we're just calling the… Toll call using the toll call definitions as agent, and the multi-agent system would be… You just have, multiple agents coordinated, together. They each have… maybe they have own… their own unique, tools to use, maybe they have the same tools to use, but then there are, Communicating, in the system, probably using chat, using text. And they'll automatically decide what… Kind of branches, logical branches to go from… to go to. And I've been using LaneGraph, so, you know, LaneGraph is a typical library for building these kind of systems. I've also used Autogen. Before, that's more conversation, systems, not necessarily having tools. I think they have tool calls support now, but when I use it, I didn't use it for the tool calls. So yeah, that's my Definition."}
{"speaker": "Interviewer", "text": "Alright, that was great. So, could you now tell me about, like, one or two representative projects that have used multi-agent system? You can start from the line graph one that I just mentioned."}
{"speaker": "Participant", "text": "Though… That project was a multi-agent system for tax analytics, So, I built a system that the user enters a goal, like a text analytics goal, say if they want to Analyze, customer comments, and they have, like, 10,000 customer comments. And then they just enter. Now when they analyze it, then an agent will first decompose it into multiple steps of And analysis methods and analysis tasks. And then each task will be conducted using another agent, because most of these tasks will be using prompts. To, complete, or they will generate code to complete those tasks. So I have one agent do the decomposition, multiple agents do all of the analysis tasks, and then one final agent to aggregate all the results, back into something the user can understand."}
{"speaker": "Interviewer", "text": "I see, I see. So there could be multiple agents in the middle doing different kind of, analytics, analytic."}
{"speaker": "Participant", "text": "Yes."}
{"speaker": "Interviewer", "text": "Okay. Yeah. And then, so… Did you, was this, like, this overall structure pretty intuitive, to you in the beginning? Didn't I have to worry too much about the structure, or did you, like, kind of tweak it?"}
{"speaker": "Participant", "text": "I think the overall structure is… has been clear to me from the very beginning. We have referred to… we have looked at… I mean, I myself, I'm a researcher in visual text analytics, so I know how typical Next analytics pipeline or workflow works already, so I know there should be, you know, some processing, and then you run the analysis methods, and then you have post-processing to do that. So the overall, like, the architecture has been clear from me in the beginning. But exactly how to implement those agents and how to connect them together is something we experimented a lot with. And of course, there's… we encounter many technical issues, how, like, consistent, how reliable these agent responses are. It's always hard to control the agents in, like, a software, like a…"}
{"speaker": "Interviewer", "text": "Yup."}
{"speaker": "Participant", "text": "program."}
{"speaker": "Interviewer", "text": "Okay. Alright, we'll go into that in a bit. But before that, so, did you also… So given this task, did you think… at the very beginning, did you think a multi-agent system is, like. the most. optimal approach? Did you try to, like, just use a single LLM? Single agent."}
{"speaker": "Participant", "text": "So… Did I try to use a single one? I think I do, yes. So… when I, I think it depends on… What you mean by a single agent? Does it mean I put all the system prompts in it, and I have all the tool codes in it, and then…"}
{"speaker": "Interviewer", "text": "Potentially, yeah, just in a single model. Have all the tools."}
{"speaker": "Participant", "text": "I don't think I've tried the final version that we have, but we have definitely tried some simpler ones. So, say I have… I give it a goal, I give it a few analysis tasks, not the full list that we have. And I ask it to write the code to, like, connect the codes together to generate a response,"}
{"speaker": "Interviewer", "text": "I see."}
{"speaker": "Participant", "text": "for the user, I think I know from the beginning that it will fail, it's just an experiment to prove to, like, the professor that it will fail. So of course, I don't think, even today, I don't think a single agent could Do a task, end-to-end task, analytic task, end-to-end, If it's, you know, too complicated and involves multiple tool calls, and everything."}
{"speaker": "Interviewer", "text": "Yeah. And just to clarify, so, like, what kind of texts that you'll be analyzing using the system, and, could you, like, give one or two examples of, like, the analytic tasks that you will do?"}
{"speaker": "Participant", "text": "Yeah. So the simpler one… the simplest one is, say, literature review, right? So you… if I give it… have a data set of publications, maybe there's 10,000 articles in it, and ask it to… Tell me which, how many, like, how would you categorize the research topics in this field?"}
{"speaker": "Interviewer", "text": "Maybe there are 10, maybe there are more."}
{"speaker": "Participant", "text": "Apparently, this is a task that involves multiple NLP, or text analysis methods."}
{"speaker": "Interviewer", "text": "Yep."}
{"speaker": "Participant", "text": "You need to do segmentation, you need to, do clustering, topic modeling, keyword extraction, and then, of course, summarization, and there's many kind of way to do this already, and then these multiple kind of ways, there's parameters to set. I don't think it's a thing that a single agent can do, for sure."}
{"speaker": "Interviewer", "text": "For sure. And then… Okay, so you mentioned that, Communication between agents, also, like. Context management was, one of the challenges that you guys had. Yeah. Could you, like, elaborate on that?"}
{"speaker": "Participant", "text": "So, I… in this project, in this system, I… we don't specifically encounter issues in context management. I don't know what you mean by context management, but this is not… I mean, I can talk about it in my other project, but in this project, it's more about decomposition and connecting agents together, because… Actually… The decomposer agent, So first of all, we have a decomposer agent, right? And we have the execution agents. We have a… we even have an emulation, agent. And then… We have to use or come up with our own, DSL, or Domain Specific language, in… a JSON format, because, you know, that's how best to communicate between agents. The decomposer agent outputs some result in the DSL we have, and then the execution agent takes it, and then outputs something… another thing, and then, the evaluation do another thing. So this DSL is something we, experimented with a lot, how to design this DSL, how to, make everything robust, how to do, like, error correction. Or output checks, consistency checks, stuff like that. Another thing is, text analysis methods. Is a very long list. to… to pick from. And so, sometimes we still have issues, by the time we finish the project, we still have issues with agents sometimes picking the wrong methods, or not the."}
{"speaker": "Interviewer", "text": "True."}
{"speaker": "Participant", "text": "The full list of, methods that we need it to… that we need."}
{"speaker": "Interviewer", "text": "Right."}
{"speaker": "Participant", "text": "Another very specific to text analytics issue that we have Is the results are chained, Between the execution pipeline. Say we start with an article that's, like, free from unstructured text."}
{"speaker": "Interviewer", "text": "we."}
{"speaker": "Participant", "text": "Say, do a summarization of it. And then… We also do a keyword extraction from the text. So now we have this branched out output, one is the summitization, one is the keyword, and any subsequent tests. That follows… after this, too, can choose to operate on the summarization, or the keywords, or the original article. So it's… there's 3 possible inputs that a subsequent task can take. And we need a way to specify that dynamically, because the outputs are dynamically generated."}
{"speaker": "Interviewer", "text": "Okay."}
{"speaker": "Participant", "text": "And so… I guess… One, like, in high level, one general issue we have in this is we're generating a data pipeline that's dynamic. And… We need a system that supports this dynamic generation and dynamic connection of the pipeline, which is quite hard. And we have to, of course, have to limit it to text analytics to make this thing possible. I can't imagine if we want to support every task. I don't… I don't think, at least for our scale, maybe if you have a multi-billion dollar company, you can do that, but not… not for us."}
{"speaker": "Interviewer", "text": "So, like, how did you eventually reach that dynamic, input? Was it, like. Was it more engineering, or, like, was it those system prompts that…"}
{"speaker": "Participant", "text": "I think it's both. So, first, we have to come up with a new conceptual framework to do this. So, basically, we, in the land graph, language, we formulate every task. as, as having a map-reduced chain. So… So, basically, the input of a task for any length graph node. It's mapped… is first mapped to, like, any, input that we want it to be. Let's say the input is a… it has all three fields, right? Original article, summary, and keyword. We do a map to pick the input we want, maybe multiple ones, and then we have some actual analysis method note, or chain, that takes this input, executes a function, and it generates an output. Then we have a reduce step that aggregates the output into another format that we want it to be. Okay. Every node is in the structure, and using the structure, we're able to support all the analysis methods we want it to support. While ensuring that these things can be dynamically connected. So that's both… I think that's both a conceptual thing, research thing, and an engineering thing, because this is not a typical way to use LaneGraph."}
{"speaker": "Interviewer", "text": "We're actually generating then graph nodes on the fly."}
{"speaker": "Participant", "text": "And generating the functions that the engraf node needs to execute on the fly, too. So that's, engineering effort. So, I guess it's both, yeah."}
{"speaker": "Interviewer", "text": "I see, I see. And then… okay. So… A lot of my other interviews mentioned that, writing system prompts was, another challenge is when, when, when they were trying to build a multi-agent system. Yeah. Because, it's… it's a lot of trials and errors, you kind of just have an initial prompt, and then you run the pipeline, like, multiple times, just wait for unexpected behaviors, and then, if you see them, like, you would just go back and, try to include that unexpected behavior into the system prompt. And, for example, like, do not do this, do not do that."}
{"speaker": "Participant", "text": "Yeah, yeah."}
{"speaker": "Interviewer", "text": "Did you have similar issues?"}
{"speaker": "Participant", "text": "Yes, so we have the exact issue, too, but my collaborator was mainly responsible for writing the system prompts. I, myself, I know how to write the system prompts, it's just, you know, we delegate… I delegate it to my collaborator. But the system prompt that we ended up with are very long. I think it's… it's, like, 50 to 100 lines, even more, and I saw him having to do exactly like you said, do not do this, or he has to give a lot of future examples in the prompt."}
{"speaker": "Interviewer", "text": "Yep."}
{"speaker": "Participant", "text": "So, and that's just one system prompt, because we need a system prompt for every analysis method. that we have, and we have a list of analysis methods. We also need system prompts for all the agents, the composer, the evaluation agents. Those are also very hard to, like, craft a prompt. I think we eventually ended up with because we have… we're at the paper, and then in the appendix, we have all the system prompts. We have at least 20… 30 different system prompts in the system. And they're all, like, very long and very hard to write. We spend a lot of time trial and error this. I don't think there's a… And I think that's also why we have to limit few text analytics, is because the prompts are very specifically written for text analytics, and it will not generalize to other kinds of tasks. And… I think he, my collaborators, spent a lot of time trial and narrowing this, and… I think it's this… Effort of traveling and airling that got us to… Coming up with that conceptual framework that we ended up with."}
{"speaker": "Interviewer", "text": "That eventually we are… at least make it…"}
{"speaker": "Participant", "text": "Feasible to do, and we don't, like, spiral down the chaos."}
{"speaker": "Interviewer", "text": "Okay. Sorry, could you, re-explain the conceptual framework? I think I was lost a little bit. You talked about Wrapping the inputs and outputs."}
{"speaker": "Participant", "text": "Yes, so that's very specific to text analytics, for sure, but essentially, it's a framework to handle the input-output format between the analysis methods we're gonna run. So the first step is summarization, second step is sentiment analysis, and then topic modeling, and then keyword extraction, stuff like that. All of these tabs have a specific input and output format that it needs, and they're all generated dynamically. So, first, we have to generate these, I mean, we can have these steps conceptually in general, generated, but we're actually generating the parameters of each function."}
{"speaker": "Interviewer", "text": "We have to generate it in SQL."}
{"speaker": "Participant", "text": "or sequentially, because, say, the second step is sentiment analysis, I need to know the input of the sentiment analysis, right? And the input could be generated by the first step. to generate the output of the first step first, and then generate the input… use that as the input as the second step. So we're kind of filling in these, function parameters. And of course, the function body itself needs to also sometimes needs to be generated, too."}
{"speaker": "Interviewer", "text": "Okay."}
{"speaker": "Participant", "text": "but then everything, when everything is, like, being dynamic… dynamically generated, first, the output, format needs to be exactly the ones that we want it to be. That's why we have the DSL, to make sure of that. We can check the type errors and stuff like that. And we need that conceptual framework. To formulate, every node in this chain, and consistently, because we have to generate something, and we… if we don't have a structure, we don't know what we're generating, and it's not going to… it's… it's going to have type issues, or syntax issues, or any kind of issues that you could have in generating codes. So we have to have that dynamic framework, conceptual framework, and the agent is essentially filling in the templates."}
{"speaker": "Interviewer", "text": "I see."}
{"speaker": "Participant", "text": "of the conceptual framework. That makes it much easier to do."}
{"speaker": "Interviewer", "text": "I see. Could you also talk about, a little bit about, the debugging, process? like, you mentioned, like, having all these errors, did you… I know Langraph has this, It's called… I think it's called Lane Smith Studio."}
{"speaker": "Participant", "text": "Yeah."}
{"speaker": "Interviewer", "text": "For you to, like, monitor."}
{"speaker": "Participant", "text": "It's just…"}
{"speaker": "Interviewer", "text": "Did you use something like that, or did you have your own methods?"}
{"speaker": "Participant", "text": "when we developed the project, I think Len Smith was still… Under development, or at least, if not exactly what we wanted. So we did not use Stan Smith. So the debugging has been… purely, traditional machine learning, or Python. And debugging?"}
{"speaker": "Interviewer", "text": "Okay."}
{"speaker": "Participant", "text": "It's a lot of printing. We're looking at."}
{"speaker": "Interviewer", "text": "The, intermediate outputs."}
{"speaker": "Participant", "text": "Yes, yes."}
{"speaker": "Interviewer", "text": "Okay."}
{"speaker": "Participant", "text": "I was using Jupyter Notebook, of course, to do the debugging."}
{"speaker": "Interviewer", "text": "Okay. Because that way I can save the states and, you know, execute one cell."}
{"speaker": "Participant", "text": "But that's all that we have. I think LaneGraph has some… Built-in, debugging support. for, I don't know if they're called Debug Explorer, just… Memory handling, or state handling, and stuff. But I would say the whole debugging is just… Python."}
{"speaker": "Interviewer", "text": "Okay, I see. And then… You talked about your… like, sort of your agent roles, you have decomposer, and then a bunch of execution agents, and then, some evaluation agent. What was… Like, which agent failed the most? According to your experience."}
{"speaker": "Participant", "text": "domain felt the most. I mean… the most. Of course, it's the execution agent. So the decomposer agent basically just does… text generation, as long as the generated text fits the upload format, and we want it's not a complicated format, it's good, it's less likely to fail. The execution agent is where it generates codes, and… connect co… connect functions together. So the logic has to be… It cannot be wrong. Like, none of the logic can be wrong, and whenever there's something wrong in it, it's gonna fail. And I think eventually, we have to use… we have to engage the user to, surface the errors and help… let them fix the errors, because the, ultimate system where we have is not guaranteed to generate… to always generate a working pipeline. It's, like, 80% of the time, maybe, it can generate a working pipeline. But there's always edge cases where it just doesn't pick the right parameter, or it doesn't pick the right, method altogether. And something has to be done to intervene and fix the issues. The evaluator agents are actually… They should be… equally complicated with the execution Agent, but we did not go too deep into the evaluator Agent, so we intentionally let it be simple. It just runs, it just runs self-evaluation judges, on the execution results, so, that's… That turned out to be much easier and much less likely to fail, to have, like, technical issues to fail. Of course, it has some… compromises in the evaluation rigorousness, but, technically the system, you know, is at least working. We can run the user study on it, at least. And so, in general, I think, whenever… so, one of the… thing we're generating is prompts, too. So for some… for all of the evaluation methods, we're generating prompts For that evaluation criteria, and…"}
{"speaker": "Interviewer", "text": "Correct."}
{"speaker": "Participant", "text": "So, whenever we're generating prompts, it's much like… much, much less likely to fail. Because the LMs are already trained I guess OpenAI tuned their model to follow the JSON output, quite well, so as long as we have that defined, At least, there's no… there's not going to be… Syntax errors, for sure, you can have… you cannot have syntax error in the prompts, or you cannot… you won't have the format issues, Because, you know, it follows the format. Whereas in code generation, it can generate syntax errors, for sure. It can generate the wrong parameter that doesn't exist. or… In our conceptual framework, it needs to pick The, so for a block, for a node, it needs to pick its input, key. Which is, you know, the output of a previous node, and sometimes it can pick the wrong key. And that's the major issue of the fail. So that could happen in… that's most likely to happen in execution. Agents."}
{"speaker": "Interviewer", "text": "That's it. So, so you talked about, in the end, you wanted to have, like, a reliable pipeline. And then… So, like, what made you decide that, okay, this is… pretty much viable, we can put this in a paper."}
{"speaker": "Participant", "text": "It's… to be honest, it's because the deadline is approaching. about the system."}
{"speaker": "Interviewer", "text": "We have tried…"}
{"speaker": "Participant", "text": "And we have, actually. Tried all the methods that we can think of to increase… to improve the reliability of the system."}
{"speaker": "Interviewer", "text": "Yeah."}
{"speaker": "Participant", "text": "I think the decision for cutting off development was… partly influenced by the deadline, but also because I think we think it's enough to run a user study. And we're able to get, reasonable or, like, interesting insights from the user study, even though some corner cases might not be supported, we'll just… try to not let that happen in the user study, which is focused on… because eventually the research is on the human-centered factors of the teaching systems, not just about the technical contribution. And so, if we can get something interesting from this study, that's enough. Of course, we'll have to… we'll explain the reliability issues in the paper, and that's enough for us."}
{"speaker": "Interviewer", "text": "Okay. And then… So… you mentioned that, so you used Langraph, and then… I want to ask if there's any features That I think… that you think, it would be great to have, to add to LaneGraph. But of course, like, you guys, eventually even developed a framework, to make it more… like, suited for your project. But are there any, like, new features that you think would be great to have, to speed up either development or… Create more reliable pipelines."}
{"speaker": "Participant", "text": "I think… So, it has been… Half a year. since I used Langraphs, I don't know what new features they've added to the library, but just from my PAX experience. I… don't remember, but I think there are some… like, Linegraph is supposed to support all the possible logical… Branches that you could have with nodes. I think… I don't remember, but maybe there's some logics that is very hard to express using land graph specifications. And then… Typically, right, you would add… you would, you would… House. to… I think LaneGraph was not built for this, but in my case, I was dynamically generating the nodes. And that's not something that LineGraph natively supports. I think that could be supported, because I imagine there's going to be, like, really complex cases Where you, like, the developer needs to generate a new node on the fly. It cannot be all covered with the existing, With their existing development. effort. And then the… Langraph is mostly for the evaluation part. Most of Langraph has support for, like, checking the formats, like, throwing errors for it. I think that's what they intended to do. But for, I think, many cases, especially seeing that agents are so unreliable, it's good if they have some native support for evaluation in general. say… I want to, like, it's basically most of the agents are just generating text, right? Yeah. So you could have… evaluations on, like, based on these text generation, methods. There are existing validation methods to do this. I don't think they have support for any of them, so we have to do it ourselves. I think… The other ones are, to construct a graph, not using codes, so I think they have support for this. I… at least AutoGene have the support for this, I don't know if Lancraph has it. It shouldn't be too hard to do this."}
{"speaker": "Interviewer", "text": "Not using code, so using, like, natural language."}
{"speaker": "Participant", "text": "Oh, no, using, like… Why?"}
{"speaker": "Interviewer", "text": "Okay."}
{"speaker": "Participant", "text": "Yeah, so Autogen has Autogen Studio to do this. I don't know if… I don't know if I have a LaneGraph Studio or something."}
{"speaker": "Interviewer", "text": "They actually do, but they're… I don't think they're, like, the… the best."}
{"speaker": "Participant", "text": "Yeah, yeah, yeah."}
{"speaker": "Interviewer", "text": "There's many, different, you know, like, those kind of UI out there."}
{"speaker": "Participant", "text": "Yeah. And so, once you have the UI, if they have a really good UI, and thengraph has this human-in-the-loop method, like, for interrupts, Right now, the interrupts is just, you know, having a breakpoint, and then… Have some human input, like, because they're essentially a library. a Python library. So, if you have a UI, and if they have support for human in the loop on the UI. It can be for develop… just for developers, or it can be something that the developer can release into a full system, a customer-facing system, and the customer can intervene in the loop That would be nice, because right now, we have to do it all ourselves."}
{"speaker": "Interviewer", "text": "Hmm, I see. Okay. And then… that was a lot of information. I think I'll… I won't let you describe as in detail, but, you said that you also worked on other projects, that used a multi-agent system, just, like, Couple sentences. What was, like, the goal of the system, and then, just, like, a high-level structure of the system architecture."}
{"speaker": "Participant", "text": "So, an ongoing system we're building is for multi-agent debate, or multi-agent dialogue, and we're building a system to see Where are the opportunities for human intervention, and how we support that human intervention? I've also mentioned another project that, involves contacts management, but that's not exactly a multi-agent system, that's more like a single-agent system that has rack, has tool calls… not tool calls, it's just a… it's just a rack agent."}
{"speaker": "Interviewer", "text": "Okay."}
{"speaker": "Participant", "text": "So, yeah."}
{"speaker": "Interviewer", "text": "I see. So, you actually worked on quite many, projects that used multi-agent system, so, like, if you have to generalize, for what type of tasks or problems do you think a multi-agent system is, like. The, the optimal approach."}
{"speaker": "Participant", "text": "I think it's… So, first, it's always, because the LLM's capabilities are always improving. We now even have reasoning models. When I did the multi-agent system, we didn't have reasoning models. So, I think… some of the multi-agent systems can now be replaced by a reasoning model… reasoning agent, for sure. So… We have to always look at the capability of a single agent to decide if we need multiple agents. It's only when the case… the use case is so complex that the single agent cannot handle. That you employ a multi-agent system to do this, because when you have multiple agents, it's always more technical complexity. More… or less transparency. It's harder to control. But there's also another case where if the… problem you're dealing with is well-defined, or you can… if you can find a good theoretical… Brainwork or structure, underlying it. Then you can build a multi-agent system that roughly follows that structure. So… Yeah. Now I remind, like, you might… I remind you of one of the past projects. It's, it's not an agent, but it's a… Mental health chatbots. So, as you can imagine, mental health is, you know, they have a bunch of guidelines, principles for that. And so that's what I meant as a very well-defined guidelines and principles, and you can easily build a multi-agent system following that guideline, or you can transfer that guideline into a multi-agent system. And theoretically, right, this multi-agent system should perform more… rigorously, compared to a single agent, because single agents, they always have contact issues, it might not always follow the steps you give it, but if you."}
{"speaker": "Interviewer", "text": "you have."}
{"speaker": "Participant", "text": "a multi-agent system, you can force it to follow… force the system to follow the structure you want. Because every agent is just one step in this flow, then, you know, you just need to control the connection of the nodes, and you can guarantee you have the structure you want. So that's good. And then, there's also other cases where you want to have… this is more like a… Multi-agent dialogue, where you have… you want to have a diversity of opinion. or you want to have a diversity of… basically, you want diversity. Then you can have, different role-playing agents. Each agent plays as a role. Say you're, making your policymaker, and you're dealing with, multiple, stakeholders in that policy, you can have a typical resident, you can have an insurance company, you can have private companies, private sector companies, public sector companies, and you can have each of them represented by an agent that then engage in a debate, or a conversation or a dialogue, and then Try to get a more diverse, outcome of it. That's also one thing I can think of to use water agents. Let's see… I think that's it for now. Yeah. I think so."}
{"speaker": "Interviewer", "text": "Also… Very comprehensive response. And okay, that was actually the last question I have. Can go ahead and stop the recording."}
{"speaker": "Participant", "text": "Great."}
