{"speaker": "Interviewer", "text": "Okay. Alright, let's go ahead and start. I'm gonna start off with a pretty abstract question. So, what is your definition of a multi-agent system?"}
{"speaker": "Participant", "text": "So, to me, the multi-agent system is the system where, we ask… like, where we, like, just input the system with a task that we want the system to… to done that for us. And the system will do that autonomously, by the interaction between multiple agents. So that it can do the task autonomously, but, with the interaction between multiple agents."}
{"speaker": "Interviewer", "text": "Okay. And can you tell me about one or two, Representative projects that have used multi-agent system, in the past."}
{"speaker": "Participant", "text": "Okay, the very… the very recent, project that I have done for… with the multi-agents is, a system, like, an agent that I wanted to… I wanted to do… to, like, somewhat, like, create the… because I am a PhD student, so what I want is, like, I want the… I want to create, the system that can help me with the literature review, given a topic that I want to… That I want… that I want to do. So, I developed a system that do that. Given the… Given the topic, and it will autonomously, like, autonomously, crawl all of the papers that are relevant on archive. And analyze those papers, and generate the, like. the literature review, so that I can base on that and work from that for my own literature review."}
{"speaker": "Interviewer", "text": "I see, I see. So… How many agents are there? You said that there's a web crawling agent, probably, like, another summary agent?"}
{"speaker": "Participant", "text": "Yep. So, like, I will start with, a web agent, like, to crawl all of the papers, so that is the first agent that I… that I need. Second one, like, I will use the LN Power agent. to read all of the abstract, because we know that, like, like, I asked them to read all of the abstract, and I asked them to generate some tags. And from those tasks, I will decide whether I will keep this paper or not. So after all of that, I will… have, sort of, like, papers that might be relevant to the topic that I know. But of course, like, because those, filtering is just based on the abstract. So it's not, we cannot 100% sure that it's gonna be relevant. So after all of that, I will… After having, like, list up the potential relevant, papers. I will try to let another agent to read all of the content in the paper. And then decide if we still want to keep this paper or not. And after all of that, after all of that, I will need another agent to verify all of the information that I already have."}
{"speaker": "Interviewer", "text": "Okay."}
{"speaker": "Participant", "text": "Yeah, like, and, and ultimately, we still need a human to verify all of these information, so I maybe call it, like, human agent in the very final step."}
{"speaker": "Interviewer", "text": "I see, I see. And then you'll be, like, the human that's doing the check-in."}
{"speaker": "Participant", "text": "Yep, yep."}
{"speaker": "Interviewer", "text": "Okay."}
{"speaker": "Participant", "text": "Me and, like, my, my, my teammate."}
{"speaker": "Interviewer", "text": "I see. Did you use any framework, for this pipeline that you just described? Like, Lang Cheng or Lang Broth?"}
{"speaker": "Participant", "text": "Yeah, I initially tried to use LenChen, but at the end of the day, I feel like it's too simple to use LenChen, so I just built up from scratch."}
{"speaker": "Interviewer", "text": "Just built that from scratch. I see. And then… so, when you…"}
{"speaker": "Participant", "text": "Oh, by the way, like, I wanted to know that, like, from the code, like, from the coding perspective, I built that from scratch. But, for the database, I use a very simple thing. It's… I use Notion database to save everything, and, and because Notion is very, intuitive. So I use, I use Notion, for the, like, because, like, Notion is simple, simple to set up, and also, like, it's… it's very intuitive, and, like, it can have a very good visualization for human, check… human verification step later."}
{"speaker": "Interviewer", "text": "Hmm. I actually haven't used Notion, so… The database? means that you store all the paper in Notion?"}
{"speaker": "Participant", "text": "Yep. Store of, like, information, and store all of the steps, right? Like, I just told you before, like, there are multiple steps, right? the abstract analysis, and then, like, the full paper analysis, and human verification. So for each step, I need to save everything, and those information will be saved in the Notion database, with multiple columns to."}
{"speaker": "Interviewer", "text": "Yep."}
{"speaker": "Participant", "text": "to, like, to let you know that we already done the step one, and right now we are in the step two and step three, so that would be easier for us to verify, like, verify later, because I think the human verification is very imp… is very important in this project. Because, we know that, like, LM can do LM or multiple… multi-agent system can do every… can do this very, very well. That can save a lot of time for us, but at the end of the day, it's still the research. We need to verify, because we know that, like, recently, there is a lot of, like, literature reviews that are out there, but they are… they can generate some fake, like, generate, like, some fake citation. So, human verification is very important. And Notion Database is one of the… like, in our project, we find that Notion Database is one of the best ways for us to help with the verification of human verification."}
{"speaker": "Interviewer", "text": "So, for this human verification, so, did you have this idea at the very beginning, or did you not have this and try it and saw that LLM can still generate fake content, so you decided to add this?"}
{"speaker": "Participant", "text": "So actually, like, we… we… we keep that in mind on a, in a very… in a very first step when we build this project, because, so, so, because, like. the very, like, ultimate goal in this project is to create the LM based, like, an LM, a power agent that can help us generate the literature review that."}
{"speaker": "Interviewer", "text": "Yep."}
{"speaker": "Participant", "text": "That is, like, reliable. But we know that, like, a lot of, like, our imaging can be hallucination. So that's why we keep that in mind and build that from scratch, with the human verification, keep in mind."}
{"speaker": "Interviewer", "text": "So when you're… when you were developing, what was, like, the… did you encounter any challenges, or what was, like, the part that you kind of have to trial and error? For example, like, the system prompts, or, like, just communication between agents?"}
{"speaker": "Participant", "text": "Okay, so, because we, in our process, we were trying to make it, The most transparent as possible, because we want to…"}
{"speaker": "Interviewer", "text": "We want the, the, the reliable, generated literature review,"}
{"speaker": "Participant", "text": "you cannot leave. So, that is… so… We have to make it transparent enough, but when we develop it. The thing that we need, like, we feel like is… is the most difficult is about how to Design the threshold. For the… for each step. So, for example, so as I told you before, the very first step is, like, to to use an, web, like, web crawling, right, to web crawling to crawl all of the paper that's relevant. So, the very first step, like, we need to define what Keywords we need to, to, to input to the web crawler. And even though, like, we set a lot of them, or maybe if we… if we just keep the, like, for example, like, I… the topic that I, I, I worked on is scalable oversight. So, if I just import the keyword Scala Oversight, it will give me a very relevant paper. But might not enough, like, so the precision is very high, but the recall is very low. Right. So that's why, like, we want to include a lot of, like, keywords, but then, like, the recall will be very high, but the precision is very low. So that's a trade-off that we need to work on. So, And also, another thing is, like, when we… we also use ROM to analyze the abstract, or even the full paper. So, when we analyze those, them, we need… so, because we know that LM can be hallucinated."}
{"speaker": "Interviewer", "text": "Right."}
{"speaker": "Participant", "text": "So, we need to develop a way to remove, or to at least, like, reduce the hallucination. Through, like, engineering, and so on. And the last thing that we faced in this project."}
{"speaker": "Interviewer", "text": "Sorry, Owen. 13."}
{"speaker": "Participant", "text": "Divia."}
{"speaker": "Interviewer", "text": "So you said prompt engineering, could you also, like, explain what type of engineering that you did?"}
{"speaker": "Participant", "text": "Okay, I think it's very, simple. It's just based on our, observation. So, what we did is, like, we, in our process, what we're trying to do is, like, we use an LM to read, like, to read, say, the abstract."}
{"speaker": "Interviewer", "text": "That ends."}
{"speaker": "Participant", "text": "And give a score. Like, give the relevant score to the topic. So, for example, I work on scalable Oversight. I want to decide whether this paper will be included or not. Like, instead of just a yes or no question."}
{"speaker": "Interviewer", "text": "Yep."}
{"speaker": "Participant", "text": "And it's gonna be very, very hard for us to verify later. We will ask the RLM to generate a score. And yeah, in the very first step, we just asked them to generate a score from 0 to 10, whether it's relevant to the topic. I thought that it works, but it's still not very clear for our verification step later. So what we did is, like, we give them a red light for each round of score. We give them the criteria for this score. For example, if you generate a 10, it's going to be perfectly fit. It is a… it's fit, but somewhat, not very relevant. But if zero is completely, like, off."}
{"speaker": "Interviewer", "text": "So, we have a…"}
{"speaker": "Participant", "text": "rubric. Maybe we call it a rubric, so, that we can…"}
{"speaker": "Interviewer", "text": "Berkeley."}
{"speaker": "Participant", "text": "So that we can, like, like… interpret the score later, so that, like, human is… it's easier for human to… to verify later, so that is our… one of the strategies that we use for the prompt engineering. Another, another way that we, we will try to improve the, the, the, the, the promise that, because we know that, like, we are not, like. in… in our group. Most, like, all of us are not, like, a native speaker in English. So, what we have done here is a bit… a little bit tricky, but it's like, we were trying to, design the problem ourselves first."}
{"speaker": "Interviewer", "text": "And we will."}
{"speaker": "Participant", "text": "And we will ask the ChatGPT to improve the, the, the problem, yeah."}
{"speaker": "Interviewer", "text": "Yeah."}
{"speaker": "Participant", "text": "Yeah, so, that is probably, some tricks or some way that we can use to improve the prompt."}
{"speaker": "Interviewer", "text": "I see. And that works, pretty well."}
{"speaker": "Participant", "text": "As of now, like, it's worked pretty, pretty well, like, in terms of, like, giving us the, the, the, the literature, like, the generate, the, the alternate, automate, generated, literature review, and also, like, it gave us a very. Intuitive way to verify, each step of the… of the pipeline."}
{"speaker": "Interviewer", "text": "I see. I see. So, you said that, for, like, each… intermediate outputs from the agents. For example, like, the web crawler and the abstract reader, those… all of those intermediate outputs are also stored in a Notion, right?"}
{"speaker": "Participant", "text": "Yes, yes."}
{"speaker": "Interviewer", "text": "Okay, so Notion was sort of, like, the… The place, that you monitor the entire system."}
{"speaker": "Participant", "text": "Yep, true."}
{"speaker": "Interviewer", "text": "Got it. Could you brief me, walk me, or explain what a structure looks like? Because I haven't used Notion myself, I don't know, or just described the interface."}
{"speaker": "Participant", "text": "You want me to walk you through, like, the, the, the, like, the structure of the database, or the way that I use to set up it?"}
{"speaker": "Interviewer", "text": "Or could you just screen share, if that's…"}
{"speaker": "Participant", "text": "Oh, sure, sure, sure."}
{"speaker": "Interviewer", "text": "Yeah, I'll let you. Give you the permission."}
{"speaker": "Participant", "text": "If you want, I think I can share with you the whole structure of this."}
{"speaker": "Interviewer", "text": "Oh, yeah, yeah, that would be perfect. Okay."}
{"speaker": "Participant", "text": "Okay, okay, can you see the screen now?"}
{"speaker": "Interviewer", "text": "Yep."}
{"speaker": "Participant", "text": "Okay, so it's gonna be this, let me show with you. So, like, the reason why I use Notion is because, like, I also use Notion for taking notes. Yeah, so it's very convenient to me, and very convenient, very familiar with me, and also with my teammates. So, for the… global oversight, so this is gonna be the… the Notion tab that we use for the whole project. So, for example, here we have, like, the meeting note. So, example, for meeting note here. In this paper, it's gonna be, oh, like… This is gonna be the literature review that we want to have."}
{"speaker": "Interviewer", "text": "Yeah."}
{"speaker": "Participant", "text": "So, you see here, we have a structure. This is, from human, like, we design all of this first. So we redesign all of this, so, But what we want to do here is, like, we want the LLM to autonomously generate somewhat, a literature review for us to work from that, instead of, like, we work from that, like, we work, from scratch. So, what we've done is here is, like, we divide it into two, topics that we will work on."}
{"speaker": "Interviewer", "text": "It's gonna."}
{"speaker": "Participant", "text": "The agent, monitor, monitoring, and the scalable oversight method. So for each topic, we will have, like, each, like, multiple people to work on this. And for each topic. For example, this is the workflow that we used, as I show you just now, like… Here. This is the whole pipeline for the multi-agents system that I told you before. Yep. So, what we really need here is, like. So, a very… because it's autonomous, so…"}
{"speaker": "Interviewer", "text": "Yeah."}
{"speaker": "Participant", "text": "what we need is just, like, for each topic, I showed you here, like, there are two topics, right? So for each topic, what I need is just some input, some configuration."}
{"speaker": "Interviewer", "text": "Right."}
{"speaker": "Participant", "text": "And then the whole steps here, from step 1 to step 8, will be saved Will be done autonomously. So, for example, for step zero here, you need to define the archive query, main query, relevance score. It's like, it's like, it's a threshold to cut off some paper."}
{"speaker": "Interviewer", "text": "Yeah. And some LLM system prompt."}
{"speaker": "Participant", "text": "some, LRM abstract analysis form, so I told you here, like, this is the rubric that I use."}
{"speaker": "Interviewer", "text": "Yeah, I see, I see."}
{"speaker": "Participant", "text": "Yeah, yeah. And then all of this, I will work on the JSON format, because it's gonna be easier to interact between the agents. And then, we'll do it, like, we have a… this is a perplexity, summarization. Don't care about, like, perplexity. It's autonomous, it's not used the inference. But what we hear, like, as I told you before, like, after we analyze a lot of, like, abstracts and find a shorted list of paper, we will have, like, a prompt to analyze the whole paper. And then, given all of the paper that have been, like, saved, we were trying to put that into a taxonomy generation from So that it's gonna… can generate a whole, like, taxonomy."}
{"speaker": "Interviewer", "text": "Tectonomy."}
{"speaker": "Participant", "text": "Yeah. And then finally, we will ask them to generate a writing Like, it… this is not the one that we will use, because we… at the end of the day, we still want humans to write the paper."}
{"speaker": "Interviewer", "text": "Yep."}
{"speaker": "Participant", "text": "This writing is sort of, like, give you the whole story, what's going on in this, in this research topic, so that, like, we can, have a sense what's going on, and then we will write that later. And for the database, as I took here, this is the whole pipeline. But for the database here, so for each, for each topic, we will have our own database. So, for example, I work on this one, so I can walk you through this one first. So, for each database, in the very first step, like, when you use the web crawler to crawl the paper, we will have, like, title."}
{"speaker": "Interviewer", "text": "Right."}
{"speaker": "Participant", "text": "All this excluded or further considered will be none here, right, at that step. But we will have… the text is all… the text will be generated by LRM after you read the… you read the abstract. you have URIO, you will not have this one. This will be none at this time."}
{"speaker": "Interviewer", "text": "Okay."}
{"speaker": "Participant", "text": "This will be… these categories will be the archive category, so you will have this one. But at this time, the key contribution, it will be none too. Man fighting will be none too. Methodology will be none too. And this one will be… yeah, you will have this one. You will also have the relevant score generated by you reading the abstract. The source is from archive and unique ID, so this unique ID will be used throughout our process to identify the paper, yeah. And author, too. So after that, when you have all of this, when you have all of this one, so the thing that you will have after this step will be… after the very first step will be the relevant score. So after you have the relevant score, you will know that, like, it's based on the threshold that you set. If, say, if you set the threshold is 9. So everything will be… everything above 9 will be, included, but everything will be below the 9 will be excluded. But if you see here after this process, we will have a verification step. So the very second step here is, like, you have to look at all of the excluded paper. Yeah, like, for example, you can just, just here, from here, you can just filter, like, if it's checked or it's unchecked. So here, I want that it's gonna be, like, check, because I want to, like, investigate all of the."}
{"speaker": "Interviewer", "text": "Don't agree with us."}
{"speaker": "Participant", "text": "Yeah, so what humans need to do right now, he is just filter all of this, have the excluded paper, and you're gonna decide whether this is the true Excluded paper. If it's not, you just click it here. It's gonna be… it's gonna become the, the included paper. But I would say, like, this is a very rare cases, because if it's already, like… I think the reason why I want to have this human verification is, like, I don't really trust LLM, yeah, analysis. But I think at the end of the day, for… at least for this step. It's worked pretty well. It works pretty well. Yeah, yeah. So after that, like, after we have, like, started, like, the first human verification on the excluded paper, I will probably regenerate all of this, use another LLM to generate all of the keyword manfinding and methodology. For us to have, so, by doing this one, we will… the next, human verification step is gonna be, more, like, like, high demanding. The reason is, like, right now, we will check all of the included paper, so we have to look at here. Right now, we check all of the included paper. So for all the included paper, we need to look at the tags here. We need to look at the key contribution here, look key, key manfinding and methodology. So from here. What we do is, like, we will have to carefully check if this paper is the… is a true included paper."}
{"speaker": "Interviewer", "text": "Hmm."}
{"speaker": "Participant", "text": "And if it's yes, then we just leave it there. Otherwise, we have to check. If you really want to just remove it, we just click it here. Otherwise, if you, like, like, 50-50, you don't know, you are not very sure about that, you can check the further consider. Here. So the reason why we decided for the consider here is, like, we want to have a consensus among the teammates. So if someone not sure about this one, the other one can have a check later."}
{"speaker": "Interviewer", "text": "I see."}
{"speaker": "Participant", "text": "Yep, so that is how we decide that. And after all of that, we will decide, because we have, like, included paper or excluded paper, it's whether, like, it's further considered or it's not further considered. And after all of this process, we will sit together… we will sit together and decide which paper will be included. Based on these two, and after all, like, if we have all of the included paper, which we already have here, so, we have the… all of the included paper, we will also exclude the further consider And then, based on all of this writing, we will ask the LLM to generate the taxonomy. And from the taxonomy, the taxonomy, the, the… the structure of the taxonomy is the JSON, where you have the main topic and multiple, like, citations, and then we will map back to this database, so everything will work on the database to generate this category. So at the end of the day, like, we have a few, like, if I'm not wrong, we have 10 categories here. And for each category, we'll be mapped back to each paper. So, if you see here, we only include the paper that Some paper that we want to include, and then we generate this back. And that is how we work… how it's worked in this pipeline. And Notion… how the Notion can be used here. I think Notion, it just… we use Notion just for the sake of, like, Simple, and also it's very intuitive for us to do the human verification."}
{"speaker": "Interviewer", "text": "Yeah. I see, I see."}
{"speaker": "Participant", "text": "Yep."}
{"speaker": "Interviewer", "text": "And."}
{"speaker": "Participant", "text": "Have you."}
{"speaker": "Interviewer", "text": "Have you used this pipeline, multiple times, or just… actually just for this project?"}
{"speaker": "Participant", "text": "I actually just use this, like, just use this for, for this project, and this project, but for two database, like this, this topic and the other topic."}
{"speaker": "Interviewer", "text": "Any other topic. I see. Okay. And then… So… Sorry, I'm trying to locate the question."}
{"speaker": "Participant", "text": "Yeah, but no worries."}
{"speaker": "Interviewer", "text": "You did, you did say that, you didn't really trust, like, LLM, so you needed this, like, human verification."}
{"speaker": "Participant", "text": "Oh."}
{"speaker": "Interviewer", "text": "And then, so, do you feel like trust towards LLM or, like, the reliability is a kind of an issue?"}
{"speaker": "Participant", "text": "okay, I would say that would be the issue, cause… So, okay, let me tell you, like, the reason why I really want to do this process is, like, I… I… I don't trust the… the… the… the literature, the literature review generated, like, just generated by some, like, LLM agent out there."}
{"speaker": "Interviewer", "text": "Mmm."}
{"speaker": "Participant", "text": "The reason why is, like, I… I… because I… I do research, previously, so I… I… I'm somewhat aware of the literature review. And when I try to work on the other project, like, the other topic, I am, like, entirely a new one. a new person. So when I'm trying, like, the very first thing that I need to do is, like, I need to know the literature review. So I'm trying to ask, like. Perplexity, and, Grok, and OpenAI, to generate a taxonomy. Just, just, like, type it, like, I want to work on this one, can you generate a taxonomy, or generate a literature review for me? So it generates something, because I'm not an expert, I haven't worked on that before, so I cannot verify that. So that's the first thing. So, how to verify that? I map back to what I already done, where I have sort of, like, have some sense of the literature review, and I ask them to generate a literature review for me to verify. And I, a lot of times, I feel like, those ChatGPT perplexity and, OpenAI, that's why it's very good at, like, communication, at, like, in, like, coding or something. But in terms of, like, literature review, it's… it's missed a lot of, like, important, it, like, it's missed a lot of important, works out there. And also, for a lot of time, especially, perplexity is… is… I would say perplexity is good. It's, it, it's, like, it's not generally the hallucinated vapors. But it's, it's just, like, it, it's, it can, have a very high precision. But the recoil is very low. However, the OpenAI is… it can generate a very, very good writing for the literature review. But, it's some… it's, it's… I'm not very sure for the very new version. But for the 01 version that I used before. I think it's generated a lot of, like, hallucinated papers, which is very, very, very, like, I think it's a serious concern."}
{"speaker": "Interviewer", "text": "I see."}
{"speaker": "Participant", "text": "Yep."}
{"speaker": "Interviewer", "text": "Okay, so… Besides this project, for any sort of, multi-agent system, do you feel like Human verification is kind of, like, always needed, or there can be some, Transparency signals or design that would help with this?"}
{"speaker": "Participant", "text": "I would say at the end of the day, human verification is still… is still the most important, step, when we… when we work with the LLM. But, I think, as you will see in the future, or maybe now, I think LM will be soon, like, better than humans. Okay. At least for some, for some, tasks. maybe, like, math or something. I don't think a normal human, like, average human can make, like, LLM in terms of math problems or, like, coding problems. So it's very, very hard for a human to verify the LLM's output in the future. So I would say we, instead, like. But I still believe human verification is a need. But we might need to… Have a better, like, pipeline for the human verification instead of, like, just have a human verification from scratch. So one of the examples that I know people are doing right now, and I strongly believe that will be the… will be the future, is, through the debate. So, instead of, like, there is an assumption that, like, evaluating is easier than generation. So… What you have done here is, like, instead of, like, as human to generate the verification from scratch, based on… say, you asked the LLM."}
{"speaker": "Interviewer", "text": "To generate… to… to generate the…"}
{"speaker": "Participant", "text": "To generate a math question, like, to generate a math answer from a question, instead of just ask the human to read from scratch."}
{"speaker": "Interviewer", "text": "Read from."}
{"speaker": "Participant", "text": "And… and verify that. So, rather, you will ask the outline to generate an answer. And you will add another, like, multiple agents. to debate about that. Like, for one, we'll be, trying to debate that this answer is a correct answer. Another, another agent will also, debate that this is gonna be the wrong answer. And they… Due to the debate process, they will, they will only highlight the most, like, the most important, components in the answer. So that it can lean towards the correct answer or the incorrect answer. So that humans, instead of read from scratch for the whole, answer, they can just only look at the important parts, and they will verify that later."}
{"speaker": "Interviewer", "text": "I see. Yeah, I actually had another interviewee who specifically worked on something you just described. He had this debate multi-agent system, where, each… I guess each agent is also assigned with, or they also ask each agent to output some, confidence, for, I think each of their argument."}
{"speaker": "Participant", "text": "Okay."}
{"speaker": "Interviewer", "text": "And then they showed that, in the end, it had higher accuracy than just having a single LLM."}
{"speaker": "Participant", "text": "Yep, yep."}
{"speaker": "Interviewer", "text": "Yeah. Okay, and Okay, I have two more questions, which are a little bit more, broad and general, so, you don't have to think This project specifically. And okay, and first. what, what do you think the biggest challenge is in developing a multi-agent system? I know it's a big question, so, I guess just, talk about anything that's, in your head. Like, the biggest challenge in developing."}
{"speaker": "Participant", "text": "The biggest challenge in developing It's really a very broad."}
{"speaker": "Interviewer", "text": "I guess I can give some examples to make it less broad. So, for example, I have… I had other interviewees saying that System Prompt is something that they really struggled with. I had this one, person, she worked at… She did an intern at Amazon, and then the internship was 3 months long, right? And… She was developing this multi-agent system from scratch. the structure or the architecture itself wasn't too complicated, like, she had a few agents, but she really struggled with writing system prompts. The… she… I think she… she ended up spending two months just trying to refine her system prompts, because, At first, she was just writing the system from scratch. And then she had to, like, run the system and wait for some unexpected behavior, and if that happens, she will have to, like, manually include that into the… into the system from herself. And so it's just, like, a lot of, trialing… trialing and erroring. And which took a lot of time. And, I think other people also mentioned, like, the… Just the communication between agents, like, defining inputs and outputs, and how to make the entire, orchestration to work."}
{"speaker": "Participant", "text": "Oh, okay, so it's really about the tech… okay, something. Okay, if that is the case, then I think I have an answer in my head now. This might not be, verified, because I haven't really, faced a really, concrete challenge on that. But, just from… based on my background, because, I previously worked on the, robustness of the model. And I think I also see that quite a lot of time, yeah, in the… When I use the LLM. So, I would say, one of the challenges is about the safety of the…"}
{"speaker": "Interviewer", "text": "Of the airline."}
{"speaker": "Participant", "text": "communication. So, what I mean by safety here is that, I know that, like, there is some people, they're trying to develop on multi-agents, they're trying to, develop, Specifically, they're trying to develop multi-agents for… for… for web agents. So, so, when we do with… when we're done with that, the… the problem is, like, the behavior of the agent system. We cannot control the behavior of the agent system. And, for example, like, that is a web, like, web shop agent. Like, there are multiple agents that we… we will interact with. So, for each agent, we're gonna be responsible for one, like, one store. In the webshop, so there are multiple webshops, right?"}
{"speaker": "Interviewer", "text": "Yep."}
{"speaker": "Participant", "text": "The question is, like, when you… when you're trying to… so, as a human, if you… If you, like, have multiple options for use, but there are multiple stores in the workshop, you will chat with one system, and then based on the conversation, will you, you will decide to move to another system."}
{"speaker": "Interviewer", "text": "Yeah."}
{"speaker": "Participant", "text": "Yeah, so that will be the case, how humans will interact with that. So right now, what we want is, we want the another agent to have done that for us. But the question is, like, it's gonna be… it can raise the bias. Because I, I, I, I actually have an interview with, like, with, with Visa. for the internship. So, in that interview, like, we know that, like, in Visa. They have an issue about the bias. for the communication. What I mean by the bias is, like, instead of, like, when you chat with one agent, instead of it's refer to a better, store, and you will work with the agent in that store. So it just have a loop that you chat with that chat box, and it's the new, like, the next suggested agent. This rule, just that one. So it's gonna be a loop, yeah. So, I think, one of the ways that, Visa have sold this, I, I just got this during my interview, is that, like, they were trying to, generate, some… they, they would introduce some randomness. instead of, like, just, 100% believe in the next decision of the LLM, they just, based on… they just introduce some randomness so that it can refer to another agent. during the interaction."}
{"speaker": "Interviewer", "text": "But can't they just… Like, hard engineered to… Like… Have the agent not being able to choose like…"}
{"speaker": "Participant", "text": "Yeah."}
{"speaker": "Interviewer", "text": "self."}
{"speaker": "Participant", "text": "Yeah, yeah, yeah, I think that is one of the answers that, like, I already, I already answered during the interview, and they say, like, because it's… will be… have a lot of things there, especially, like, prom… like, prom, injection. So it still happens, even though we really designed a very careful, like, system prompt."}
{"speaker": "Interviewer", "text": "I see."}
{"speaker": "Participant", "text": "Yep. So, that's the first thing. So, like, basically, like, the, the bias during the… the bias during the, the, the… communication, that's the first thing that I already know in Visa. Another one is very relevant to what I have. Like, my research interest is about a prong, like, prone injection."}
{"speaker": "Interviewer", "text": "I see, I see."}
{"speaker": "Participant", "text": "The problem is, like, problem ingestion, like, we know that, like, it's a very challenging, vulnerability of the agent model. And I… and I feel like, like, the more agents that we have… so previously, we only had, like, one, LLM, one agent. And just a prime injection there, and we can prevent from that. But the more, like, you know that, like, when we have multiple agents, we will have a communication. And the communication is not just between them, between the agent, but rather we still have, like, all the external data during the interaction. So, the problem injection will be, like, the surface for the problem injection will be increased a lot. So that, it will be another issue compared to the, like, single agent, system before."}
{"speaker": "Interviewer", "text": "Yeah. Yeah, I guess… I'm not too familiar with safety, but… Since there's… there are more sources. So there's, like, more places to attack, kind of. Yeah. I see."}
{"speaker": "Participant", "text": "Yep. Actually, like, in my group, we have, like, one paper called, like, to compare the vulnerabilities between the single agent. And a multi-agent system."}
{"speaker": "Interviewer", "text": "Especially, like, compare the LLM."}
{"speaker": "Participant", "text": "and the LLM deployed in the web. environment, like, in the web environment. And we showed that, like, is the, the web agents will, like, the, the web… the LLM deploy in the web agent? we have, like, more than 40% vulnerability in the prompt injection, compared to the… compared to the single stack, like, single LM agent alone."}
{"speaker": "Interviewer", "text": "I see."}
{"speaker": "Participant", "text": "Wow."}
{"speaker": "Interviewer", "text": "Could you share the paper?"}
{"speaker": "Participant", "text": "Yeah, sure. Let me share with you two. So… Just give me a few minutes."}
{"speaker": "Interviewer", "text": "Also, that was the last question I had, so I can… Stick your time, I'll stop the recording."}
{"speaker": "Participant", "text": "Yeah, thanks. No."}
