{"speaker": "Interviewer", "text": "Okay. Okay, then I'll start. So I'll start off, with a pretty abstract question. Pretty big question. So, how… what is your definition of multi-agent system? And, what does it mean to you? So, like, how do you see it from… different from a single agent, or a single LLM?"}
{"speaker": "Participant", "text": "So, if we have the multi-agent… If we have energetic systems with multi-agents, and usually, these days, we put it into the space of, like, LLMs. Or… Or multi-modal language models. That can do all sorts of tasks, then these systems have… I have aspects like autonomy. To a certain degree, they should be deciding by themselves. They, are dynamic. In a way that they can handle all sorts of tasks. They're not restricted, not hard-coded, not hardwired to a certain set of problems, but they should be problem-solving, We should be able to do problem-solving tasks by themselves, without having a predefined set of problems to work on. then there's much more to it. Then there's a very big topic all about communication. In a way, how do these… Agents communicate. With the user, but also with each other's. How do you structure the… This leads to, like, questions on how to structure the software. Yeah. What kind of flows do you want to integrate? And then, there's much more to it. Then multi-agent systems are usually also designed in a way that they are… Specialized, in a way. So there's certain agents focusing on certain tasks. And this is very interesting these days, when there's lots of models coming up with certain specialities. So there's models that are good in reading tables, then there's other models that you use for image generation, and this way you can… have, very complex tasks being distributed among agents. But also, it comes with a lot of problems and, with a lot of questions. I try to be as vague as possible, because There's people that say, look, we should not have them too autonomous, we should have them automating things, and still having the user to decide, because as of now. There's… there's questions on who is taking responsibility. Of course, we… in the end, it's… it has to be a human, but the question is, like, how do we design a system? How do we abstract it in a way to have the best separation of concerns, to have the best abstraction level on where the user can still stay in the loop? Yes, so this is how I would try… To roughly describe it, and… This is not necessarily a definition. But I hope it helps."}
{"speaker": "Interviewer", "text": "No, no, it's fine, it's fine, it's just, your thoughts. Okay. Okay, then I'll… I'll move on. Could you tell me about one or two, projects that have used multi-agent systems that you worked on in the past?"}
{"speaker": "Participant", "text": "Yes. I worked on a system with multi-agents, where I was generating research reports. Let me think… we were using, on a technical site, a database. Right. And then we were integrating it with the Python server running Langraph, and this LangGraph was orchestrating Different agents for different tasks."}
{"speaker": "Interviewer", "text": "Yeah."}
{"speaker": "Participant", "text": "And then we had, in this case, We had them… Organized in a kind of deterministic way. And… If you want, I can go a little bit into the technical details on what we decided and how we implemented, if that's interesting."}
{"speaker": "Interviewer", "text": "Yeah, yeah, please go ahead. Also, like, what are the agents? What are their roles?"}
{"speaker": "Participant", "text": "Yes, okay. So… In the end, it… it came from the idea we want to have a very rough. User's description on a certain problem. And resulting in a sophisticated report."}
{"speaker": "Interviewer", "text": "But at the same time, it's also manageable by a user, because."}
{"speaker": "Participant", "text": "What often happens is you… put something in… if you just use an LLM for a certain type of interaction, you say, hey, look, I have… hear these questions and give me the response, then what comes out is a very sophisticated answer, but it's not very… It's very stateful, in a way. So we wanted to, we wanted to… Have all the information, all the assets, the artifacts that are being generated, to be persisted separately in a way that we can manage it, so that we can say, for a certain artifact, that the user can still manipulate certain things. Without having to re-run the whole conversation. How did we do this, or what is it about? So, consider our, our, our report generation, pipeline, where we had the initial query, which was then going through, APIs. for a, initial research on existing literature. In this case, I can give you a very precise description. It was a semantic scholar API. And this was something which was very helpful for this. for this one agent, which was overall having the role. We didn't talk so much in roles, we broke it down more into tasks and actions, and in this particular case, it was more about generate… like, getting the related work. And… It was doing it by having this query and translating this human user query into a search string, because often cases, you have databases that have a certain syntax in order for their querying, and agents are very helpful in translating… in translating, in general, but also translating a human language into a search query string for this semantic scholar database. We were also connecting, then. some other databases. We use different agents that were, like. specifically strong, and, with these certain… or they had the respective system prompts in the end with the respective documentation on the database's query string, which is very easy to set up, so…"}
{"speaker": "Interviewer", "text": "Yeah. This was a very easy, small agent, which was…"}
{"speaker": "Participant", "text": "basically taking over this, search. And then, when this search was going on, we persisted all this data. That was coming out into our database. to have our agentic system being kind of operational, so when it breaks down at a certain point, you know where did it stop, and where do we want to go on, and we kind of hardwired it into our… into our workflow in a way that we persisted, like, we were tracking all the agents, with tasks and actions, so we said this is our overall task, and this consists of different actions, and each action is persisted into the database, including state, pending. Like, if it's pending, if it's active, or whatever, also with the results, so we knew at any time what was happening. In terms of the process flow. And then, We were going on, after this first initial screening, we then had a, overall An agent which was giving a… overall strategy for the report, I would say, so it was more a… more a, strategic approach, where this one agent said, look, based on this information, we want to have a report which goes a little bit into this or that direction. It was kind of broad. But then we stopped, and then we had a different agent, and gave him the task in this particular direction. With this set of background information, please generate a set of stories. That we should, like, it was going a little bit more on a layer below, it was going more operational, more tactical, let's say. Let's say tactical. To make more precise storylines for this report. And then, finally, we were going one step further. to have, then, actual data manipulations. Then we had other agents that were supposed to solve, like, these kind of precise problems by generating code, you know? Then we had agents that were generating code for this one overall questions that was used to run on a… data set to then generate also visualizations. So we had another agent, which, after, like, we broke down our overall idea into smaller sub-components, and then this was generating code to generate actual artifacts, like a visualization that was fully interactive. And this way. we created, like, you can imagine, along the way, we had lots of different steps, which was… which were tracked on two ways. The one was… the one I already, explained. We had the, Actually, three ways. The one was we were tracking, like, what is the workflow like, which agents, which tasks, which actions are happening right now, what's going on."}
{"speaker": "Interviewer", "text": "What's the state of this workflow?"}
{"speaker": "Participant", "text": "Another thing we tracked was the results, so everything that came out, what that came back, we put into a separate table. And there you can see it was not that highly dynamic, in a way, because we had our fixed database schema, to have a kind of clear artifact that is coming out that the user can interact on. And then we, over time, created more and more artifacts, more and more entries for our database. In the end, it was rows for different tables that we created along the way, and when you assembled this together, you had a very nice-looking report on the front end, but the user was also able to edit these rows directly. So he can particularly say, hey, look, at this part. for this certain part of this, report, I want to change this full story. I want to replace it with another one, I want to remove it, or I want to regenerate parts of it, you know? Also, the strength… comes when you have multi-agent systems, that they are focused on a certain sub-step, that you can say, hey, look, this sub-step, this sub-process of this overall workflow, I want to regenerate in a way, that is… that it's fixed. And you can… and you do not have to rerun the whole conversation, you know? So, at any point of time, we want to."}
{"speaker": "Interviewer", "text": "This one from the… The agent that you want to intervene."}
{"speaker": "Participant", "text": "Yes, it's kind of stateless, you know, so we have… we will be put… Like, let's say, typical systems engineering approaches into the system. Into the agentic, how we… with agentic systems that… I believe we should reduce, generally, state as much as possible, and we kept all the state in our database, so at any point of time. We wanted our agentic system to be able to Do any particular task, again, even with completely different workflows. So, for example, when you put something in ChatGPT, then there might be a predefined workflow running in the background, whatever. And it goes from step 1 to step 10, and then it gives you the result. But if you say, hey, look, please only do step 5, but change the initial data and only do that, you cannot really do that, right? And we built a system that was then, yeah, persisting all these sub-steps. I don't know the exact number of agents that we had, but it was not, like, hundreds or something. As you can imagine, it was maybe… 15 different, calls, different agents that we had. We were using…"}
{"speaker": "Interviewer", "text": "That's still… that's still a lot. I think, by far, you probably had the most agents. Out of all my interviewees."}
{"speaker": "Participant", "text": "Yes, I mean, it also depends on how you define an agent, so for me, one agent is one specialized, system prompt, more or less, you know?"}
{"speaker": "Interviewer", "text": "Yeah, yeah."}
{"speaker": "Participant", "text": "So, And then… I mean, not… an agent doesn't have to be even an LLM interaction, right? It could be even… ideally, it's even something deterministic, which is much faster, much more reliable and stuff."}
{"speaker": "Interviewer", "text": "Yeah, yeah."}
{"speaker": "Participant", "text": "then, we were having our land graph, and as I said, we had this flow, which was very powerful in a way that we generated very nice data into our database. And we could even, to a certain degree, we were thinking about it, and we could do it at a later stage of time, but we weren't there at all to, Have another agent which is distributing the tasks among the others. But at this point, it was not even needed, because we can just have our line graph flow, our LangGraph node. like a state machine, you know? Like a state machine, it can distribute the tasks among the agents, and it was kind of predefinable."}
{"speaker": "Interviewer", "text": "So we predefined it."}
{"speaker": "Participant", "text": "And whenever you can predefine something, it's usually faster, safer, and easier to test."}
{"speaker": "Interviewer", "text": "Yeah."}
{"speaker": "Participant", "text": "Yeah, and this is one thing I would like… I'm talking a lot, I don't know if it's going into the right direction, but I want to give one more insight."}
{"speaker": "Interviewer", "text": "It is, it is."}
{"speaker": "Participant", "text": "Okay, so when working with agents and building agentic systems, it's… much more… unreliable to a certain degree than traditional programming, than traditional systems building, because you have to always test certain workflows, which are the… which are heavily depending on all the context, first of all. Which is hard to mock, you can do, but, still, you want it to be various, so it's very hard to mock it sophisticated… sufficiently for, like, a very solid testing. Then it's depending on a synchronous calls, so you're always talking to an external API, because you can, of course, run your local LLM, but they will be way too slow, and you usually don't have enough computing power, so we end up using OpenAI, Gemini, Cloud, or…"}
{"speaker": "Interviewer", "text": "Yeah. DeepSeek, whatever."}
{"speaker": "Participant", "text": "And they are slow, and you have their rate limitations? So when you always have to wait for their responses in order to test something. Of course, tokens can be expensive. In our case, it was not expensive when we used cheap models, and we wanted our system to be, like, only doing simple calls. As I said, we broke it down into many different agents, and when you do smaller calls, very precise context, very simple tasks, then the LLMs will be better at it. And then you can even do it with the smaller models. So we wanted our… if you build an agentic system, it's good to have it agent-agnostic. LLM agnostic, so you can, switch LLMs. And then, there's a third, or a fourth, problem, when you, when you develop it, because they're not deterministic, so you have to check what is happening with the output. And oftentimes, you can predefine a certain schema, and oftentimes it also works, but not always, and then you have to do lots of, Yeah, error handling… Loading state management. And… enormous amount of prompt engineering, I would say, like, fine-tuning your prompts. And this is… this is very interesting, in a way that you sometimes have to add a very random sentence like… Please be very careful with what you should."}
{"speaker": "Interviewer", "text": "Yeah, yeah."}
{"speaker": "Participant", "text": "And it changes the performance completely. Sometimes you have to take away a very logical sentence for the… like, it's… it's lots of trial and error, and what we used also to improve this over time, and I think this is… Also very critical, and… it's very hard to build agentic systems without it, is a proper telemetry LLM analysis tool, so in our case, we used an open source tool, which is called Langfuse. I don't know if you heard of it."}
{"speaker": "Interviewer", "text": "What does it do?"}
{"speaker": "Participant", "text": "And LengthViews is, a tool which tracks LLM calls. it's a cool… it's a tool that you can use to… you can integrate it even with Langgraph. It was very neat. You have… we had a… if you… if you stick to the frameworks properly, you can have very few lines of code, and your LangFuse is integrated with the length graph, and whenever there is an LLM call, you have your… you have a, you have a tracking of this… of this call. You see the input data, the output data, the duration it took, and, even the cost, if it's configured correctly, and you can see, really, the graph of your system on how things, interact with each other, so this is enormously… like, they're very helpful for optimizing the… Prompts."}
{"speaker": "Interviewer", "text": "I see."}
{"speaker": "Participant", "text": "If you want, I can, try to give you an example, and show it."}
{"speaker": "Interviewer", "text": "Oh yeah, yeah, please."}
{"speaker": "Participant", "text": "if I… if I should share my screen, but…"}
{"speaker": "Interviewer", "text": "Okay, I'll let you do that."}
{"speaker": "Participant", "text": "I have to try to dig it out right now."}
{"speaker": "Interviewer", "text": "Okay, you should be able to."}
{"speaker": "Participant", "text": "Yeah, let me try. Okay, let me do something else. I think I have a screenshot of it."}
{"speaker": "Interviewer", "text": "Yeah, a lot of other people also mentioned, prompt engineering or prompt refining. I had this one interviewee, she had an internship at Amazon, and then The internship was 3 months, and then she had to spend almost 2 entire months just, tweaking the prompts. It's a lot of trial… trial and error, She basically just have to, you know, write some system prompt, and then run the entire workflow, wait for some unexpected behavior to come up, and then just manually add that behavior back to the prompt, so that it doesn't happen. It's definitely one of the issues."}
{"speaker": "Participant", "text": "So… In this case, I can… I can show you this tool."}
{"speaker": "Interviewer", "text": "I see."}
{"speaker": "Participant", "text": "Because… because it's… because exactly for this, this is such an enormous amount of work. You need a good… you need to have a good tool. And in this case, we have… this was a different system now. Than the one that I… or a different workflow that I described, but… We see here a diff… we see in the back different traces."}
{"speaker": "Interviewer", "text": "Yep."}
{"speaker": "Participant", "text": "We see a list of… Kind of workflows? that we're running with an LLM. And… Then you can see the timestamps, so you can see these were all… On the same day, for example, and then… They are cert… they have certain names. And then you can zoom in into one. Again, like, I didn't find the right project right now in the system itself where it's hosted, but you can then see in this screenshot here still how it was used, and in this case, you see then this workflow, which has different steps. And… Then you have different prompts. For example, here you have something which says, extract and implicit… extract implicit knowledge. So in this case, there was a user interaction, and then we were… then this prompt… this is actually a great example. This prompt was… or this step of this overall agentic system was supposed to extract implicit domain knowledge on user interactions, so there was a… one information, and then the user changed this information to something else. So there was a difference. And this difference was done by a human user, and then we have an LLM, then we tracked all these differences, and then for each difference. we were running a separated, separate, LLM query to say, look. what is the difference, and what does it mean in our context? Because sometimes the user were maybe saying some information in this one spot, which can be also relevant for something else, and for such use case, we then wanted to know, what was the user's intent, maybe, so if we can find out something like that. And then we had our system prompt here on the right-hand side. You can see this is, like, a prompt that the AI… that the system was preparing. So we have here the original AI-generated content, and then the final user-edited content, and then we… Yeah, let the LLM judge it. And then we also persisted this information. And then later on, we can say, hey, look, this is the list of all the information that we extracted from user editing, and maybe we can condense it to a certain, knowledge that we can also apply somewhere else. And then you have, like, user added at some point, but you can, over time, optimize your whole system based on this information of this user. And, then we can see how we have a certain yeah, what configurations we had, so in this case, it was a very similar Jemi, Nye. model with a certain temperature, and this was working fine. We were thinking also about it, how do we do it? And it's also about, like, what kind of prompts do you have? And it's also, I said it before, it's about not messing up with the context of the LLM. So this was, in this case, it was just 3 edits, but still we have put it into 3 different… queries. We would have enough context to put everything. But we div into one single prompt, and we could have used a much bigger model to handle everything and stuff like that, but we found out that it's actually totally sufficient, and even… even… I would say even better, but you never know if it's really better, at least from my perspective. We found out that at this point of time, when we built the system with the LLMs and their capabilities back then, it was… from the, it was a very sufficient approach to split it down into very simple tasks, have multiple of them, run them in parallel, and have a very small model at this point, and we have enough information. This was sufficient, but again, like. maybe now the models are different, and you should handle it differently, but at this point of time, this was what we ended up with, and I was quite happy with the solution. Then you can see how we see the next step, where it was just some planning from the landgraph framework, and then there was another question, which was then… Running a little bit longer, you can see this has been, this has taken more time."}
{"speaker": "Interviewer", "text": "Yeah. She and…"}
{"speaker": "Participant", "text": "3 seconds, because this had a bigger task. But even this was running on a very small model. And then you can even see, like, how this, planner was shifting You know, was orchestrating these tasks, so it was going first to this task, then coming back, then to this task, and then ending it. But… the system I was describing before, which had many more steps for certain workflows, This had a much more… Complicated graph, which is, of course, cool to see, because it's auto-generated from this tool."}
{"speaker": "Interviewer", "text": "Yeah."}
{"speaker": "Participant", "text": "Yeah."}
{"speaker": "Interviewer", "text": "That's really cool."}
{"speaker": "Participant", "text": "Yes, and… and this is the… This is the tool we open source, we hosted this ourself, and… Yeah, you can do many, many things with it. It's a little bit difficult to set it up in the first place. But afterward, it's definitely worth it, depending on the scope of your project and Where was it? Yeah, here. And it comes with, with integrations for existing libraries, so in our case, we… We're using this one."}
{"speaker": "Interviewer", "text": "Yep."}
{"speaker": "Participant", "text": "And then it was… As I said, it was just a few lines of code here. And I think also here we had to… Provide the callback. And then it was fine, and here you see their screenshot."}
{"speaker": "Interviewer", "text": "Yeah."}
{"speaker": "Participant", "text": "of an example trace here, where… yes, it's the same thing. Then you can explore around, play with it. And here you can even see… the model… And, Yes, and if you configure it correctly, you can even have the prices for the LLM, so you see the cost. Here it is. Yeah, yeah. So you have the cost breakdown. It's very nice. So you can estimate. I hear you can even see how much the overall workflow cost. You see, this is what… This was the price of this. of this…"}
{"speaker": "Interviewer", "text": "Yeah, I see."}
{"speaker": "Participant", "text": "listing. There isn't… That's basically it. it from the lengthfuse, yeah."}
{"speaker": "Interviewer", "text": "Yeah."}
{"speaker": "Participant", "text": "Yeah."}
{"speaker": "Interviewer", "text": "So this is, like, your, interface, basically, to… for you to monitor the entire workflow."}
{"speaker": "Participant", "text": "Good question. So… Yes, it's good… it's… it's also once you're… once you're finished with the development, this is also good for monitoring, but you even use it during the development process."}
{"speaker": "Interviewer", "text": "I see."}
{"speaker": "Participant", "text": "Yes. But this is a good point, because you're not finished developing your agent tick systems. Once you're finished, they are going into production, and then when you want to operate them into a real-world scenario, you have to constantly monitor what the LLMs are doing, and you have to put certain metrics into it to find out if there's problems with it, because it's hard to test, you know? I'm always… I'm always seeing a risk there if you have an external system connected to a running application. That this external system might change its behavior, and this is… can be especially… true for… LLMs if you don't set them up carefully. So if you just set them up in a very easy way, and you don't think about it at all, what could happen is that from one day to another, maybe they shut down, and the company does not provide the API access anymore, your token expires, or, Or, even worse, or even more hard to track, and this is where these tools are very helpful for, that the AI maybe develops a certain bias. All these LLMs are biased, and you never know how, or you should know how. And, you have to work with these biases, right?"}
{"speaker": "Interviewer", "text": "Yeah. Yo. So… so even with this tool, what, like, what type of, what type of errors, come up the most often? During a development."}
{"speaker": "Participant", "text": "Wow, this is a great question. there's… there's very… very many types of errors, and I… do not have the right number for this, I can just give you my… my feeling about it, which is that there is, of course. sometimes…"}
{"speaker": "Interviewer", "text": "I'll just say there's…"}
{"speaker": "Participant", "text": "When things run in parallel. race conditions you might have. So you have to set up your system in a way, Which… which is that… one LLM prompt takes longer than the other one, and then you have inconsistencies in your process flow, which is a programming mistake, it's a programming error, and it's much easier to make programming errors with LLMs, but it's not an error which is happening inside the LLM, or with the data that comes out, but more from the nature that the API response just… maybe takes longer than usual, and then you break other things in the system. Other than that, you often… in agentic systems, you predefine a schema, and you predefine the response schema of the prompt that you put into the system that you say, for example, in our case, I just said, give me valid Python code to generate a A, visualization, or in this case, what we just saw, we had… we had a problem where we say, give us structured information on what knowledge might be extracted from this user interaction. So you need to have a certain data format in order to process it a bit further. Depends on what you do, but in our case, we usually put it into a database where we have our database schema, which we have to stick to, because we used a relational database. But also, if you create a… if you use it to generate code that you then execute, you also have to have valid Python code that you can… or whatever kind of programming language, but you have to have valid code that is syntactically correct, and hopefully also semantically, in order to be even able to execute it. And then, For other queries, for other workflows, you might not have such a strict Syntax requirements or formatting requirements, where you, for example, have one result that you just put into another LLM, and LLMs, they can't understand very much. So you can also just give them plain text, but the database usually cannot understand very much. They needed to have it in a certain schema, and… This is… this is one set of errors that comes out, which is basically the response formatting, and sometimes the responses also include certain type of, Moderation. And, and you say, just give me out the plain JSON. And oftentimes it works, but sometimes it still says, here's the JSON, and then it gives you the JSON. And the problem is, is here's the JSON, it's not JSON, so already it breaks there."}
{"speaker": "Interviewer", "text": "Yeah. There's a… there… you can provide schemas for that, you can provide a certain output schema that you expect to catch this error, and then it's usually fine."}
{"speaker": "Participant", "text": "Or then it… it can be… you can iterate on it, you know? Then you can validate it deterministically, okay, this output data has… matches our schema, or it compiles, or whatsoever, you can do this deterministically, and then you can rerun the prompt and give it… and then after looping a little bit, you can hopefully fix it. And this you can track, for example, how often you had to fix until you see, oh, this is maybe very unreliable in returning the results, and just in terms of formatting. Content-wise, it's a completely different topic, because Content-wise, I have faced two other errors, that I also want to mention. that are especially important for this particular use case. So, there was… when you generate code, or when you say… tell to the LLM, generate code, and then you execute it during runtime, this can lead to crazy stuff. If you have, then, code which is not performing well, you iterate on the LLM and please refine the code that you then use to generate a certain artifact. on the… during runtime, this, of course, is highly, highly risky for many, many reasons, because, you can introduce all kinds of errors, you can introduce all kinds of security gaps that we might not know about. Of course, you can't say, execute this code safely. But who would… who would prove… who would tell you that you can completely, safely execute a code in your application coming from an LLM without introducing any security issues? And then another thing that can happen is, that the code is just incredibly slow or unperformed. So, for example, why… I mean, why do we use it in the first place? We use this code generation if we want to have a deterministic calculation. For example, I had a long, long list of data. I had 10,000 rows of all kinds of numbers, and I want to generate a visualization, or I want to do some data science with it. And then you can ask the LLM, give me code that does this analysis, or that creates a certain chart. Because you cannot usually… you should not ask the LLM, give me this chart directly, because, first of all, it might be too much for the context window if you give it the raw data, and secondly, it hallucinates. So if… it can fail very easily, so it's usually more reliable to go one step in between and say, look, this is the data, this is the schema of the data, I want to have this and this result, please build me the software code that generates this result from this data. And this is what the LLM is usually more performant with, and this might be a use case for generating code, and then what happens is this code generation maybe has recursion while statements whatsoever, and then you have endless loops within LLM-generated code, which, is very, very hard to debug, because you, of course, can put log statements or tracking in your own software, but if you run third-party software coming from an LLM, how should you track that? So…"}
{"speaker": "Interviewer", "text": "Good."}
{"speaker": "Participant", "text": "by definition, it's maybe even a bad idea to do it at all, but, I mean, at the same time, it's a very powerful use cases that we can only do because of, like, Gentex systems, right?"}
{"speaker": "Interviewer", "text": "I want to give you a third problem that I faced. Okay. Go ahead."}
{"speaker": "Participant", "text": "Hmm… And then, and then I've… I've said everything from the top of my head about this one, which is sometimes you give prompts in a way with a certain context. So, sometimes you want the LLM to exclude its own domain knowledge. Or its… its own model knowledge. LLMs are very smart, they know already lots of things, but maybe you have a… maybe you have a certain set of information. And you have a certain… biography about a certain person or whatsoever, which might be incomplete. Maybe you have the first 20 years of Albert Einstein or whatsoever, and then you ask the LLM, where did Albert Einstein go to school, or whatsoever? And then the LLM, it reads this text. And then it tells you, hey, look, he went to this primary school, this secondary school, this high school, and then it also tells you he also went to this university, and he later on was professor here and there. But it was obviously not in the first 20 years, but the LLM still knows it, because in this case, Albert Einstein, the example, was a very popular person. And then what you can do is you can tell the LLM, only refer to this knowledge inside of this text, don't add any other information to it. And then, it rarely works."}
{"speaker": "Interviewer", "text": "Really? Oh, okay. Yeah, like, prompting is probably the only way, right? There's no other way you could, manipulate, like. Whether you should refer to its internal knowledge or not."}
{"speaker": "Participant", "text": "Exactly. So when you ask a question about some knowledge that the LLM already has, it's very hard to have the LLM not using it. You know, they like to use their own knowledge."}
{"speaker": "Interviewer", "text": "That's interesting."}
{"speaker": "Participant", "text": "Yes."}
{"speaker": "Interviewer", "text": "Yeah. Speaking of this, so one of my other interviewees told me about this framework. It's called, I do not know how to pronounce it. But it's… Okay, so it's a framework for prompts? optimization of… I don't know if that's the right, right, term. But basically, it's, Instead of writing the prompt, the long, super long system prompt, the string yourself, you define the input and output, and then it… and you also provide some few-shot examples. And, it runs this prompt optimization algorithm to figure out the best system prompt."}
{"speaker": "Participant", "text": "for you."}
{"speaker": "Interviewer", "text": "It works well, or I think it works well for some of the cases where you have a very clear… let's say you define a schema of input and output. It works well in those scenarios, but it may not still solve the issue that you just mentioned. But I… I checked out the… I didn't fully re… Like, read the paper, But… I think it can be useful for some… for some use cases."}
{"speaker": "Participant", "text": "Yes. It seems interesting. I mean… this… I don't know when it was created, but I didn't see it yet, and there's so much… iteration. On this overall topic. So, for example, something like LangFuse, when I built the first Agentic workflow, it was… 2020… 2. or 2023, a few years ago, something like this, like Langchain, Langgraph and stuff, this didn't even exist, I think. Or I didn't even know of it back then, and now there's LangFuse, which makes it so much easier. Now, apparently, these tools they seem… it seems very promising. I mean, I don't know how much it takes in terms of training. To… to get out the… Right, promo."}
{"speaker": "Interviewer", "text": "I think it… it doesn't take… A lot of training, that's why it's useful. It also does… don't need a lot of data, so…"}
{"speaker": "Participant", "text": "Hmm."}
{"speaker": "Interviewer", "text": "But I haven't tried it out, myself yet. Okay, I want to ask you a question. It might not have an answer, but I feel like a lot of the people who have experience in developing multi-agent systems are struggling with just system prompts, because, it's so… There are just… there can be so many issues that come with it, because the use case scenarios are also, like, very different. Like, what do you think can help? I think it's, like, theoretically very, like, difficult because, you always just have to run the model or, like, run the workflow in order to know the results. And there's kind of just no way of, like. Like, forecasting the results when you're still writing that prompt."}
{"speaker": "Participant", "text": "Yeah, so… I mean… This is also… so this is definitely a big challenge that we have. how do we do… how do we come to the right system from… there might be, Already lots of research on that. But from my experience, it was oftentimes a trial and error, and this is why I was active… why we are actively seeking and building tools like LangFuse, and this is not the only one. There's lots of other tracking tools, that integrate this kind of LLM telemetry. And, observability of, yeah, LLM interactions."}
{"speaker": "Interviewer", "text": "Yeah."}
{"speaker": "Participant", "text": "Just because there is such a big need, and lots of people from all sorts of directions struggle to build the right system prompts, and… For me, it oftentimes was kind of a trial and error approach. Oftentimes, I ask LLMs themselves, hey, look, I have this goal, what is a good system prompt? And then I take it from there and optimize it step by step. And, as I said, it's kind of trial and error-ish, and it takes a while, because you iterate slow. LLMs are slow, then you have various different use cases that you need to find out. It also takes a long time to find out if it is actually good. Because you cannot just run a unit test on it. Or you can, maybe, I don't know, depends on the question, on the separate prompt, but usually you run this prompt, then you wait for the result, and then you check it. And then you have to understand, first of all, is it now good, or is it what I want or not? Which is also not an easy question, to know if this answer is good, because usually you Sometimes you process it further, so… Then there… there are certain guidelines on how to build system prompts. But also, I feel that these have changed quite recently, so over time, some of the latest Technologies in this overall space, Seemed to be only for short-term, and very, like… Not final in a way, so that they're just, like. steps, but not the final best approach, not the final best practice, I think, some of these steps. So, for example, we have it's a different topic, it's not about building system problems, but we had this MCP, concept, which was, supposed to be the way to go. And now, a few weeks ago, I think from Entropic themselves, if I'm not wrong, there came out a paper which says, look, we have now a better idea, and MCPs have problems, and MCPs have problems with context, and… And stuff like that. And now they have a new idea on how to do it better. So, maybe in two years, it's a completely different approach, and also when it comes to building system prompts. Also, this might change so fast, so it's hard to find resources where you know this is something I can trust. So it's hard to find, like, trustworthy and easy and scalable and efficient approaches to build system problems, I think."}
{"speaker": "Interviewer", "text": "Yeah, yeah. And I think, even… even prompt engineering died out, because the… the models are… Way more capable now. I was talking to my friend. About this, and he told me that, he's working in the industry, and he told me that, like, his company had a couple of prompt engineer positions. it's not a meme, but, like, they actually had those positions, and now, they're gone. Yeah."}
{"speaker": "Participant", "text": "Yeah. Yeah, it's a very fast-changing, environment, and… And even in industry, like, in big corporates, They are just… Trying. at least from what I hear from the people that I know from industry, and… and there's interesting examples how they literally put something like, please be very, very careful with this prompt and do your very best to make it work, which goes into production. So, energetic systems, there are those kind of prompts out there. To optimize, and… And it's a lot of trial and error also in industry, or especially in industry."}
{"speaker": "Interviewer", "text": "Okay, so… Apart from, trial and error, so I think for a lot of Like I mentioned before, so your project actually has the most number of agents that I have. seen so far. A lot of other people, they have less agents, but each agent might, just get a much longer system prompt. They have, like, many tool costs that each agent can do. So, the system prompt itself ended up being super long. And I'm thinking, so… if you're… if you're a software engineer, and then you're… you're building the system, you're… you're editing the system from inside of whatever IDE that you're using, and… Do you think… if… if we have, like, a visual structure, because I feel like system prompts for a multi-agent system, they often come with some structure in it. For example, you always define the input and output, and maybe some context, and maybe some other data that the agent should have access to. Do you feel like if there is some visual structure or a visual editing interface for System Prompt, it'll be helpful?"}
{"speaker": "Participant", "text": "Yes, in general, it needs… Also on the technical side, when you're a system builder. A certain type of… Let's say prompt management. By prompt management, I mean, that you should separate your prompts from the code, in a way that you have them stored into… inside of a different database or whatsoever. Like, you should have a… you should have a folder of all the system prompts to see, because when you… you don't want to run a new deployment whenever your prompts change. Maybe, as I said, there's something very urgently, then you want to fix them, so whenever somebody… the next person runs the query, you run the prompt automatically, because they are stored in a certain type of database, and this also led me to the question on how… how should you handle it? Because prompt management, you can even do it with LangViews, for example."}
{"speaker": "Interviewer", "text": "Hmm."}
{"speaker": "Participant", "text": "But even there, it was… Kind of difficult to understand it a little bit, and it would have been… I mean, I'm just thinking out loud, maybe it would be nice."}
{"speaker": "Interviewer", "text": "Yes, yeah."}
{"speaker": "Participant", "text": "a centralized, space of all the system prompts that you're using for different systems, and having this somehow integrated with the tracing, so that when you see there is a certain type of, errors always happening in certain prompts, that they are then highlighted in a certain way, so you know this is something where I have to take action, or that you… And you asked it before, to have certain error classes that we have for certain prompts, so that we say, for example, the input data oftentimes, or the data formatting is oftentimes very wrong, so we can assume the output part of the system prompt, because you're right, there are certain structures for prompts that we can maybe highlight the output part might be worth optimizing, or when we say the prompt always results in a very wrong answer, which just goes in a completely different direction. We can maybe assume that maybe in the beginning of the prompt, there might be something wrong, and then we highlight it, and then we can, give maybe the prompt Engineers, if such thing exists, we can give them some indication, look, your intro is too long, your output definition is too long, the context definition might be incomplete, and… And this way, it would be a very nice approach, as always, to try to break down things, trying to break down the system prompts into different sub-components, and optimizing them one by one, because as I said, we had this overall very complex idea, and I think it was a good idea to have many small prompts many small agents. to run as independently as possible on a very simple task. Of course, it's more easy and more difficult to set it up like this, when you have to program all these kind of things. But it led to better performance and better results. And I think the same is true for so many problems that we face, just in general. We have to break down things, divide and conquer, and I think same… we should try for system prompts, but so far I… I only see, like, this… this one system prompt. Yeah. Full stop. And now there's one full system prompt with 1, 2, 3, 4, 5 sub-components, or whatever."}
{"speaker": "Interviewer", "text": "Yup. And also, like, the tracing error back to the, like, which part of the prompt that you just mentioned, I was thinking about this in my head, and I feel like… is LLM… like, LLM probably, like, the only solution to do that? But again, we're having another LLM, as, like, the… the verifier, I guess, or the debugger."}
{"speaker": "Participant", "text": "Yes, I mean, you… you cannot do it very precisely, so you… I mean, there will be… I don't even have an overview of all types of errors that we will face with LLMs, and I would be very interesting to see all the types of errors. For, like, traditional programming, you can say, look, there's a stack overflow error, there's a null pointer exception, there's a whatever kind of exception. But for LLMs, I think it's much more difficult to name all of these, and then to be aware of it. And what you could do is to slowly get there. To slowly get there, for example, by having a deterministic check. Afterwards, as I said, you can have a schema validation, or you can run deterministic code, which maybe just gives you some data on it. The amount of token that was spent, or the runtime that it took, and then you can optimize from there so that you can… measure this kind of information, and then you can maybe see over time, like. That this metadata, for example, like, output length, input length, and time it took token consumption. These are just very easy numbers to get, but then you can go one step further, and for… depending on the query that you have, run further validations. Like, for a structured JSON, you can… of course, first of all, you can run the schema validation, but maybe later on, at a certain point, you can also… Collect some performance information about the content quality."}
{"speaker": "Interviewer", "text": "Yeah."}
{"speaker": "Participant", "text": "so for… And then what you can also do is you can ask the LLM itself to generate some metadata. So, I was sometimes asking LLMs to categorize certain elements that they gave out. Right? And then you can use these as tags so… I think, in general. You will have to have another layer. you will have to have another layer in between. So I think it's hard to just have the LLM calls, and then the, monitoring calls of the system prompts, you know? I think you need to have one abstraction layer that somehow transforms all of this output into information on how you can improve system prompts."}
{"speaker": "Interviewer", "text": "Yeah. If that makes sense."}
{"speaker": "Participant", "text": "Sounds a little bit abstract."}
{"speaker": "Interviewer", "text": "Yep. I guess, yeah, the reason why we're having this interview is just that, we're trying to see, like, the problems that developers are having. And I just kept thinking about system problems, because it's always mentioned by people, and, just trying to see, like, what we can do about it."}
{"speaker": "Participant", "text": "Yes. Yes, it's about system prompts, and then it's also about… It's also a little bit about knowing, like, how to… divide them, right? So, of course, you can say a system prompts, how to build a single system prompt, but also about the system prompt intent. What's my overall goal with the system prompt? So. Should it be one prompt? Or should it be more? I also gave you the little show when I gave you this LangFuse, where I said, look, we have put it into different prompts, but maybe sometimes it makes sense who have many, many more, or maybe you can even combine completely different ones. Also, this is something that is always a question which is hard to say yes or no to. It's always, let's try to separate it. It feels more natural. But you never… yeah. And then, of course, like, building system prompts, all this telemetry and performance monitoring, and then also another challenge is knowing which model to use. Maybe also, like, having some kind of abstraction layer of saying. I just give you my prompt, Or my goal… As a system, and this system then chooses how to, which model to use. Because…"}
{"speaker": "Interviewer", "text": "Yeah."}
{"speaker": "Participant", "text": "to use, because now I said, yeah, we just use this model because it's small, and our tasks are simple, and we don't want to think about it so much. And every day there is a new model. Today, there was something I saw on any kind of benchmarks, there was Grok 1 Fast or something, I never heard it before. Code 1 fast or something, I don't know, and Maybe it's good, maybe it's better, and then you can have a self-optimizing system, Without having to spend so much efforts, because… In general, building agentic systems, it's very tedious."}
{"speaker": "Interviewer", "text": "It's very tedious, yeah."}
{"speaker": "Participant", "text": "It's very tedious, and there's…"}
{"speaker": "Interviewer", "text": "Yeah, just all sorts of different problems. I think… another person also made this analogy. He was saying how, like, agents or, like, the nodes in multi-agent systems. It's just, like… like you talk about, like, divide and conquer is, they… they kind of… they're kind of just, like, the functions that we used to have before in programming, but they're… they're more intelligent. And more, like… they fail, they fail more often because of its… because it's LLMs, they're non-deterministic sometimes."}
{"speaker": "Participant", "text": "Hmm."}
{"speaker": "Interviewer", "text": "And they run into all kinds of Earth."}
{"speaker": "Participant", "text": "Yes, yes, and this is true, and… I sometimes even try to think of how can we design our prompts and our system prompts, and our agents in a way that certain things… certain things we don't know yet about. But if we use these prompts in a way. that we can generate knowledge to solve this problem deterministically without using an LLM in the first place. I think it's always the main goal. So, I always… I would love a world where we have these agents, or LLM-based agents, just as an… as a step. that we use them as a step right now, because if we would solve this problem, like, with code, it would take too long, or it's certificate, or it doesn't work, so we use an LLM. But let's see if there are certain parts of this overall problem that the LLM maybe can be… get rid of, and we… Somehow transpose it into deterministic programming, so we become faster and more reliable."}
{"speaker": "Interviewer", "text": "Yeah. So, I think…"}
{"speaker": "Participant", "text": "I think this is why, what he says, they are very intelligent, and this is why we love it to just… okay, we don't solve this problem now programmatically, we just use an LLM, and we just use it as a function that just can do everything. But it would be nice to somehow always also see if there's something that we can take away from the LLM and put it into a traditional function, because I think they are still better. If they… if, I mean, they have a limited scope. But if we find these scopes for them, they will always be outperformed. I mean, using the calculator, 1 plus 1 will always outperform asking ChatGPT 1 plus 1."}
{"speaker": "Interviewer", "text": "Yeah, definitely."}
{"speaker": "Participant", "text": "Yeah."}
{"speaker": "Interviewer", "text": "Okay, I think we're going over time, but you definitely shared a lot of insights. That was… that was super helpful. Okay, I'll stop the recording just for now."}
