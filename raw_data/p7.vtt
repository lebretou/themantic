WEBVTT

1
00:00:01.100 --> 00:00:02.070
Zhongzheng Xu: Okay.

2
00:00:02.720 --> 00:00:13.800
Zhongzheng Xu: Alright, let's go ahead and start. I'm gonna start off with a pretty abstract question. So, what is your definition of a multi-agent system?

3
00:00:14.710 --> 00:00:29.710
Sy Tuyen Ho: So, to me, the multi-agent system is the system where, we ask… like, where we, like, just input the system with a task that we want the system to… to done that for us.

4
00:00:29.760 --> 00:00:37.229
Sy Tuyen Ho: And the system will do that autonomously, by the interaction between multiple agents.

5
00:00:37.630 --> 00:00:45.300
Sy Tuyen Ho: So that it can do the task autonomously, but, with the interaction between multiple agents.

6
00:00:45.300 --> 00:00:52.420
Zhongzheng Xu: Okay. And can you tell me about one or two,

7
00:00:52.530 --> 00:00:57.199
Zhongzheng Xu: Representative projects that have used multi-agent system, in the past.

8
00:00:57.730 --> 00:01:11.799
Sy Tuyen Ho: Okay, the very… the very recent, project that I have done for… with the multi-agents is, a system, like, an agent that I wanted to… I wanted to do…

9
00:01:12.040 --> 00:01:13.780
Sy Tuyen Ho: to, like,

10
00:01:14.270 --> 00:01:30.190
Sy Tuyen Ho: somewhat, like, create the… because I am a PhD student, so what I want is, like, I want the… I want to create, the system that can help me with the literature review, given a topic that I want to…

11
00:01:30.310 --> 00:01:37.220
Sy Tuyen Ho: That I want… that I want to do. So, I developed a system that do that. Given the…

12
00:01:37.330 --> 00:01:43.309
Sy Tuyen Ho: Given the topic, and it will autonomously, like, autonomously,

13
00:01:43.550 --> 00:01:47.100
Sy Tuyen Ho: crawl all of the papers that are relevant on archive.

14
00:01:47.200 --> 00:01:52.680
Sy Tuyen Ho: And analyze those papers, and generate the, like.

15
00:01:52.990 --> 00:01:59.020
Sy Tuyen Ho: the literature review, so that I can base on that and work from that for my own literature review.

16
00:01:59.560 --> 00:02:02.140
Zhongzheng Xu: I see, I see. So…

17
00:02:02.720 --> 00:02:11.340
Zhongzheng Xu: How many agents are there? You said that there's a web crawling agent, probably, like, another summary agent?

18
00:02:11.710 --> 00:02:25.379
Sy Tuyen Ho: Yep. So, like, I will start with, a web agent, like, to crawl all of the papers, so that is the first agent that I… that I need. Second one, like, I will use the LN Power agent.

19
00:02:25.530 --> 00:02:36.760
Sy Tuyen Ho: to read all of the abstract, because we know that, like, like, I asked them to read all of the abstract, and I asked them to generate some tags.

20
00:02:37.120 --> 00:02:42.130
Sy Tuyen Ho: And from those tasks, I will decide whether I will keep this paper or not.

21
00:02:42.730 --> 00:02:45.479
Sy Tuyen Ho: So after all of that, I will…

22
00:02:45.580 --> 00:03:03.870
Sy Tuyen Ho: have, sort of, like, papers that might be relevant to the topic that I know. But of course, like, because those, filtering is just based on the abstract. So it's not, we cannot 100% sure that it's gonna be relevant. So after all of that, I will…

23
00:03:03.920 --> 00:03:09.100
Sy Tuyen Ho: After having, like, list up the potential relevant, papers.

24
00:03:09.350 --> 00:03:16.350
Sy Tuyen Ho: I will try to let another agent to read all of the content in the paper.

25
00:03:16.490 --> 00:03:28.709
Sy Tuyen Ho: And then decide if we still want to keep this paper or not. And after all of that, after all of that, I will need another agent to verify all of the information that I already have.

26
00:03:29.220 --> 00:03:29.650
Zhongzheng Xu: Okay.

27
00:03:29.650 --> 00:03:42.379
Sy Tuyen Ho: Yeah, like, and, and ultimately, we still need a human to verify all of these information, so I maybe call it, like, human agent in the very final step.

28
00:03:42.800 --> 00:03:44.339
Zhongzheng Xu: I see, I see.

29
00:03:44.850 --> 00:03:48.920
Zhongzheng Xu: And then you'll be, like, the human that's doing the check-in.

30
00:03:48.920 --> 00:03:49.870
Sy Tuyen Ho: Yep, yep.

31
00:03:50.180 --> 00:03:50.590
Zhongzheng Xu: Okay.

32
00:03:50.590 --> 00:03:53.169
Sy Tuyen Ho: Me and, like, my, my, my teammate.

33
00:03:53.600 --> 00:03:58.949
Zhongzheng Xu: I see. Did you use any framework, for this pipeline that you just described?

34
00:03:59.240 --> 00:04:01.490
Zhongzheng Xu: Like, Lang Cheng or Lang Broth?

35
00:04:02.160 --> 00:04:12.240
Sy Tuyen Ho: Yeah, I initially tried to use LenChen, but at the end of the day, I feel like it's too simple to use LenChen, so I just built up from scratch.

36
00:04:12.240 --> 00:04:14.200
Zhongzheng Xu: Just built that from scratch. I see.

37
00:04:14.320 --> 00:04:18.660
Zhongzheng Xu: And then… so, when you…

38
00:04:18.990 --> 00:04:28.309
Sy Tuyen Ho: Oh, by the way, like, I wanted to know that, like, from the code, like, from the coding perspective, I built that from scratch.

39
00:04:28.420 --> 00:04:40.250
Sy Tuyen Ho: But, for the database, I use a very simple thing. It's… I use Notion database to save everything, and, and because Notion is very, intuitive.

40
00:04:40.380 --> 00:04:57.679
Sy Tuyen Ho: So I use, I use Notion, for the, like, because, like, Notion is simple, simple to set up, and also, like, it's… it's very intuitive, and, like, it can have a very good visualization for human, check… human verification step later.

41
00:04:58.180 --> 00:04:59.020
Zhongzheng Xu: Hmm.

42
00:04:59.120 --> 00:05:04.380
Zhongzheng Xu: I actually haven't used Notion, so… The database?

43
00:05:04.650 --> 00:05:08.400
Zhongzheng Xu: means that you store all the paper in Notion?

44
00:05:08.720 --> 00:05:16.939
Sy Tuyen Ho: Yep. Store of, like, information, and store all of the steps, right? Like, I just told you before, like, there are multiple steps, right?

45
00:05:17.390 --> 00:05:35.360
Sy Tuyen Ho: the abstract analysis, and then, like, the full paper analysis, and human verification. So for each step, I need to save everything, and those information will be saved in the Notion database, with multiple columns to.

46
00:05:35.360 --> 00:05:35.840
Zhongzheng Xu: Yep.

47
00:05:35.840 --> 00:05:52.739
Sy Tuyen Ho: to, like, to let you know that we already done the step one, and right now we are in the step two and step three, so that would be easier for us to verify, like, verify later, because I think the human verification is very imp… is very important in this project.

48
00:05:53.080 --> 00:06:01.679
Sy Tuyen Ho: Because, we know that, like, LM can do LM or multiple… multi-agent system can do every… can do this very, very well.

49
00:06:01.740 --> 00:06:21.719
Sy Tuyen Ho: That can save a lot of time for us, but at the end of the day, it's still the research. We need to verify, because we know that, like, recently, there is a lot of, like, literature reviews that are out there, but they are… they can generate some fake, like, generate, like, some fake citation. So, human verification is very important.

50
00:06:21.830 --> 00:06:35.479
Sy Tuyen Ho: And Notion Database is one of the… like, in our project, we find that Notion Database is one of the best ways for us to help with the verification of human verification.

51
00:06:36.030 --> 00:06:49.289
Zhongzheng Xu: So, for this human verification, so, did you have this idea at the very beginning, or did you not have this and try it and saw that LLM can still generate fake content, so you decided to add this?

52
00:06:49.720 --> 00:07:02.429
Sy Tuyen Ho: So actually, like, we… we… we keep that in mind on a, in a very… in a very first step when we build this project, because, so, so, because, like.

53
00:07:02.490 --> 00:07:10.229
Sy Tuyen Ho: the very, like, ultimate goal in this project is to create the LM

54
00:07:10.400 --> 00:07:19.339
Sy Tuyen Ho: based, like, an LM, a power agent that can help us generate the literature review that.

55
00:07:19.340 --> 00:07:20.010
Zhongzheng Xu: Yep.

56
00:07:20.010 --> 00:07:22.540
Sy Tuyen Ho: That is, like, reliable.

57
00:07:23.250 --> 00:07:27.079
Sy Tuyen Ho: But we know that, like, a lot of, like, our imaging can be hallucination.

58
00:07:27.240 --> 00:07:35.440
Sy Tuyen Ho: So that's why we keep that in mind and build that from scratch, with the human verification, keep in mind.

59
00:07:36.350 --> 00:07:55.569
Zhongzheng Xu: So when you're… when you were developing, what was, like, the… did you encounter any challenges, or what was, like, the part that you kind of have to trial and error? For example, like, the system prompts, or, like, just communication between agents?

60
00:07:55.900 --> 00:08:02.280
Sy Tuyen Ho: Okay, so, because we, in our process, we were trying to make it,

61
00:08:02.590 --> 00:08:08.569
Sy Tuyen Ho: The most transparent as possible, because we want to…

62
00:08:08.580 --> 00:08:14.070
Zhongzheng Xu: We want the, the, the reliable, generated literature review,

63
00:08:14.070 --> 00:08:17.459
Sy Tuyen Ho: you cannot leave. So, that is… so…

64
00:08:17.710 --> 00:08:22.309
Sy Tuyen Ho: We have to make it transparent enough, but when we develop it.

65
00:08:22.750 --> 00:08:30.349
Sy Tuyen Ho: The thing that we need, like, we feel like is… is the most difficult is about how to

66
00:08:30.640 --> 00:08:32.980
Sy Tuyen Ho: Design the threshold.

67
00:08:33.190 --> 00:08:35.300
Sy Tuyen Ho: For the… for each step.

68
00:08:35.510 --> 00:08:40.969
Sy Tuyen Ho: So, for example, so as I told you before, the very first step is, like, to

69
00:08:41.900 --> 00:08:53.460
Sy Tuyen Ho: to use an, web, like, web crawling, right, to web crawling to crawl all of the paper that's relevant. So, the very first step, like, we need to define what

70
00:08:53.620 --> 00:08:58.830
Sy Tuyen Ho: Keywords we need to, to, to input to the web crawler.

71
00:08:58.940 --> 00:09:07.940
Sy Tuyen Ho: And even though, like, we set a lot of them, or maybe if we… if we just keep the, like, for example, like, I…

72
00:09:08.160 --> 00:09:14.069
Sy Tuyen Ho: the topic that I, I, I worked on is scalable oversight.

73
00:09:14.820 --> 00:09:21.360
Sy Tuyen Ho: So, if I just import the keyword Scala Oversight, it will give me a very relevant paper.

74
00:09:21.510 --> 00:09:41.060
Sy Tuyen Ho: But might not enough, like, so the precision is very high, but the recall is very low. Right. So that's why, like, we want to include a lot of, like, keywords, but then, like, the recall will be very high, but the precision is very low. So that's a trade-off that we need to work on.

75
00:09:41.430 --> 00:09:43.009
Sy Tuyen Ho: So,

76
00:09:43.120 --> 00:09:51.879
Sy Tuyen Ho: And also, another thing is, like, when we… we also use ROM to analyze the abstract, or even the full paper.

77
00:09:52.160 --> 00:09:59.179
Sy Tuyen Ho: So, when we analyze those, them, we need… so, because we know that LM can be hallucinated.

78
00:09:59.520 --> 00:10:00.110
Zhongzheng Xu: Right.

79
00:10:00.110 --> 00:10:07.979
Sy Tuyen Ho: So, we need to develop a way to remove, or to at least, like, reduce the hallucination.

80
00:10:08.570 --> 00:10:13.499
Sy Tuyen Ho: Through, like, engineering, and so on.

81
00:10:13.610 --> 00:10:17.160
Sy Tuyen Ho: And the last thing that we faced in this project.

82
00:10:17.160 --> 00:10:18.709
Zhongzheng Xu: Sorry, Owen. 13.

83
00:10:18.710 --> 00:10:19.290
Sy Tuyen Ho: Divia.

84
00:10:19.290 --> 00:10:24.419
Zhongzheng Xu: So you said prompt engineering, could you also, like, explain what type of engineering that you did?

85
00:10:24.780 --> 00:10:39.320
Sy Tuyen Ho: Okay, I think it's very, simple. It's just based on our, observation. So, what we did is, like, we, in our process, what we're trying to do is, like,

86
00:10:39.430 --> 00:10:45.430
Sy Tuyen Ho: we use an LM to read, like, to read, say, the abstract.

87
00:10:45.430 --> 00:10:46.500
Zhongzheng Xu: That ends.

88
00:10:46.500 --> 00:10:51.230
Sy Tuyen Ho: And give a score. Like, give the relevant score to the topic.

89
00:10:51.460 --> 00:10:54.180
Sy Tuyen Ho: So, for example, I work on scalable Oversight.

90
00:10:54.770 --> 00:10:58.790
Sy Tuyen Ho: I want to decide whether this paper will be included or not.

91
00:10:59.650 --> 00:11:02.680
Sy Tuyen Ho: Like, instead of just a yes or no question.

92
00:11:02.680 --> 00:11:03.290
Zhongzheng Xu: Yep.

93
00:11:03.290 --> 00:11:07.679
Sy Tuyen Ho: And it's gonna be very, very hard for us to verify later.

94
00:11:07.860 --> 00:11:10.860
Sy Tuyen Ho: We will ask the RLM to generate a score.

95
00:11:11.350 --> 00:11:20.249
Sy Tuyen Ho: And yeah, in the very first step, we just asked them to generate a score from 0 to 10, whether it's relevant to the topic.

96
00:11:20.940 --> 00:11:37.920
Sy Tuyen Ho: I thought that it works, but it's still not very clear for our verification step later. So what we did is, like, we give them a red light for each round of score. We give them the criteria for this score.

97
00:11:37.960 --> 00:11:47.670
Sy Tuyen Ho: For example, if you generate a 10, it's going to be perfectly fit. It is a… it's fit, but somewhat, not very relevant.

98
00:11:47.730 --> 00:11:51.810
Sy Tuyen Ho: But if zero is completely, like, off.

99
00:11:52.220 --> 00:11:53.919
Zhongzheng Xu: So, we have a…

100
00:11:53.920 --> 00:11:57.980
Sy Tuyen Ho: rubric. Maybe we call it a rubric, so, that we can…

101
00:11:57.980 --> 00:11:58.400
Zhongzheng Xu: Berkeley.

102
00:11:58.400 --> 00:12:02.450
Sy Tuyen Ho: So that we can, like, like…

103
00:12:02.650 --> 00:12:15.309
Sy Tuyen Ho: interpret the score later, so that, like, human is… it's easier for human to… to verify later, so that is our… one of the strategies that we use for the prompt engineering.

104
00:12:16.820 --> 00:12:26.290
Sy Tuyen Ho: Another, another way that we, we will try to improve the, the, the, the, the promise that, because we know that, like, we are not, like.

105
00:12:26.710 --> 00:12:34.229
Sy Tuyen Ho: in… in our group. Most, like, all of us are not, like, a native speaker in English.

106
00:12:34.340 --> 00:12:43.599
Sy Tuyen Ho: So, what we have done here is a bit… a little bit tricky, but it's like, we were trying to, design the problem ourselves first.

107
00:12:43.710 --> 00:12:44.770
Zhongzheng Xu: And we will.

108
00:12:44.860 --> 00:12:50.170
Sy Tuyen Ho: And we will ask the ChatGPT to improve the, the, the problem, yeah.

109
00:12:50.170 --> 00:12:50.820
Zhongzheng Xu: Yeah.

110
00:12:51.270 --> 00:12:58.360
Sy Tuyen Ho: Yeah, so, that is probably, some tricks or some way that we can use to improve the prompt.

111
00:12:59.060 --> 00:13:01.599
Zhongzheng Xu: I see. And that works, pretty well.

112
00:13:02.100 --> 00:13:19.490
Sy Tuyen Ho: As of now, like, it's worked pretty, pretty well, like, in terms of, like, giving us the, the, the, the literature, like, the generate, the, the alternate, automate, generated, literature review, and also, like, it gave us a very.

113
00:13:19.780 --> 00:13:26.639
Sy Tuyen Ho: Intuitive way to verify, each step of the… of the pipeline.

114
00:13:27.000 --> 00:13:28.650
Zhongzheng Xu: I see. I see.

115
00:13:28.850 --> 00:13:34.370
Zhongzheng Xu: So, you said that, for, like, each…

116
00:13:34.670 --> 00:13:41.010
Zhongzheng Xu: intermediate outputs from the agents. For example, like,

117
00:13:41.340 --> 00:13:49.980
Zhongzheng Xu: the web crawler and the abstract reader, those… all of those intermediate outputs are also stored in a Notion, right?

118
00:13:49.980 --> 00:13:50.860
Sy Tuyen Ho: Yes, yes.

119
00:13:50.860 --> 00:13:53.320
Zhongzheng Xu: Okay, so Notion was sort of, like, the…

120
00:13:53.690 --> 00:13:56.840
Zhongzheng Xu: The place, that you monitor the entire system.

121
00:13:57.100 --> 00:13:58.380
Sy Tuyen Ho: Yep, true.

122
00:13:58.680 --> 00:14:10.289
Zhongzheng Xu: Got it. Could you brief me, walk me, or explain what a structure looks like? Because I haven't used Notion myself, I don't know, or just described the interface.

123
00:14:10.810 --> 00:14:19.979
Sy Tuyen Ho: You want me to walk you through, like, the, the, the, like, the structure of the database, or the way that I use to set up it?

124
00:14:20.260 --> 00:14:23.180
Zhongzheng Xu: Or could you just screen share, if that's…

125
00:14:23.180 --> 00:14:24.620
Sy Tuyen Ho: Oh, sure, sure, sure.

126
00:14:24.620 --> 00:14:26.460
Zhongzheng Xu: Yeah, I'll let you.

127
00:14:27.090 --> 00:14:28.920
Zhongzheng Xu: Give you the permission.

128
00:14:35.040 --> 00:14:40.339
Sy Tuyen Ho: If you want, I think I can share with you the whole structure of this.

129
00:14:40.340 --> 00:14:42.770
Zhongzheng Xu: Oh, yeah, yeah, that would be perfect. Okay.

130
00:14:43.310 --> 00:14:47.410
Sy Tuyen Ho: Okay, okay, can you see the screen now?

131
00:14:48.060 --> 00:14:48.950
Zhongzheng Xu: Yep.

132
00:14:50.050 --> 00:14:59.760
Sy Tuyen Ho: Okay, so it's gonna be this, let me show with you. So, like, the reason why I use Notion is because, like, I also use Notion for taking notes.

133
00:15:00.820 --> 00:15:08.470
Sy Tuyen Ho: Yeah, so it's very convenient to me, and very convenient, very familiar with me, and also with my teammates.

134
00:15:08.710 --> 00:15:10.800
Sy Tuyen Ho: So, for the…

135
00:15:10.990 --> 00:15:18.069
Sy Tuyen Ho: global oversight, so this is gonna be the… the Notion tab that we use for the whole project.

136
00:15:18.220 --> 00:15:25.910
Sy Tuyen Ho: So, for example, here we have, like, the meeting note. So, example, for meeting note here.

137
00:15:26.130 --> 00:15:31.350
Sy Tuyen Ho: In this paper, it's gonna be, oh, like…

138
00:15:32.130 --> 00:15:35.689
Sy Tuyen Ho: This is gonna be the literature review that we want to have.

139
00:15:36.780 --> 00:15:37.220
Zhongzheng Xu: Yeah.

140
00:15:37.220 --> 00:15:44.010
Sy Tuyen Ho: So, you see here, we have a structure. This is, from human, like, we design all of this first.

141
00:15:45.030 --> 00:15:47.870
Sy Tuyen Ho: So we redesign all of this, so,

142
00:15:48.040 --> 00:15:54.149
Sy Tuyen Ho: But what we want to do here is, like, we want the LLM to autonomously

143
00:15:54.210 --> 00:16:11.000
Sy Tuyen Ho: generate somewhat, a literature review for us to work from that, instead of, like, we work from that, like, we work, from scratch. So, what we've done is here is, like, we divide it into two, topics that we will work on.

144
00:16:11.340 --> 00:16:11.730
Zhongzheng Xu: It's gonna.

145
00:16:11.730 --> 00:16:17.459
Sy Tuyen Ho: The agent, monitor, monitoring, and the scalable oversight method.

146
00:16:17.810 --> 00:16:24.639
Sy Tuyen Ho: So for each topic, we will have, like, each, like, multiple people to work on this.

147
00:16:24.770 --> 00:16:27.070
Sy Tuyen Ho: And for each topic.

148
00:16:27.440 --> 00:16:36.050
Sy Tuyen Ho: For example, this is the workflow that we used, as I show you just now, like… Here.

149
00:16:36.830 --> 00:16:42.639
Sy Tuyen Ho: This is the whole pipeline for the multi-agents system that I told you before.

150
00:16:43.210 --> 00:16:47.670
Sy Tuyen Ho: Yep. So, what we really need here is, like.

151
00:16:49.180 --> 00:16:52.330
Sy Tuyen Ho: So, a very… because it's autonomous, so…

152
00:16:52.330 --> 00:16:52.740
Zhongzheng Xu: Yeah.

153
00:16:52.740 --> 00:17:02.719
Sy Tuyen Ho: what we need is just, like, for each topic, I showed you here, like, there are two topics, right? So for each topic, what I need is just some input, some configuration.

154
00:17:02.910 --> 00:17:03.360
Zhongzheng Xu: Right.

155
00:17:03.360 --> 00:17:10.449
Sy Tuyen Ho: And then the whole steps here, from step 1 to step 8, will be saved

156
00:17:11.119 --> 00:17:14.040
Sy Tuyen Ho: Will be done autonomously.

157
00:17:14.160 --> 00:17:23.139
Sy Tuyen Ho: So, for example, for step zero here, you need to define the archive query, main query, relevance score.

158
00:17:23.400 --> 00:17:27.989
Sy Tuyen Ho: It's like, it's like, it's a threshold to cut off some paper.

159
00:17:28.010 --> 00:17:30.899
Zhongzheng Xu: Yeah. And some LLM system prompt.

160
00:17:30.900 --> 00:17:37.380
Sy Tuyen Ho: some, LRM abstract analysis form, so I told you here, like, this is the rubric that I use.

161
00:17:40.400 --> 00:17:41.520
Zhongzheng Xu: Yeah, I see, I see.

162
00:17:41.520 --> 00:17:42.320
Sy Tuyen Ho: Yeah, yeah.

163
00:17:42.630 --> 00:17:51.969
Sy Tuyen Ho: And then all of this, I will work on the JSON format, because it's gonna be easier to interact between the agents.

164
00:17:52.620 --> 00:18:04.420
Sy Tuyen Ho: And then, we'll do it, like, we have a… this is a perplexity, summarization. Don't care about, like, perplexity. It's autonomous, it's not used the inference.

165
00:18:04.600 --> 00:18:19.370
Sy Tuyen Ho: But what we hear, like, as I told you before, like, after we analyze a lot of, like, abstracts and find a shorted list of paper, we will have, like, a prompt to analyze the whole paper.

166
00:18:19.850 --> 00:18:31.690
Sy Tuyen Ho: And then, given all of the paper that have been, like, saved, we were trying to put that into a taxonomy generation from

167
00:18:32.000 --> 00:18:36.090
Sy Tuyen Ho: So that it's gonna… can generate a whole, like, taxonomy.

168
00:18:36.090 --> 00:18:37.010
Zhongzheng Xu: Tectonomy.

169
00:18:37.010 --> 00:18:42.559
Sy Tuyen Ho: Yeah. And then finally, we will ask them to generate a writing

170
00:18:42.800 --> 00:18:51.170
Sy Tuyen Ho: Like, it… this is not the one that we will use, because we… at the end of the day, we still want humans to write the paper.

171
00:18:51.620 --> 00:18:51.990
Zhongzheng Xu: Yep.

172
00:18:51.990 --> 00:19:06.419
Sy Tuyen Ho: This writing is sort of, like, give you the whole story, what's going on in this, in this research topic, so that, like, we can, have a sense what's going on, and then we will write that later.

173
00:19:07.340 --> 00:19:12.770
Sy Tuyen Ho: And for the database, as I took here, this is the whole pipeline.

174
00:19:13.030 --> 00:19:17.510
Sy Tuyen Ho: But for the database here, so for each,

175
00:19:17.550 --> 00:19:36.060
Sy Tuyen Ho: for each topic, we will have our own database. So, for example, I work on this one, so I can walk you through this one first. So, for each database, in the very first step, like, when you use the web crawler to crawl the paper, we will have, like, title.

176
00:19:36.080 --> 00:19:37.290
Zhongzheng Xu: Right.

177
00:19:37.410 --> 00:19:43.409
Sy Tuyen Ho: All this excluded or further considered will be none here, right, at that step.

178
00:19:43.520 --> 00:19:50.769
Sy Tuyen Ho: But we will have… the text is all… the text will be generated by LRM after you read the… you read the abstract.

179
00:19:50.940 --> 00:19:56.739
Sy Tuyen Ho: you have URIO, you will not have this one. This will be none at this time.

180
00:19:56.970 --> 00:19:57.470
Zhongzheng Xu: Okay.

181
00:19:57.470 --> 00:20:02.400
Sy Tuyen Ho: This will be… these categories will be the archive category, so you will have this one.

182
00:20:02.860 --> 00:20:09.480
Sy Tuyen Ho: But at this time, the key contribution, it will be none too. Man fighting will be none too. Methodology will be none too.

183
00:20:09.540 --> 00:20:29.249
Sy Tuyen Ho: And this one will be… yeah, you will have this one. You will also have the relevant score generated by you reading the abstract. The source is from archive and unique ID, so this unique ID will be used throughout our process to identify the paper, yeah.

184
00:20:29.520 --> 00:20:43.410
Sy Tuyen Ho: And author, too. So after that, when you have all of this, when you have all of this one, so the thing that you will have after this step will be… after the very first step will be the relevant score.

185
00:20:44.030 --> 00:20:53.089
Sy Tuyen Ho: So after you have the relevant score, you will know that, like, it's based on the threshold that you set. If, say, if you set the threshold is 9.

186
00:20:54.040 --> 00:21:08.090
Sy Tuyen Ho: So everything will be… everything above 9 will be, included, but everything will be below the 9 will be excluded. But if you see here after this process, we will have a verification step.

187
00:21:08.530 --> 00:21:13.990
Sy Tuyen Ho: So the very second step here is, like, you have to look at all of the excluded paper.

188
00:21:14.480 --> 00:21:28.759
Sy Tuyen Ho: Yeah, like, for example, you can just, just here, from here, you can just filter, like, if it's checked or it's unchecked. So here, I want that it's gonna be, like, check, because I want to, like, investigate all of the.

189
00:21:28.760 --> 00:21:30.220
Zhongzheng Xu: Don't agree with us.

190
00:21:30.220 --> 00:21:40.299
Sy Tuyen Ho: Yeah, so what humans need to do right now, he is just filter all of this, have the excluded paper, and you're gonna decide whether this is the true

191
00:21:40.400 --> 00:21:41.830
Sy Tuyen Ho: Excluded paper.

192
00:21:42.160 --> 00:22:00.979
Sy Tuyen Ho: If it's not, you just click it here. It's gonna be… it's gonna become the, the included paper. But I would say, like, this is a very rare cases, because if it's already, like… I think the reason why I want to have this human verification is, like, I don't really trust LLM,

193
00:22:01.330 --> 00:22:06.710
Sy Tuyen Ho: yeah, analysis. But I think at the end of the day, for… at least for this step.

194
00:22:07.080 --> 00:22:10.639
Sy Tuyen Ho: It's worked pretty well. It works pretty well. Yeah, yeah.

195
00:22:11.120 --> 00:22:28.339
Sy Tuyen Ho: So after that, like, after we have, like, started, like, the first human verification on the excluded paper, I will probably regenerate all of this, use another LLM to generate all of the keyword manfinding and methodology.

196
00:22:28.520 --> 00:22:34.799
Sy Tuyen Ho: For us to have, so, by doing this one, we will…

197
00:22:35.180 --> 00:22:49.089
Sy Tuyen Ho: the next, human verification step is gonna be, more, like, like, high demanding. The reason is, like, right now, we will check all of the included paper, so we have to look at here.

198
00:22:49.240 --> 00:22:51.869
Sy Tuyen Ho: Right now, we check all of the included paper.

199
00:22:52.240 --> 00:23:05.180
Sy Tuyen Ho: So for all the included paper, we need to look at the tags here. We need to look at the key contribution here, look key, key manfinding and methodology. So from here.

200
00:23:05.660 --> 00:23:14.189
Sy Tuyen Ho: What we do is, like, we will have to carefully check if this paper is the… is a true included paper.

201
00:23:14.420 --> 00:23:15.350
Zhongzheng Xu: Hmm.

202
00:23:15.350 --> 00:23:32.109
Sy Tuyen Ho: And if it's yes, then we just leave it there. Otherwise, we have to check. If you really want to just remove it, we just click it here. Otherwise, if you, like, like, 50-50, you don't know, you are not very sure about that, you can check the further consider.

203
00:23:32.290 --> 00:23:41.110
Sy Tuyen Ho: Here. So the reason why we decided for the consider here is, like, we want to have a consensus among the teammates.

204
00:23:41.970 --> 00:23:47.400
Sy Tuyen Ho: So if someone not sure about this one, the other one can have a check later.

205
00:23:48.000 --> 00:23:48.890
Zhongzheng Xu: I see.

206
00:23:48.890 --> 00:24:00.599
Sy Tuyen Ho: Yep, so that is how we decide that. And after all of that, we will decide, because we have, like, included paper or excluded paper, it's whether, like, it's further considered or it's not further considered.

207
00:24:00.610 --> 00:24:09.209
Sy Tuyen Ho: And after all of this process, we will sit together… we will sit together and decide which paper will be included.

208
00:24:10.020 --> 00:24:22.900
Sy Tuyen Ho: Based on these two, and after all, like, if we have all of the included paper, which we already have here, so, we have the… all of the included paper, we will also exclude the further consider

209
00:24:23.280 --> 00:24:30.030
Sy Tuyen Ho: And then, based on all of this writing, we will ask the LLM to generate the taxonomy.

210
00:24:30.150 --> 00:24:33.490
Sy Tuyen Ho: And from the taxonomy, the taxonomy, the, the…

211
00:24:33.730 --> 00:24:49.910
Sy Tuyen Ho: the structure of the taxonomy is the JSON, where you have the main topic and multiple, like, citations, and then we will map back to this database, so everything will work on the database to generate this category.

212
00:24:50.250 --> 00:24:56.110
Sy Tuyen Ho: So at the end of the day, like, we have a few, like, if I'm not wrong, we have 10 categories here.

213
00:24:56.570 --> 00:25:03.969
Sy Tuyen Ho: And for each category, we'll be mapped back to each paper. So, if you see here, we only include the paper that

214
00:25:04.140 --> 00:25:10.070
Sy Tuyen Ho: Some paper that we want to include, and then we generate this back.

215
00:25:10.190 --> 00:25:23.180
Sy Tuyen Ho: And that is how we work… how it's worked in this pipeline. And Notion… how the Notion can be used here. I think Notion, it just… we use Notion just for the sake of, like,

216
00:25:23.270 --> 00:25:30.209
Sy Tuyen Ho: Simple, and also it's very intuitive for us to do the human verification.

217
00:25:30.560 --> 00:25:32.589
Zhongzheng Xu: Yeah. I see, I see.

218
00:25:32.590 --> 00:25:33.140
Sy Tuyen Ho: Yep.

219
00:25:34.540 --> 00:25:35.790
Zhongzheng Xu: And.

220
00:25:37.390 --> 00:25:39.149
Sy Tuyen Ho: Have you.

221
00:25:39.690 --> 00:25:45.659
Zhongzheng Xu: Have you used this pipeline, multiple times, or just… actually just for this project?

222
00:25:46.430 --> 00:25:56.910
Sy Tuyen Ho: I actually just use this, like, just use this for, for this project, and this project, but for two database, like this, this topic and the other topic.

223
00:25:56.910 --> 00:25:58.939
Zhongzheng Xu: Any other topic. I see.

224
00:25:59.150 --> 00:25:59.990
Zhongzheng Xu: Okay.

225
00:26:00.370 --> 00:26:04.260
Zhongzheng Xu: And then… So…

226
00:26:07.610 --> 00:26:11.230
Zhongzheng Xu: Sorry, I'm trying to locate the question.

227
00:26:11.890 --> 00:26:12.949
Sy Tuyen Ho: Yeah, but no worries.

228
00:26:13.070 --> 00:26:21.369
Zhongzheng Xu: You did, you did say that, you didn't really trust, like, LLM, so you needed this, like, human verification.

229
00:26:21.370 --> 00:26:22.260
Sy Tuyen Ho: Oh.

230
00:26:22.280 --> 00:26:32.039
Zhongzheng Xu: And then, so, do you feel like trust towards LLM or, like, the reliability is a kind of an issue?

231
00:26:32.420 --> 00:26:39.800
Sy Tuyen Ho: okay, I would say that would be the issue, cause…

232
00:26:40.020 --> 00:26:46.479
Sy Tuyen Ho: So, okay, let me tell you, like, the reason why I really want to do this process is, like,

233
00:26:46.590 --> 00:26:56.759
Sy Tuyen Ho: I… I… I don't trust the… the… the… the literature, the literature review generated, like, just generated by some,

234
00:26:57.260 --> 00:27:00.650
Sy Tuyen Ho: like, LLM agent out there.

235
00:27:00.930 --> 00:27:01.960
Zhongzheng Xu: Mmm.

236
00:27:01.960 --> 00:27:04.099
Sy Tuyen Ho: The reason why is, like,

237
00:27:04.590 --> 00:27:14.950
Sy Tuyen Ho: I… I… because I… I do research, previously, so I… I… I'm somewhat aware of the literature review.

238
00:27:15.340 --> 00:27:23.829
Sy Tuyen Ho: And when I try to work on the other project, like, the other topic, I am, like, entirely a new one.

239
00:27:23.940 --> 00:27:34.679
Sy Tuyen Ho: a new person. So when I'm trying, like, the very first thing that I need to do is, like, I need to know the literature review. So I'm trying to ask, like.

240
00:27:34.750 --> 00:27:49.329
Sy Tuyen Ho: Perplexity, and, Grok, and OpenAI, to generate a taxonomy. Just, just, like, type it, like, I want to work on this one, can you generate a taxonomy, or generate a literature review for me?

241
00:27:49.550 --> 00:27:57.400
Sy Tuyen Ho: So it generates something, because I'm not an expert, I haven't worked on that before, so I cannot verify that.

242
00:27:57.920 --> 00:28:12.020
Sy Tuyen Ho: So that's the first thing. So, how to verify that? I map back to what I already done, where I have sort of, like, have some sense of the literature review, and I ask them to generate a literature review for me to verify.

243
00:28:12.060 --> 00:28:24.380
Sy Tuyen Ho: And I, a lot of times, I feel like, those ChatGPT perplexity and, OpenAI, that's why it's very good at, like, communication, at, like, in, like, coding or something.

244
00:28:24.490 --> 00:28:34.639
Sy Tuyen Ho: But in terms of, like, literature review, it's… it's missed a lot of, like, important, it, like, it's missed a lot of important, works out there.

245
00:28:34.980 --> 00:28:51.120
Sy Tuyen Ho: And also, for a lot of time, especially, perplexity is… is… I would say perplexity is good. It's, it, it's, like, it's not generally the hallucinated vapors.

246
00:28:51.410 --> 00:28:57.739
Sy Tuyen Ho: But it's, it's just, like, it, it's, it can, have a very high precision.

247
00:28:58.030 --> 00:29:08.210
Sy Tuyen Ho: But the recoil is very low. However, the OpenAI is… it can generate a very, very good writing for the literature review.

248
00:29:08.660 --> 00:29:11.020
Sy Tuyen Ho: But, it's some… it's, it's…

249
00:29:11.310 --> 00:29:15.040
Sy Tuyen Ho: I'm not very sure for the very new version.

250
00:29:15.320 --> 00:29:18.360
Sy Tuyen Ho: But for the 01 version that I used before.

251
00:29:18.500 --> 00:29:27.630
Sy Tuyen Ho: I think it's generated a lot of, like, hallucinated papers, which is very, very, very, like, I think it's a serious concern.

252
00:29:28.540 --> 00:29:29.489
Zhongzheng Xu: I see.

253
00:29:29.490 --> 00:29:30.040
Sy Tuyen Ho: Yep.

254
00:29:30.690 --> 00:29:45.030
Zhongzheng Xu: Okay, so… Besides this project, for any sort of, multi-agent system, do you feel like

255
00:29:45.420 --> 00:29:51.169
Zhongzheng Xu: Human verification is kind of, like, always needed, or there can be some,

256
00:29:51.590 --> 00:29:56.340
Zhongzheng Xu: Transparency signals or design that would help with this?

257
00:29:57.320 --> 00:30:08.239
Sy Tuyen Ho: I would say at the end of the day, human verification is still… is still the most important, step, when we… when we work with the LLM.

258
00:30:08.450 --> 00:30:20.130
Sy Tuyen Ho: But, I think, as you will see in the future, or maybe now, I think LM will be soon, like, better than humans.

259
00:30:20.450 --> 00:30:25.540
Sy Tuyen Ho: Okay. At least for some, for some, tasks.

260
00:30:25.880 --> 00:30:40.020
Sy Tuyen Ho: maybe, like, math or something. I don't think a normal human, like, average human can make, like, LLM in terms of math problems or, like, coding problems.

261
00:30:40.110 --> 00:30:48.700
Sy Tuyen Ho: So it's very, very hard for a human to verify the LLM's output in the future.

262
00:30:48.800 --> 00:30:51.330
Sy Tuyen Ho: So I would say we, instead, like.

263
00:30:51.570 --> 00:30:58.839
Sy Tuyen Ho: But I still believe human verification is a need. But we might need to…

264
00:30:58.950 --> 00:31:08.750
Sy Tuyen Ho: Have a better, like, pipeline for the human verification instead of, like, just have a human verification from scratch.

265
00:31:09.110 --> 00:31:18.679
Sy Tuyen Ho: So one of the examples that I know people are doing right now, and I strongly believe that will be the… will be the future, is, through the debate.

266
00:31:19.050 --> 00:31:27.990
Sy Tuyen Ho: So, instead of, like, there is an assumption that, like, evaluating is easier than generation.

267
00:31:28.560 --> 00:31:37.109
Sy Tuyen Ho: So… What you have done here is, like, instead of, like, as human to generate

268
00:31:37.490 --> 00:31:42.210
Sy Tuyen Ho: the verification from scratch, based on… say, you asked the LLM.

269
00:31:42.540 --> 00:31:45.500
Zhongzheng Xu: To generate… to… to generate the…

270
00:31:45.700 --> 00:31:55.040
Sy Tuyen Ho: To generate a math question, like, to generate a math answer from a question, instead of just ask the human to read from scratch.

271
00:31:55.040 --> 00:31:55.359
Zhongzheng Xu: Read from.

272
00:31:55.360 --> 00:31:57.500
Sy Tuyen Ho: And… and verify that.

273
00:31:57.830 --> 00:32:02.490
Sy Tuyen Ho: So, rather, you will ask the outline to generate an answer.

274
00:32:02.790 --> 00:32:06.269
Sy Tuyen Ho: And you will add another, like, multiple agents.

275
00:32:06.430 --> 00:32:20.579
Sy Tuyen Ho: to debate about that. Like, for one, we'll be, trying to debate that this answer is a correct answer. Another, another agent will also, debate that this is gonna be the wrong answer.

276
00:32:21.320 --> 00:32:22.620
Sy Tuyen Ho: And they…

277
00:32:23.070 --> 00:32:33.239
Sy Tuyen Ho: Due to the debate process, they will, they will only highlight the most, like, the most important, components in the answer.

278
00:32:33.490 --> 00:32:48.970
Sy Tuyen Ho: So that it can lean towards the correct answer or the incorrect answer. So that humans, instead of read from scratch for the whole, answer, they can just only look at the important parts, and they will verify that later.

279
00:32:49.210 --> 00:32:59.120
Zhongzheng Xu: I see. Yeah, I actually had another interviewee who specifically worked on something you just described. He had this debate

280
00:32:59.220 --> 00:33:16.289
Zhongzheng Xu: multi-agent system, where, each… I guess each agent is also assigned with, or they also ask each agent to output some, confidence, for, I think each of their argument.

281
00:33:16.290 --> 00:33:16.790
Sy Tuyen Ho: Okay.

282
00:33:16.790 --> 00:33:23.329
Zhongzheng Xu: And then they showed that, in the end, it had higher accuracy than just having a single LLM.

283
00:33:23.330 --> 00:33:24.180
Sy Tuyen Ho: Yep, yep.

284
00:33:24.360 --> 00:33:29.480
Zhongzheng Xu: Yeah. Okay, and

285
00:33:34.080 --> 00:33:42.740
Zhongzheng Xu: Okay, I have two more questions, which are a little bit more, broad and general, so, you don't have to think

286
00:33:43.000 --> 00:33:48.850
Zhongzheng Xu: This project specifically. And okay, and first.

287
00:33:49.150 --> 00:34:01.139
Zhongzheng Xu: what, what do you think the biggest challenge is in developing a multi-agent system? I know it's a big question, so, I guess just, talk about anything that's, in your head.

288
00:34:01.470 --> 00:34:04.130
Zhongzheng Xu: Like, the biggest challenge in developing.

289
00:34:04.970 --> 00:34:09.810
Sy Tuyen Ho: The biggest challenge in developing

290
00:34:11.139 --> 00:34:13.919
Sy Tuyen Ho: It's really a very broad.

291
00:34:13.929 --> 00:34:32.369
Zhongzheng Xu: I guess I can give some examples to make it less broad. So, for example, I have… I had other interviewees saying that System Prompt is something that they really struggled with. I had this one, person, she worked at…

292
00:34:32.369 --> 00:34:38.129
Zhongzheng Xu: She did an intern at Amazon, and then the internship was 3 months long, right?

293
00:34:38.139 --> 00:34:43.879
Zhongzheng Xu: And… She was developing this multi-agent system from scratch.

294
00:34:45.759 --> 00:35:03.799
Zhongzheng Xu: the structure or the architecture itself wasn't too complicated, like, she had a few agents, but she really struggled with writing system prompts. The… she… I think she… she ended up spending two months just trying to refine her system prompts, because,

295
00:35:03.799 --> 00:35:07.359
Zhongzheng Xu: At first, she was just writing the system from scratch.

296
00:35:07.449 --> 00:35:23.239
Zhongzheng Xu: And then she had to, like, run the system and wait for some unexpected behavior, and if that happens, she will have to, like, manually include that into the… into the system from herself. And so it's just, like, a lot of, trialing… trialing and erroring.

297
00:35:23.239 --> 00:35:31.949
Zhongzheng Xu: And which took a lot of time. And, I think other people also mentioned, like, the…

298
00:35:32.349 --> 00:35:43.249
Zhongzheng Xu: Just the communication between agents, like, defining inputs and outputs, and how to make the entire, orchestration to work.

299
00:35:44.070 --> 00:35:55.930
Sy Tuyen Ho: Oh, okay, so it's really about the tech… okay, something. Okay, if that is the case, then I think I have an answer in my head now.

300
00:35:55.930 --> 00:36:04.300
Sy Tuyen Ho: This might not be, verified, because I haven't really, faced a really, concrete challenge on that.

301
00:36:04.390 --> 00:36:11.910
Sy Tuyen Ho: But, just from… based on my background, because, I previously worked on the, robustness of the model.

302
00:36:12.730 --> 00:36:17.890
Sy Tuyen Ho: And I think I also see that quite a lot of time, yeah, in the…

303
00:36:18.100 --> 00:36:26.450
Sy Tuyen Ho: When I use the LLM. So, I would say, one of the challenges is about the safety of the…

304
00:36:26.450 --> 00:36:28.550
Zhongzheng Xu: Of the airline.

305
00:36:28.550 --> 00:36:40.120
Sy Tuyen Ho: communication. So, what I mean by safety here is that, I know that, like, there is some people, they're trying to develop on multi-agents, they're trying to, develop,

306
00:36:40.300 --> 00:36:45.500
Sy Tuyen Ho: Specifically, they're trying to develop multi-agents for… for… for web agents.

307
00:36:46.550 --> 00:36:58.939
Sy Tuyen Ho: So, so, when we do with… when we're done with that, the… the problem is, like, the behavior of the agent system. We cannot control the behavior of the agent system.

308
00:36:59.280 --> 00:37:15.539
Sy Tuyen Ho: And, for example, like, that is a web, like, web shop agent. Like, there are multiple agents that we… we will interact with. So, for each agent, we're gonna be responsible for one, like, one store.

309
00:37:15.840 --> 00:37:19.180
Sy Tuyen Ho: In the webshop, so there are multiple webshops, right?

310
00:37:19.430 --> 00:37:20.040
Zhongzheng Xu: Yep.

311
00:37:20.460 --> 00:37:25.129
Sy Tuyen Ho: The question is, like, when you… when you're trying to… so, as a human, if you…

312
00:37:25.300 --> 00:37:40.309
Sy Tuyen Ho: If you, like, have multiple options for use, but there are multiple stores in the workshop, you will chat with one system, and then based on the conversation, will you, you will decide to move to another system.

313
00:37:40.590 --> 00:37:41.180
Zhongzheng Xu: Yeah.

314
00:37:41.330 --> 00:37:48.310
Sy Tuyen Ho: Yeah, so that will be the case, how humans will interact with that. So right now, what we want is, we want the

315
00:37:48.630 --> 00:37:55.609
Sy Tuyen Ho: another agent to have done that for us. But the question is, like, it's gonna be… it can raise the bias.

316
00:37:56.060 --> 00:38:02.379
Sy Tuyen Ho: Because I, I, I, I actually have an interview with, like, with, with Visa.

317
00:38:03.030 --> 00:38:09.019
Sy Tuyen Ho: for the internship. So, in that interview, like, we know that, like, in Visa.

318
00:38:09.390 --> 00:38:12.170
Sy Tuyen Ho: They have an issue about the bias.

319
00:38:12.340 --> 00:38:27.499
Sy Tuyen Ho: for the communication. What I mean by the bias is, like, instead of, like, when you chat with one agent, instead of it's refer to a better, store, and you will work with the agent in that store.

320
00:38:28.020 --> 00:38:37.250
Sy Tuyen Ho: So it just have a loop that you chat with that chat box, and it's the new, like, the next suggested agent.

321
00:38:37.380 --> 00:38:39.339
Sy Tuyen Ho: This rule, just that one.

322
00:38:39.500 --> 00:38:41.400
Sy Tuyen Ho: So it's gonna be a loop, yeah.

323
00:38:41.780 --> 00:38:55.560
Sy Tuyen Ho: So, I think, one of the ways that, Visa have sold this, I, I just got this during my interview, is that, like, they were trying to, generate, some… they, they would introduce some randomness.

324
00:38:55.850 --> 00:39:09.539
Sy Tuyen Ho: instead of, like, just, 100% believe in the next decision of the LLM, they just, based on… they just introduce some randomness so that it can refer to another agent.

325
00:39:09.710 --> 00:39:11.270
Sy Tuyen Ho: during the interaction.

326
00:39:11.550 --> 00:39:18.890
Zhongzheng Xu: But can't they just… Like, hard engineered to… Like…

327
00:39:19.280 --> 00:39:22.910
Zhongzheng Xu: Have the agent not being able to choose like…

328
00:39:22.910 --> 00:39:23.370
Sy Tuyen Ho: Yeah.

329
00:39:23.370 --> 00:39:23.760
Zhongzheng Xu: self.

330
00:39:23.760 --> 00:39:38.859
Sy Tuyen Ho: Yeah, yeah, yeah, I think that is one of the answers that, like, I already, I already answered during the interview, and they say, like, because it's… will be… have a lot of things there, especially, like, prom… like, prom, injection.

331
00:39:39.160 --> 00:39:46.720
Sy Tuyen Ho: So it still happens, even though we really designed a very careful, like, system prompt.

332
00:39:47.710 --> 00:39:48.550
Zhongzheng Xu: I see.

333
00:39:49.060 --> 00:39:49.620
Sy Tuyen Ho: Yep.

334
00:39:49.780 --> 00:39:57.129
Sy Tuyen Ho: So, that's the first thing. So, like, basically, like, the, the bias during the… the bias during the, the, the…

335
00:39:57.340 --> 00:40:05.159
Sy Tuyen Ho: communication, that's the first thing that I already know in Visa. Another one is very relevant to what I have.

336
00:40:05.330 --> 00:40:09.270
Sy Tuyen Ho: Like, my research interest is about a prong, like, prone injection.

337
00:40:09.790 --> 00:40:10.749
Zhongzheng Xu: I see, I see.

338
00:40:10.750 --> 00:40:18.200
Sy Tuyen Ho: The problem is, like, problem ingestion, like, we know that, like, it's a very challenging, vulnerability of the agent model.

339
00:40:18.290 --> 00:40:27.639
Sy Tuyen Ho: And I… and I feel like, like, the more agents that we have… so previously, we only had, like, one, LLM, one agent.

340
00:40:27.700 --> 00:40:38.310
Sy Tuyen Ho: And just a prime injection there, and we can prevent from that. But the more, like, you know that, like, when we have multiple agents, we will have a communication.

341
00:40:38.450 --> 00:40:54.519
Sy Tuyen Ho: And the communication is not just between them, between the agent, but rather we still have, like, all the external data during the interaction. So, the problem injection will be, like, the surface for the problem injection will be increased a lot.

342
00:40:54.920 --> 00:41:02.529
Sy Tuyen Ho: So that, it will be another issue compared to the, like, single agent, system before.

343
00:41:02.530 --> 00:41:03.130
Zhongzheng Xu: Yeah.

344
00:41:07.020 --> 00:41:10.499
Zhongzheng Xu: Yeah, I guess… I'm not too familiar with safety, but…

345
00:41:10.690 --> 00:41:13.860
Zhongzheng Xu: Since there's… there are more sources.

346
00:41:14.370 --> 00:41:17.699
Zhongzheng Xu: So there's, like, more places to attack, kind of. Yeah.

347
00:41:17.820 --> 00:41:18.370
Zhongzheng Xu: I see.

348
00:41:18.370 --> 00:41:19.060
Sy Tuyen Ho: Yep.

349
00:41:19.240 --> 00:41:29.739
Sy Tuyen Ho: Actually, like, in my group, we have, like, one paper called, like, to compare the vulnerabilities between the single agent.

350
00:41:29.860 --> 00:41:32.329
Sy Tuyen Ho: And a multi-agent system.

351
00:41:32.360 --> 00:41:35.099
Zhongzheng Xu: Especially, like, compare the LLM.

352
00:41:35.100 --> 00:41:38.109
Sy Tuyen Ho: and the LLM deployed in the web.

353
00:41:38.470 --> 00:41:50.190
Sy Tuyen Ho: environment, like, in the web environment. And we showed that, like, is the, the web agents will, like, the, the web… the LLM deploy in the web agent?

354
00:41:50.190 --> 00:42:01.680
Sy Tuyen Ho: we have, like, more than 40% vulnerability in the prompt injection, compared to the… compared to the single stack, like, single LM agent alone.

355
00:42:02.190 --> 00:42:03.130
Zhongzheng Xu: I see.

356
00:42:03.130 --> 00:42:03.920
Sy Tuyen Ho: Wow.

357
00:42:04.030 --> 00:42:05.740
Zhongzheng Xu: Could you share the paper?

358
00:42:05.740 --> 00:42:07.240
Sy Tuyen Ho: Yeah, sure.

359
00:42:08.350 --> 00:42:10.590
Sy Tuyen Ho: Let me share with you two.

360
00:42:11.860 --> 00:42:13.820
Sy Tuyen Ho: So…

361
00:42:26.730 --> 00:42:29.220
Sy Tuyen Ho: Just give me a few minutes.

362
00:42:29.220 --> 00:42:33.080
Zhongzheng Xu: Also, that was the last question I had, so I can…

363
00:42:33.590 --> 00:42:35.850
Zhongzheng Xu: Stick your time, I'll stop the recording.

364
00:42:36.160 --> 00:42:37.649
Sy Tuyen Ho: Yeah, thanks.

365
00:42:37.890 --> 00:42:38.810
Sy Tuyen Ho: No.

