WEBVTT

1
00:00:00.910 --> 00:00:01.850
Zhongzheng Xu: Okay.

2
00:00:02.220 --> 00:00:12.470
Zhongzheng Xu: Okay, then I'll start. So I'll start off, with a pretty abstract question.

3
00:00:12.580 --> 00:00:26.129
Zhongzheng Xu: Pretty big question. So, how… what is your definition of multi-agent system? And, what does it mean to you? So, like, how do you see it from… different from a single agent, or a single LLM?

4
00:00:27.560 --> 00:00:33.080
au774186: So, if we have the multi-agent…

5
00:00:33.950 --> 00:00:42.700
au774186: If we have energetic systems with multi-agents, and usually, these days, we put it into the space of, like, LLMs.

6
00:00:44.150 --> 00:00:49.000
au774186: Or… Or multi-modal language models.

7
00:00:49.390 --> 00:00:59.099
au774186: That can do all sorts of tasks, then these systems have… I have aspects like autonomy.

8
00:00:59.520 --> 00:01:02.969
au774186: To a certain degree, they should be deciding by themselves.

9
00:01:03.140 --> 00:01:05.730
au774186: They, are dynamic.

10
00:01:06.060 --> 00:01:16.919
au774186: In a way that they can handle all sorts of tasks. They're not restricted, not hard-coded, not hardwired to a certain set of problems, but they should be problem-solving,

11
00:01:17.180 --> 00:01:25.669
au774186: We should be able to do problem-solving tasks by themselves, without having a predefined set of problems to work on.

12
00:01:26.600 --> 00:01:31.570
au774186: then there's much more to it. Then there's a very big topic all about communication.

13
00:01:31.860 --> 00:01:36.970
au774186: In a way, how do these… Agents communicate.

14
00:01:37.250 --> 00:01:39.970
au774186: With the user, but also with each other's.

15
00:01:40.290 --> 00:01:43.089
au774186: How do you structure the…

16
00:01:43.500 --> 00:01:46.550
au774186: This leads to, like, questions on how to structure the software.

17
00:01:46.780 --> 00:01:49.839
au774186: Yeah. What kind of flows do you want to integrate?

18
00:01:50.270 --> 00:01:53.759
au774186: And then, there's much more to it.

19
00:01:54.970 --> 00:02:02.270
au774186: Then multi-agent systems are usually also designed in a way that they are… Specialized, in a way.

20
00:02:02.550 --> 00:02:05.449
au774186: So there's certain agents focusing on certain tasks.

21
00:02:05.800 --> 00:02:19.929
au774186: And this is very interesting these days, when there's lots of models coming up with certain specialities. So there's models that are good in reading tables, then there's other models that you use for image generation, and this way you can…

22
00:02:20.150 --> 00:02:31.670
au774186: have, very complex tasks being distributed among agents. But also, it comes with a lot of problems and, with a lot of questions. I try to be as vague as possible, because

23
00:02:32.130 --> 00:02:44.559
au774186: There's people that say, look, we should not have them too autonomous, we should have them automating things, and still having the user to decide, because as of now.

24
00:02:44.730 --> 00:02:46.220
au774186: There's…

25
00:02:46.510 --> 00:03:05.060
au774186: there's questions on who is taking responsibility. Of course, we… in the end, it's… it has to be a human, but the question is, like, how do we design a system? How do we abstract it in a way to have the best separation of concerns, to have the best abstraction level on where the user can still stay in the loop?

26
00:03:05.510 --> 00:03:13.530
au774186: Yes, so this is how I would try… To roughly describe it, and…

27
00:03:14.600 --> 00:03:17.279
au774186: This is not necessarily a definition.

28
00:03:17.410 --> 00:03:18.619
au774186: But I hope it helps.

29
00:03:19.360 --> 00:03:24.239
Zhongzheng Xu: No, no, it's fine, it's fine, it's just, your thoughts. Okay.

30
00:03:24.360 --> 00:03:35.549
Zhongzheng Xu: Okay, then I'll… I'll move on. Could you tell me about one or two, projects that have used multi-agent systems that you worked on in the past?

31
00:03:36.020 --> 00:03:36.850
au774186: Yes.

32
00:03:38.580 --> 00:03:50.039
au774186: I worked on a system with multi-agents, where I was generating research reports.

33
00:03:51.250 --> 00:03:57.989
au774186: Let me think… we were using, on a technical site, a database.

34
00:03:58.270 --> 00:04:06.960
au774186: Right. And then we were integrating it with the Python server running Langraph, and this LangGraph was orchestrating

35
00:04:07.290 --> 00:04:09.850
au774186: Different agents for different tasks.

36
00:04:09.850 --> 00:04:10.340
Zhongzheng Xu: Yeah.

37
00:04:10.340 --> 00:04:14.790
au774186: And then we had, in this case, We had them…

38
00:04:15.210 --> 00:04:18.849
au774186: Organized in a kind of deterministic way.

39
00:04:19.570 --> 00:04:20.380
au774186: And…

40
00:04:20.829 --> 00:04:27.970
au774186: If you want, I can go a little bit into the technical details on what we decided and how we implemented, if that's interesting.

41
00:04:27.970 --> 00:04:33.510
Zhongzheng Xu: Yeah, yeah, please go ahead. Also, like, what are the agents? What are their roles?

42
00:04:33.770 --> 00:04:34.799
au774186: Yes, okay.

43
00:04:35.370 --> 00:04:41.289
au774186: So… In the end, it… it came from the idea we want to have a very rough.

44
00:04:41.440 --> 00:04:44.229
au774186: User's description on a certain problem.

45
00:04:44.720 --> 00:04:48.989
au774186: And resulting in a sophisticated report.

46
00:04:49.110 --> 00:04:52.850
Zhongzheng Xu: But at the same time, it's also manageable by a user, because.

47
00:04:52.850 --> 00:04:54.420
au774186: What often happens is you…

48
00:04:54.560 --> 00:05:00.000
au774186: put something in… if you just use an LLM for a certain type of interaction, you say, hey, look, I have…

49
00:05:00.300 --> 00:05:07.420
au774186: hear these questions and give me the response, then what comes out is a very sophisticated answer, but it's not very…

50
00:05:08.250 --> 00:05:09.959
au774186: It's very stateful, in a way.

51
00:05:10.200 --> 00:05:13.580
au774186: So we wanted to, we wanted to…

52
00:05:13.700 --> 00:05:26.950
au774186: Have all the information, all the assets, the artifacts that are being generated, to be persisted separately in a way that we can manage it, so that we can say, for a certain artifact, that the user can still manipulate certain things.

53
00:05:27.320 --> 00:05:36.509
au774186: Without having to re-run the whole conversation. How did we do this, or what is it about? So, consider our, our,

54
00:05:36.700 --> 00:05:45.299
au774186: our report generation, pipeline, where we had the initial query, which was then going through, APIs.

55
00:05:45.480 --> 00:05:47.279
au774186: for a,

56
00:05:47.640 --> 00:05:56.069
au774186: initial research on existing literature. In this case, I can give you a very precise description. It was a semantic scholar API.

57
00:05:56.730 --> 00:05:59.660
au774186: And this was something which was very helpful for this.

58
00:05:59.740 --> 00:06:16.880
au774186: for this one agent, which was overall having the role. We didn't talk so much in roles, we broke it down more into tasks and actions, and in this particular case, it was more about generate… like, getting the related work.

59
00:06:17.830 --> 00:06:18.770
au774186: And…

60
00:06:19.280 --> 00:06:30.970
au774186: It was doing it by having this query and translating this human user query into a search string, because often cases, you have databases that have a certain

61
00:06:31.040 --> 00:06:47.469
au774186: syntax in order for their querying, and agents are very helpful in translating… in translating, in general, but also translating a human language into a search query string for this semantic scholar database. We were also connecting, then.

62
00:06:47.500 --> 00:06:52.910
au774186: some other databases. We use different agents that were, like.

63
00:06:53.040 --> 00:07:05.540
au774186: specifically strong, and, with these certain… or they had the respective system prompts in the end with the respective documentation on the database's query string, which is very easy to set up, so…

64
00:07:05.540 --> 00:07:08.649
Zhongzheng Xu: Yeah. This was a very easy, small agent, which was…

65
00:07:08.650 --> 00:07:16.019
au774186: basically taking over this, search. And then, when this search was going on, we persisted all this data.

66
00:07:16.910 --> 00:07:19.440
au774186: That was coming out into our database.

67
00:07:20.740 --> 00:07:33.330
au774186: to have our agentic system being kind of operational, so when it breaks down at a certain point, you know where did it stop, and where do we want to go on, and we kind of hardwired it into our… into our

68
00:07:33.330 --> 00:07:47.179
au774186: workflow in a way that we persisted, like, we were tracking all the agents, with tasks and actions, so we said this is our overall task, and this consists of different actions, and each action is persisted into the database, including state, pending.

69
00:07:47.180 --> 00:07:53.470
au774186: Like, if it's pending, if it's active, or whatever, also with the results, so we knew at any time what was happening.

70
00:07:53.470 --> 00:07:56.939
au774186: In terms of the process flow. And then,

71
00:07:56.950 --> 00:08:02.749
au774186: We were going on, after this first initial screening, we then had a, overall

72
00:08:02.960 --> 00:08:04.980
au774186: An agent which was giving a…

73
00:08:05.240 --> 00:08:09.840
au774186: overall strategy for the report, I would say, so it was more a…

74
00:08:10.380 --> 00:08:25.750
au774186: more a, strategic approach, where this one agent said, look, based on this information, we want to have a report which goes a little bit into this or that direction. It was kind of broad. But then we stopped, and then we had a different agent, and gave him the task in this

75
00:08:25.960 --> 00:08:32.999
au774186: particular direction. With this set of background information, please generate a set of stories.

76
00:08:33.299 --> 00:08:43.020
au774186: That we should, like, it was going a little bit more on a layer below, it was going more operational, more tactical, let's say. Let's say tactical.

77
00:08:43.179 --> 00:08:49.200
au774186: To make more precise storylines for this report. And then, finally, we were going one step further.

78
00:08:49.890 --> 00:09:08.740
au774186: to have, then, actual data manipulations. Then we had other agents that were supposed to solve, like, these kind of precise problems by generating code, you know? Then we had agents that were generating code for this one overall questions that was used to run on a…

79
00:09:08.760 --> 00:09:25.109
au774186: data set to then generate also visualizations. So we had another agent, which, after, like, we broke down our overall idea into smaller sub-components, and then this was generating code to generate actual artifacts, like a visualization that was fully interactive. And this way.

80
00:09:25.220 --> 00:09:35.479
au774186: we created, like, you can imagine, along the way, we had lots of different steps, which was… which were tracked on two ways. The one was… the one I already, explained. We had the,

81
00:09:36.430 --> 00:09:44.770
au774186: Actually, three ways. The one was we were tracking, like, what is the workflow like, which agents, which tasks, which actions are happening right now, what's going on.

82
00:09:44.770 --> 00:09:47.299
Zhongzheng Xu: What's the state of this workflow?

83
00:09:47.300 --> 00:09:54.630
au774186: Another thing we tracked was the results, so everything that came out, what that came back, we put into a separate table.

84
00:09:55.000 --> 00:10:02.490
au774186: And there you can see it was not that highly dynamic, in a way, because we had our fixed database schema, to have a

85
00:10:02.500 --> 00:10:22.210
au774186: kind of clear artifact that is coming out that the user can interact on. And then we, over time, created more and more artifacts, more and more entries for our database. In the end, it was rows for different tables that we created along the way, and when you assembled this together, you had a very nice-looking report on the front end, but the user was also able

86
00:10:22.370 --> 00:10:28.430
au774186: to edit these rows directly. So he can particularly say, hey, look, at this part.

87
00:10:28.740 --> 00:10:39.909
au774186: for this certain part of this, report, I want to change this full story. I want to replace it with another one, I want to remove it, or I want to regenerate parts of it, you know? Also, the strength…

88
00:10:40.050 --> 00:10:54.330
au774186: comes when you have multi-agent systems, that they are focused on a certain sub-step, that you can say, hey, look, this sub-step, this sub-process of this overall workflow, I want to regenerate in a way, that is… that it's fixed.

89
00:10:54.490 --> 00:11:01.030
au774186: And you can… and you do not have to rerun the whole conversation, you know? So, at any point of time, we want to.

90
00:11:01.030 --> 00:11:04.589
Zhongzheng Xu: This one from the… The agent that you want to intervene.

91
00:11:04.860 --> 00:11:09.320
au774186: Yes, it's kind of stateless, you know, so we have… we will be put…

92
00:11:09.430 --> 00:11:13.450
au774186: Like, let's say, typical systems engineering approaches into the system.

93
00:11:13.600 --> 00:11:16.409
au774186: Into the agentic, how we… with agentic systems that…

94
00:11:16.610 --> 00:11:25.509
au774186: I believe we should reduce, generally, state as much as possible, and we kept all the state in our database, so at any point of time.

95
00:11:25.750 --> 00:11:28.750
au774186: We wanted our agentic system to be able to

96
00:11:29.190 --> 00:11:33.570
au774186: Do any particular task, again, even with completely different

97
00:11:33.760 --> 00:11:41.530
au774186: workflows. So, for example, when you put something in ChatGPT, then there might be a predefined workflow running in the background, whatever.

98
00:11:42.120 --> 00:11:45.199
au774186: And it goes from step 1 to step 10, and then it gives you the result.

99
00:11:45.440 --> 00:11:59.709
au774186: But if you say, hey, look, please only do step 5, but change the initial data and only do that, you cannot really do that, right? And we built a system that was then, yeah, persisting all these sub-steps.

100
00:12:00.580 --> 00:12:06.859
au774186: I don't know the exact number of agents that we had, but it was not, like, hundreds or something. As you can imagine, it was maybe…

101
00:12:07.030 --> 00:12:13.980
au774186: 15 different, calls, different agents that we had. We were using…

102
00:12:13.980 --> 00:12:16.519
Zhongzheng Xu: That's still… that's still a lot.

103
00:12:16.680 --> 00:12:20.210
Zhongzheng Xu: I think, by far, you probably had the most agents.

104
00:12:20.460 --> 00:12:22.330
Zhongzheng Xu: Out of all my interviewees.

105
00:12:24.050 --> 00:12:32.169
au774186: Yes, I mean, it also depends on how you define an agent, so for me, one agent is one specialized, system prompt, more or less, you know?

106
00:12:32.170 --> 00:12:32.990
Zhongzheng Xu: Yeah, yeah.

107
00:12:33.680 --> 00:12:35.140
au774186: So,

108
00:12:35.470 --> 00:12:46.700
au774186: And then… I mean, not… an agent doesn't have to be even an LLM interaction, right? It could be even… ideally, it's even something deterministic, which is much faster, much more reliable and stuff.

109
00:12:46.700 --> 00:12:47.600
Zhongzheng Xu: Yeah, yeah.

110
00:12:47.900 --> 00:12:57.019
au774186: then, we were having our land graph, and as I said, we had this flow, which was very powerful in a way that we generated very nice data into our database.

111
00:12:57.750 --> 00:13:05.790
au774186: And we could even, to a certain degree, we were thinking about it, and we could do it at a later stage of time, but we weren't there at all to,

112
00:13:06.260 --> 00:13:11.230
au774186: Have another agent which is distributing the tasks among the others.

113
00:13:11.460 --> 00:13:17.660
au774186: But at this point, it was not even needed, because we can just have our line graph flow, our LangGraph node.

114
00:13:18.450 --> 00:13:26.939
au774186: like a state machine, you know? Like a state machine, it can distribute the tasks among the agents, and it was kind of predefinable.

115
00:13:27.180 --> 00:13:28.930
Zhongzheng Xu: So we predefined it.

116
00:13:29.010 --> 00:13:35.010
au774186: And whenever you can predefine something, it's usually faster, safer, and easier to test.

117
00:13:35.070 --> 00:13:36.230
Zhongzheng Xu: Yeah.

118
00:13:36.230 --> 00:13:42.910
au774186: Yeah, and this is one thing I would like… I'm talking a lot, I don't know if it's going into the right direction, but I want to give one more insight.

119
00:13:42.910 --> 00:13:43.580
Zhongzheng Xu: It is, it is.

120
00:13:44.260 --> 00:13:52.580
au774186: Okay, so when working with agents and building agentic systems, it's… much more…

121
00:13:53.460 --> 00:14:08.279
au774186: unreliable to a certain degree than traditional programming, than traditional systems building, because you have to always test certain workflows, which are the… which are heavily depending on all the context, first of all.

122
00:14:08.540 --> 00:14:18.279
au774186: Which is hard to mock, you can do, but, still, you want it to be various, so it's very hard to mock it

123
00:14:19.020 --> 00:14:27.129
au774186: sophisticated… sufficiently for, like, a very solid testing. Then it's depending on a synchronous

124
00:14:27.790 --> 00:14:41.330
au774186: calls, so you're always talking to an external API, because you can, of course, run your local LLM, but they will be way too slow, and you usually don't have enough computing power, so we end up using OpenAI, Gemini, Cloud, or…

125
00:14:41.330 --> 00:14:43.070
Zhongzheng Xu: Yeah. DeepSeek, whatever.

126
00:14:43.070 --> 00:14:47.679
au774186: And they are slow, and you have their rate limitations?

127
00:14:48.680 --> 00:14:52.529
au774186: So when you always have to wait for their responses in order to test something.

128
00:14:52.630 --> 00:15:10.899
au774186: Of course, tokens can be expensive. In our case, it was not expensive when we used cheap models, and we wanted our system to be, like, only doing simple calls. As I said, we broke it down into many different agents, and when you do smaller calls, very precise context, very simple tasks, then the LLMs will be better at it.

129
00:15:11.110 --> 00:15:19.719
au774186: And then you can even do it with the smaller models. So we wanted our… if you build an agentic system, it's good to have it agent-agnostic.

130
00:15:20.280 --> 00:15:24.819
au774186: LLM agnostic, so you can, switch LLMs.

131
00:15:25.430 --> 00:15:35.649
au774186: And then, there's a third, or a fourth, problem, when you, when you develop it, because they're not deterministic, so you have to check what is happening with the output.

132
00:15:35.750 --> 00:15:44.469
au774186: And oftentimes, you can predefine a certain schema, and oftentimes it also works, but not always, and then you have to do lots of,

133
00:15:44.570 --> 00:15:49.730
au774186: Yeah, error handling… Loading state management.

134
00:15:50.070 --> 00:15:50.900
au774186: And…

135
00:15:51.110 --> 00:16:07.850
au774186: enormous amount of prompt engineering, I would say, like, fine-tuning your prompts. And this is… this is very interesting, in a way that you sometimes have to add a very random sentence

136
00:16:08.070 --> 00:16:11.400
au774186: like… Please be very careful with what you should.

137
00:16:11.400 --> 00:16:12.790
Zhongzheng Xu: Yeah, yeah.

138
00:16:12.790 --> 00:16:25.740
au774186: And it changes the performance completely. Sometimes you have to take away a very logical sentence for the… like, it's… it's lots of trial and error, and what we used also to improve this over time, and I think this is…

139
00:16:25.910 --> 00:16:29.329
au774186: Also very critical, and…

140
00:16:29.540 --> 00:16:38.820
au774186: it's very hard to build agentic systems without it, is a proper telemetry LLM analysis tool, so in our case, we used an open source tool, which is called Langfuse.

141
00:16:39.420 --> 00:16:41.469
au774186: I don't know if you heard of it.

142
00:16:41.470 --> 00:16:42.629
Zhongzheng Xu: What does it do?

143
00:16:42.950 --> 00:16:47.139
au774186: And LengthViews is, a tool which tracks LLM calls.

144
00:16:48.560 --> 00:17:02.949
au774186: it's a cool… it's a tool that you can use to… you can integrate it even with Langgraph. It was very neat. You have… we had a… if you… if you stick to the frameworks properly, you can have very few lines of code, and your LangFuse is integrated with

145
00:17:03.060 --> 00:17:09.830
au774186: the length graph, and whenever there is an LLM call, you have your… you have a,

146
00:17:10.010 --> 00:17:25.759
au774186: you have a tracking of this… of this call. You see the input data, the output data, the duration it took, and, even the cost, if it's configured correctly, and you can see, really, the graph of your system on how things,

147
00:17:25.829 --> 00:17:34.400
au774186: interact with each other, so this is enormously… like, they're very helpful for optimizing the… Prompts.

148
00:17:34.800 --> 00:17:35.810
Zhongzheng Xu: I see.

149
00:17:36.620 --> 00:17:41.099
au774186: If you want, I can, try to give you an example, and show it.

150
00:17:41.530 --> 00:17:42.790
Zhongzheng Xu: Oh yeah, yeah, please.

151
00:17:43.450 --> 00:17:45.779
au774186: if I… if I should share my screen, but…

152
00:17:46.030 --> 00:17:47.590
Zhongzheng Xu: Okay, I'll let you do that.

153
00:17:47.590 --> 00:17:50.370
au774186: I have to try to dig it out right now.

154
00:17:57.660 --> 00:17:59.200
Zhongzheng Xu: Okay, you should be able to.

155
00:18:00.040 --> 00:18:01.109
au774186: Yeah, let me try.

156
00:18:21.170 --> 00:18:24.799
au774186: Okay, let me do something else. I think I have a screenshot of it.

157
00:18:54.680 --> 00:19:07.460
Zhongzheng Xu: Yeah, a lot of other people also mentioned, prompt engineering or prompt refining. I had this one interviewee, she had an internship at Amazon, and then

158
00:19:08.090 --> 00:19:14.850
Zhongzheng Xu: The internship was 3 months, and then she had to spend almost 2 entire months just,

159
00:19:15.140 --> 00:19:20.510
Zhongzheng Xu: tweaking the prompts. It's a lot of trial… trial and error,

160
00:19:21.310 --> 00:19:35.429
Zhongzheng Xu: She basically just have to, you know, write some system prompt, and then run the entire workflow, wait for some unexpected behavior to come up, and then just manually add that behavior back to the prompt, so that it doesn't happen.

161
00:19:36.900 --> 00:19:40.109
Zhongzheng Xu: It's definitely one of the issues.

162
00:19:41.030 --> 00:19:46.630
au774186: So… In this case, I can… I can show you this tool.

163
00:19:46.810 --> 00:19:47.480
Zhongzheng Xu: I see.

164
00:19:47.480 --> 00:19:52.240
au774186: Because… because it's… because exactly for this, this is such an enormous amount of work.

165
00:19:52.490 --> 00:19:54.820
au774186: You need a good… you need to have a good tool.

166
00:19:55.390 --> 00:20:00.579
au774186: And in this case, we have… this was a different system now.

167
00:20:00.780 --> 00:20:05.830
au774186: Than the one that I… or a different workflow that I described, but…

168
00:20:06.150 --> 00:20:10.440
au774186: We see here a diff… we see in the back different traces.

169
00:20:10.850 --> 00:20:11.210
Zhongzheng Xu: Yep.

170
00:20:11.210 --> 00:20:15.139
au774186: We see a list of… Kind of workflows?

171
00:20:15.690 --> 00:20:17.420
au774186: that we're running with an LLM.

172
00:20:17.670 --> 00:20:22.739
au774186: And… Then you can see the timestamps, so you can see these were all…

173
00:20:23.580 --> 00:20:25.940
au774186: On the same day, for example, and then…

174
00:20:26.140 --> 00:20:28.209
au774186: They are cert… they have certain names.

175
00:20:28.410 --> 00:20:46.319
au774186: And then you can zoom in into one. Again, like, I didn't find the right project right now in the system itself where it's hosted, but you can then see in this screenshot here still how it was used, and in this case, you see then this workflow, which has different steps.

176
00:20:48.240 --> 00:20:49.120
au774186: And…

177
00:20:49.460 --> 00:20:57.900
au774186: Then you have different prompts. For example, here you have something which says, extract and implicit… extract implicit knowledge.

178
00:20:58.340 --> 00:21:03.299
au774186: So in this case, there was a user interaction, and then we were… then this prompt…

179
00:21:03.750 --> 00:21:13.749
au774186: this is actually a great example. This prompt was… or this step of this overall agentic system was supposed to extract implicit domain knowledge on user interactions, so there was a…

180
00:21:13.850 --> 00:21:28.290
au774186: one information, and then the user changed this information to something else. So there was a difference. And this difference was done by a human user, and then we have an LLM, then we tracked all these differences, and then for each difference.

181
00:21:28.400 --> 00:21:34.599
au774186: we were running a separated, separate, LLM query to say, look.

182
00:21:34.940 --> 00:21:54.480
au774186: what is the difference, and what does it mean in our context? Because sometimes the user were maybe saying some information in this one spot, which can be also relevant for something else, and for such use case, we then wanted to know, what was the user's intent, maybe, so if we can find out something like that.

183
00:21:54.550 --> 00:22:08.670
au774186: And then we had our system prompt here on the right-hand side. You can see this is, like, a prompt that the AI… that the system was preparing. So we have here the original AI-generated content, and then the final user-edited content, and then we…

184
00:22:08.990 --> 00:22:11.499
au774186: Yeah, let the LLM judge it.

185
00:22:11.650 --> 00:22:22.900
au774186: And then we also persisted this information. And then later on, we can say, hey, look, this is the list of all the information that we extracted from user editing, and maybe we can condense it to a certain,

186
00:22:23.230 --> 00:22:37.590
au774186: knowledge that we can also apply somewhere else. And then you have, like, user added at some point, but you can, over time, optimize your whole system based on this information of this user. And, then we can see how we have a certain

187
00:22:37.920 --> 00:22:44.280
au774186: yeah, what configurations we had, so in this case, it was a very similar Jemi, Nye.

188
00:22:44.520 --> 00:23:03.399
au774186: model with a certain temperature, and this was working fine. We were thinking also about it, how do we do it? And it's also about, like, what kind of prompts do you have? And it's also, I said it before, it's about not messing up with the context of the LLM. So this was, in this case, it was just 3 edits, but still we have put it into 3 different…

189
00:23:03.620 --> 00:23:07.890
au774186: queries. We would have enough context to put everything.

190
00:23:08.260 --> 00:23:24.630
au774186: But we div into one single prompt, and we could have used a much bigger model to handle everything and stuff like that, but we found out that it's actually totally sufficient, and even… even… I would say even better, but you never know if it's really better, at least from my perspective.

191
00:23:24.720 --> 00:23:32.010
au774186: We found out that at this point of time, when we built the system with the LLMs and their capabilities back then, it was…

192
00:23:32.060 --> 00:23:46.689
au774186: from the, it was a very sufficient approach to split it down into very simple tasks, have multiple of them, run them in parallel, and have a very small model at this point, and we have enough information. This was sufficient, but again, like.

193
00:23:47.470 --> 00:23:55.790
au774186: maybe now the models are different, and you should handle it differently, but at this point of time, this was what we ended up with, and I was quite happy with the solution.

194
00:23:56.560 --> 00:24:05.709
au774186: Then you can see how we see the next step, where it was just some planning from the landgraph framework, and then there was another question, which was then…

195
00:24:05.960 --> 00:24:11.619
au774186: Running a little bit longer, you can see this has been, this has taken more time.

196
00:24:11.660 --> 00:24:12.739
Zhongzheng Xu: Yeah. She and…

197
00:24:12.740 --> 00:24:15.100
au774186: 3 seconds, because this had a bigger task.

198
00:24:15.240 --> 00:24:17.610
au774186: But even this was running on a very small model.

199
00:24:17.720 --> 00:24:22.609
au774186: And then you can even see, like, how this, planner was shifting

200
00:24:22.870 --> 00:24:29.829
au774186: You know, was orchestrating these tasks, so it was going first to this task, then coming back, then to this task, and then ending it.

201
00:24:30.840 --> 00:24:39.039
au774186: But… the system I was describing before, which had many more steps for certain workflows,

202
00:24:39.200 --> 00:24:40.709
au774186: This had a much more…

203
00:24:40.870 --> 00:24:45.519
au774186: Complicated graph, which is, of course, cool to see, because it's auto-generated from this tool.

204
00:24:46.030 --> 00:24:46.700
Zhongzheng Xu: Yeah.

205
00:24:46.700 --> 00:24:47.310
au774186: Yeah.

206
00:24:47.460 --> 00:24:48.960
Zhongzheng Xu: That's really cool.

207
00:24:49.340 --> 00:24:52.040
au774186: Yes, and… and this is the…

208
00:24:52.180 --> 00:24:56.449
au774186: This is the tool we open source, we hosted this ourself, and…

209
00:24:56.690 --> 00:24:59.749
au774186: Yeah, you can do many, many things with it.

210
00:25:00.670 --> 00:25:03.830
au774186: It's a little bit difficult to set it up in the first place.

211
00:25:04.060 --> 00:25:10.529
au774186: But afterward, it's definitely worth it, depending on the scope of your project and Where was it?

212
00:25:10.660 --> 00:25:13.880
au774186: Yeah, here. And it comes with,

213
00:25:14.370 --> 00:25:20.619
au774186: with integrations for existing libraries, so in our case, we… We're using this one.

214
00:25:21.460 --> 00:25:22.090
Zhongzheng Xu: Yep.

215
00:25:22.830 --> 00:25:26.959
au774186: And then it was… As I said, it was just a few lines of code here.

216
00:25:27.650 --> 00:25:33.240
au774186: And I think also here we had to… Provide the callback.

217
00:25:33.960 --> 00:25:38.949
au774186: And then it was fine, and here you see their screenshot.

218
00:25:40.100 --> 00:25:41.140
Zhongzheng Xu: Yeah.

219
00:25:42.540 --> 00:25:46.479
au774186: of an example trace here, where… yes, it's the same thing.

220
00:25:46.630 --> 00:25:49.420
au774186: Then you can explore around, play with it.

221
00:25:50.920 --> 00:25:57.180
au774186: And here you can even see… the model… And,

222
00:25:57.470 --> 00:26:10.920
au774186: Yes, and if you configure it correctly, you can even have the prices for the LLM, so you see the cost. Here it is. Yeah, yeah. So you have the cost breakdown. It's very nice. So you can estimate.

223
00:26:11.120 --> 00:26:15.669
au774186: I hear you can even see how much the overall workflow cost. You see, this is what…

224
00:26:16.100 --> 00:26:17.759
au774186: This was the price of this.

225
00:26:18.030 --> 00:26:18.980
au774186: of this…

226
00:26:19.280 --> 00:26:20.480
Zhongzheng Xu: Yeah, I see.

227
00:26:21.200 --> 00:26:22.150
au774186: listing.

228
00:26:22.310 --> 00:26:26.680
au774186: There isn't… That's basically it.

229
00:26:27.210 --> 00:26:29.769
au774186: it from the lengthfuse, yeah.

230
00:26:32.120 --> 00:26:32.870
Zhongzheng Xu: Yeah.

231
00:26:33.140 --> 00:26:33.700
au774186: Yeah.

232
00:26:34.110 --> 00:26:40.119
Zhongzheng Xu: So this is, like, your, interface, basically, to… for you to monitor the entire workflow.

233
00:26:44.950 --> 00:26:46.890
au774186: Good question. So…

234
00:26:47.220 --> 00:26:57.240
au774186: Yes, it's good… it's… it's also once you're… once you're finished with the development, this is also good for monitoring, but you even use it during the development process.

235
00:26:57.490 --> 00:26:58.570
Zhongzheng Xu: I see.

236
00:26:58.570 --> 00:26:59.290
au774186: Yes.

237
00:26:59.490 --> 00:27:10.529
au774186: But this is a good point, because you're not finished developing your agent tick systems. Once you're finished, they are going into production, and then when you want to operate them into a real-world

238
00:27:10.720 --> 00:27:21.080
au774186: scenario, you have to constantly monitor what the LLMs are doing, and you have to put certain metrics into it to find out if there's problems with it, because

239
00:27:21.470 --> 00:27:28.960
au774186: it's hard to test, you know? I'm always… I'm always seeing a risk there if you have an external system connected to a running application.

240
00:27:29.180 --> 00:27:35.540
au774186: That this external system might change its behavior, and this is… can be especially… true for…

241
00:27:36.010 --> 00:27:51.330
au774186: LLMs if you don't set them up carefully. So if you just set them up in a very easy way, and you don't think about it at all, what could happen is that from one day to another, maybe they shut down, and the company does not provide the API access anymore, your token expires, or,

242
00:27:52.060 --> 00:28:00.630
au774186: Or, even worse, or even more hard to track, and this is where these tools are very helpful for, that the AI maybe develops a certain bias.

243
00:28:01.030 --> 00:28:06.410
au774186: All these LLMs are biased, and you never know how, or you should know how.

244
00:28:06.650 --> 00:28:10.629
au774186: And, you have to work with these biases, right?

245
00:28:11.120 --> 00:28:11.770
Zhongzheng Xu: Yeah.

246
00:28:13.140 --> 00:28:14.060
Zhongzheng Xu: Yo.

247
00:28:14.440 --> 00:28:23.690
Zhongzheng Xu: So… so even with this tool, what, like, what type of, what type of errors, come up the most often?

248
00:28:24.160 --> 00:28:26.010
Zhongzheng Xu: During a development.

249
00:28:27.830 --> 00:28:29.420
au774186: Wow, this is a great question.

250
00:28:29.980 --> 00:28:35.560
au774186: there's… there's very… very many types of errors, and I…

251
00:28:35.690 --> 00:28:44.369
au774186: do not have the right number for this, I can just give you my… my feeling about it, which is that there is, of course.

252
00:28:44.910 --> 00:28:46.100
au774186: sometimes…

253
00:28:49.400 --> 00:28:50.740
Zhongzheng Xu: I'll just say there's…

254
00:28:53.000 --> 00:28:54.689
au774186: When things run in parallel.

255
00:28:55.050 --> 00:29:00.199
au774186: race conditions you might have. So you have to set up your system in a way,

256
00:29:00.830 --> 00:29:03.170
au774186: Which… which is that…

257
00:29:03.560 --> 00:29:22.270
au774186: one LLM prompt takes longer than the other one, and then you have inconsistencies in your process flow, which is a programming mistake, it's a programming error, and it's much easier to make programming errors with LLMs, but it's not an error which is happening inside the LLM, or with the data that comes out, but more from the nature that the API response just…

258
00:29:22.380 --> 00:29:40.849
au774186: maybe takes longer than usual, and then you break other things in the system. Other than that, you often… in agentic systems, you predefine a schema, and you predefine the response schema of the prompt that you put into the system that you say, for example, in our case, I just said, give me valid Python code to generate a

259
00:29:40.850 --> 00:29:54.169
au774186: A, visualization, or in this case, what we just saw, we had… we had a problem where we say, give us structured information on what knowledge might be extracted from this user interaction.

260
00:29:54.170 --> 00:30:06.890
au774186: So you need to have a certain data format in order to process it a bit further. Depends on what you do, but in our case, we usually put it into a database where we have our database schema, which we have to stick to, because we used a relational database.

261
00:30:08.960 --> 00:30:26.109
au774186: But also, if you create a… if you use it to generate code that you then execute, you also have to have valid Python code that you can… or whatever kind of programming language, but you have to have valid code that is syntactically correct, and hopefully also semantically, in order to be even able to execute it. And then,

262
00:30:26.420 --> 00:30:32.029
au774186: For other queries, for other workflows, you might not have such a strict

263
00:30:32.200 --> 00:30:36.610
au774186: Syntax requirements or formatting requirements, where you, for example, have

264
00:30:36.650 --> 00:30:47.820
au774186: one result that you just put into another LLM, and LLMs, they can't understand very much. So you can also just give them plain text, but the database usually cannot understand very much. They needed to have it in a certain schema, and…

265
00:30:47.820 --> 00:30:56.869
au774186: This is… this is one set of errors that comes out, which is basically the response formatting, and sometimes the responses also include certain type of,

266
00:30:57.050 --> 00:30:58.250
au774186: Moderation.

267
00:30:58.460 --> 00:31:00.260
au774186: And,

268
00:31:00.920 --> 00:31:13.200
au774186: and you say, just give me out the plain JSON. And oftentimes it works, but sometimes it still says, here's the JSON, and then it gives you the JSON. And the problem is, is here's the JSON, it's not JSON, so already it breaks there.

269
00:31:13.470 --> 00:31:22.099
Zhongzheng Xu: Yeah. There's a… there… you can provide schemas for that, you can provide a certain output schema that you expect to catch this error, and then it's usually fine.

270
00:31:23.540 --> 00:31:40.339
au774186: Or then it… it can be… you can iterate on it, you know? Then you can validate it deterministically, okay, this output data has… matches our schema, or it compiles, or whatsoever, you can do this deterministically, and then you can rerun the prompt and give it… and then after looping a little bit, you can

271
00:31:40.460 --> 00:31:54.490
au774186: hopefully fix it. And this you can track, for example, how often you had to fix until you see, oh, this is maybe very unreliable in returning the results, and just in terms of formatting. Content-wise, it's a completely different topic, because

272
00:31:54.650 --> 00:32:02.419
au774186: Content-wise, I have faced two other errors, that I also want to mention.

273
00:32:02.790 --> 00:32:12.070
au774186: that are especially important for this particular use case. So, there was… when you generate code,

274
00:32:12.200 --> 00:32:27.979
au774186: or when you say… tell to the LLM, generate code, and then you execute it during runtime, this can lead to crazy stuff. If you have, then, code which is not performing well, you iterate on the LLM and please refine the code that you then use to generate a certain artifact.

275
00:32:29.540 --> 00:32:45.239
au774186: on the… during runtime, this, of course, is highly, highly risky for many, many reasons, because, you can introduce all kinds of errors, you can introduce all kinds of security gaps that we might not know about. Of course, you can't say, execute this code safely.

276
00:32:45.490 --> 00:32:58.440
au774186: But who would… who would prove… who would tell you that you can completely, safely execute a code in your application coming from an LLM without introducing any security issues?

277
00:32:58.630 --> 00:33:14.770
au774186: And then another thing that can happen is, that the code is just incredibly slow or unperformed. So, for example, why… I mean, why do we use it in the first place? We use this code generation if we want to have a deterministic calculation. For example, I had a long, long list of data. I had

278
00:33:14.870 --> 00:33:28.949
au774186: 10,000 rows of all kinds of numbers, and I want to generate a visualization, or I want to do some data science with it. And then you can ask the LLM, give me code that does this analysis, or that creates a certain chart.

279
00:33:28.980 --> 00:33:42.199
au774186: Because you cannot usually… you should not ask the LLM, give me this chart directly, because, first of all, it might be too much for the context window if you give it the raw data, and secondly, it hallucinates. So if…

280
00:33:42.270 --> 00:33:56.979
au774186: it can fail very easily, so it's usually more reliable to go one step in between and say, look, this is the data, this is the schema of the data, I want to have this and this result, please build me the software code that generates this result from this data.

281
00:33:56.990 --> 00:34:05.829
au774186: And this is what the LLM is usually more performant with, and this might be a use case for generating code, and then what happens is this code generation maybe has

282
00:34:06.120 --> 00:34:24.329
au774186: recursion while statements whatsoever, and then you have endless loops within LLM-generated code, which, is very, very hard to debug, because you, of course, can put log statements or tracking in your own software, but if you run third-party software coming from an LLM, how should you track that?

283
00:34:24.460 --> 00:34:25.040
au774186: So…

284
00:34:25.040 --> 00:34:25.400
Zhongzheng Xu: Good.

285
00:34:25.409 --> 00:34:37.299
au774186: by definition, it's maybe even a bad idea to do it at all, but, I mean, at the same time, it's a very powerful use cases that we can only do because of, like, Gentex systems, right?

286
00:34:37.619 --> 00:34:40.319
Zhongzheng Xu: I want to give you a third problem that I faced.

287
00:34:40.530 --> 00:34:41.810
Zhongzheng Xu: Okay. Go ahead.

288
00:34:41.810 --> 00:34:46.249
au774186: Hmm… And then, and then I've…

289
00:34:46.449 --> 00:34:52.760
au774186: I've said everything from the top of my head about this one, which is sometimes you give prompts in a way

290
00:34:52.920 --> 00:35:00.549
au774186: with a certain context. So, sometimes you want the LLM to exclude its own domain knowledge.

291
00:35:00.780 --> 00:35:09.320
au774186: Or its… its own model knowledge. LLMs are very smart, they know already lots of things, but maybe you have a… maybe you have a certain set of information.

292
00:35:09.600 --> 00:35:11.420
au774186: And you have a certain…

293
00:35:11.570 --> 00:35:27.160
au774186: biography about a certain person or whatsoever, which might be incomplete. Maybe you have the first 20 years of Albert Einstein or whatsoever, and then you ask the LLM, where did Albert Einstein go to school, or whatsoever? And then the LLM, it reads this text.

294
00:35:27.160 --> 00:35:45.539
au774186: And then it tells you, hey, look, he went to this primary school, this secondary school, this high school, and then it also tells you he also went to this university, and he later on was professor here and there. But it was obviously not in the first 20 years, but the LLM still knows it, because in this case, Albert Einstein, the example, was a very popular person.

295
00:35:45.590 --> 00:35:54.879
au774186: And then what you can do is you can tell the LLM, only refer to this knowledge inside of this text, don't add any other information to it.

296
00:35:54.950 --> 00:35:58.100
au774186: And then, it rarely works.

297
00:35:58.460 --> 00:36:00.319
Zhongzheng Xu: Really? Oh, okay.

298
00:36:00.730 --> 00:36:03.900
Zhongzheng Xu: Yeah, like, prompting is probably the only way, right?

299
00:36:04.010 --> 00:36:07.380
Zhongzheng Xu: There's no other way you could, manipulate, like.

300
00:36:07.760 --> 00:36:11.119
Zhongzheng Xu: Whether you should refer to its internal knowledge or not.

301
00:36:11.320 --> 00:36:18.169
au774186: Exactly. So when you ask a question about some knowledge that the LLM already has, it's very hard to have the LLM not

302
00:36:18.360 --> 00:36:20.050
au774186: using it.

303
00:36:20.300 --> 00:36:22.719
au774186: You know, they like to use their own knowledge.

304
00:36:23.030 --> 00:36:24.080
Zhongzheng Xu: That's interesting.

305
00:36:24.280 --> 00:36:25.060
au774186: Yes.

306
00:36:25.280 --> 00:36:31.680
Zhongzheng Xu: Yeah. Speaking of this, so one of my other interviewees

307
00:36:31.890 --> 00:36:35.470
Zhongzheng Xu: told me about this framework. It's called,

308
00:36:35.630 --> 00:36:37.639
Zhongzheng Xu: I do not know how to pronounce it.

309
00:36:38.020 --> 00:36:44.030
Zhongzheng Xu: But it's… Okay, so it's a framework for prompts?

310
00:36:44.300 --> 00:36:51.250
Zhongzheng Xu: optimization of… I don't know if that's the right, right, term. But basically, it's,

311
00:36:52.080 --> 00:37:06.700
Zhongzheng Xu: Instead of writing the prompt, the long, super long system prompt, the string yourself, you define the input and output, and then it… and you also provide some few-shot examples.

312
00:37:06.700 --> 00:37:12.670
Zhongzheng Xu: And, it runs this prompt optimization algorithm to figure out the best system prompt.

313
00:37:12.890 --> 00:37:13.540
au774186: for you.

314
00:37:13.540 --> 00:37:31.090
Zhongzheng Xu: It works well, or I think it works well for some of the cases where you have a very clear… let's say you define a schema of input and output. It works well in those scenarios, but it may not still solve the issue that you just mentioned.

315
00:37:31.770 --> 00:37:39.410
Zhongzheng Xu: But I… I checked out the… I didn't fully re… Like, read the paper, But…

316
00:37:39.790 --> 00:37:43.269
Zhongzheng Xu: I think it can be useful for some… for some use cases.

317
00:37:45.440 --> 00:37:46.250
au774186: Yes.

318
00:37:47.150 --> 00:37:48.550
au774186: It seems interesting.

319
00:37:48.820 --> 00:37:49.800
au774186: I mean…

320
00:37:50.660 --> 00:37:56.790
au774186: this… I don't know when it was created, but I didn't see it yet, and there's so much…

321
00:37:56.930 --> 00:37:58.160
au774186: iteration.

322
00:37:58.420 --> 00:38:00.420
au774186: On this overall topic.

323
00:38:00.550 --> 00:38:08.030
au774186: So, for example, something like LangFuse, when I built the first Agentic workflow, it was… 2020… 2.

324
00:38:08.530 --> 00:38:21.579
au774186: or 2023, a few years ago, something like this, like Langchain, Langgraph and stuff, this didn't even exist, I think. Or I didn't even know of it back then, and now there's LangFuse, which makes it so much easier. Now, apparently, these tools

325
00:38:21.940 --> 00:38:26.739
au774186: they seem… it seems very promising. I mean, I don't know how much it takes in terms of training.

326
00:38:27.280 --> 00:38:32.130
au774186: To… to get out the… Right, promo.

327
00:38:32.130 --> 00:38:35.229
Zhongzheng Xu: I think it… it doesn't take…

328
00:38:35.450 --> 00:38:39.180
Zhongzheng Xu: A lot of training, that's why it's useful.

329
00:38:39.600 --> 00:38:43.600
Zhongzheng Xu: It also does… don't need a lot of data, so…

330
00:38:44.870 --> 00:38:45.520
au774186: Hmm.

331
00:38:48.780 --> 00:38:51.489
Zhongzheng Xu: But I haven't tried it out, myself yet.

332
00:38:52.430 --> 00:39:08.110
Zhongzheng Xu: Okay, I want to ask you a question. It might not have an answer, but I feel like a lot of the people who have experience in developing multi-agent systems are struggling with just system prompts, because, it's so…

333
00:39:10.690 --> 00:39:17.910
Zhongzheng Xu: There are just… there can be so many issues that come with it, because the use case scenarios are also, like, very different.

334
00:39:18.000 --> 00:39:32.529
Zhongzheng Xu: Like, what do you think can help? I think it's, like, theoretically very, like, difficult because, you always just have to run the model or, like, run the workflow in order to know the results.

335
00:39:33.350 --> 00:39:37.190
Zhongzheng Xu: And there's kind of just no way of, like.

336
00:39:37.760 --> 00:39:44.050
Zhongzheng Xu: Like, forecasting the results when you're still writing that prompt.

337
00:39:46.440 --> 00:39:55.880
au774186: Yeah, so… I mean… This is also… so this is definitely a big challenge that we have.

338
00:39:56.790 --> 00:40:02.190
au774186: how do we do… how do we come to the right system from… there might be,

339
00:40:02.310 --> 00:40:04.560
au774186: Already lots of research on that.

340
00:40:06.200 --> 00:40:17.180
au774186: But from my experience, it was oftentimes a trial and error, and this is why I was active… why we are actively seeking and building tools like LangFuse, and this is not the only one.

341
00:40:17.280 --> 00:40:21.010
au774186: There's lots of other tracking tools,

342
00:40:21.340 --> 00:40:24.969
au774186: that integrate this kind of LLM telemetry.

343
00:40:25.230 --> 00:40:30.180
au774186: And, observability of, yeah, LLM interactions.

344
00:40:30.400 --> 00:40:31.020
Zhongzheng Xu: Yeah.

345
00:40:31.580 --> 00:40:39.609
au774186: Just because there is such a big need, and lots of people from all sorts of directions struggle to build the right system prompts, and…

346
00:40:39.750 --> 00:40:52.780
au774186: For me, it oftentimes was kind of a trial and error approach. Oftentimes, I ask LLMs themselves, hey, look, I have this goal, what is a good system prompt? And then I take it from there and optimize it step by step.

347
00:40:54.480 --> 00:41:06.160
au774186: And, as I said, it's kind of trial and error-ish, and it takes a while, because you iterate slow. LLMs are slow, then you have various different use cases that you need to find out. It also takes a long time to find out if it is actually good.

348
00:41:06.160 --> 00:41:16.579
au774186: Because you cannot just run a unit test on it. Or you can, maybe, I don't know, depends on the question, on the separate prompt, but usually you run this prompt, then you wait for the result, and then you check it.

349
00:41:16.660 --> 00:41:26.029
au774186: And then you have to understand, first of all, is it now good, or is it what I want or not? Which is also not an easy question, to know if this answer is good, because usually you

350
00:41:26.160 --> 00:41:28.210
au774186: Sometimes you process it further, so…

351
00:41:28.440 --> 00:41:33.930
au774186: Then there… there are certain guidelines on how to build system prompts.

352
00:41:34.320 --> 00:41:43.760
au774186: But also, I feel that these have changed quite recently, so over time, some of the latest

353
00:41:44.830 --> 00:41:54.160
au774186: Technologies in this overall space, Seemed to be only for short-term, and very, like…

354
00:41:55.260 --> 00:41:58.790
au774186: Not final in a way, so that they're just, like.

355
00:41:59.320 --> 00:42:06.780
au774186: steps, but not the final best approach, not the final best practice, I think, some of these steps. So, for example, we have

356
00:42:07.000 --> 00:42:13.909
au774186: it's a different topic, it's not about building system problems, but we had this MCP, concept, which was,

357
00:42:14.230 --> 00:42:28.349
au774186: supposed to be the way to go. And now, a few weeks ago, I think from Entropic themselves, if I'm not wrong, there came out a paper which says, look, we have now a better idea, and MCPs have problems, and MCPs have problems with context, and…

358
00:42:28.470 --> 00:42:32.680
au774186: And stuff like that. And now they have a new idea on how to do it better.

359
00:42:33.420 --> 00:42:42.399
au774186: So, maybe in two years, it's a completely different approach, and also when it comes to building system prompts. Also, this might change so fast, so it's hard to find resources where you know

360
00:42:43.230 --> 00:42:51.410
au774186: this is something I can trust. So it's hard to find, like, trustworthy and easy and scalable and efficient approaches to build system problems, I think.

361
00:42:51.780 --> 00:42:53.080
Zhongzheng Xu: Yeah, yeah.

362
00:42:53.520 --> 00:42:59.909
Zhongzheng Xu: And I think, even… even prompt engineering died out, because the… the models are…

363
00:43:00.140 --> 00:43:02.299
Zhongzheng Xu: Way more capable now.

364
00:43:02.700 --> 00:43:05.999
Zhongzheng Xu: I was talking to my friend.

365
00:43:06.120 --> 00:43:15.109
Zhongzheng Xu: About this, and he told me that, he's working in the industry, and he told me that, like, his company had a couple of prompt engineer

366
00:43:15.780 --> 00:43:17.010
Zhongzheng Xu: positions.

367
00:43:17.400 --> 00:43:23.800
Zhongzheng Xu: it's not a meme, but, like, they actually had those positions, and now, they're gone.

368
00:43:24.140 --> 00:43:24.640
Zhongzheng Xu: Yeah.

369
00:43:24.640 --> 00:43:25.170
au774186: Yeah.

370
00:43:26.080 --> 00:43:30.840
au774186: Yeah, it's a very fast-changing, environment, and…

371
00:43:31.950 --> 00:43:38.860
au774186: And even in industry, like, in big corporates, They are just… Trying.

372
00:43:39.310 --> 00:43:48.559
au774186: at least from what I hear from the people that I know from industry, and… and there's interesting examples how they literally put something like,

373
00:43:48.880 --> 00:43:58.590
au774186: please be very, very careful with this prompt and do your very best to make it work, which goes into production. So, energetic systems, there are those kind of prompts out there.

374
00:43:58.760 --> 00:44:05.050
au774186: To optimize, and… And it's a lot of trial and error also in industry, or especially in industry.

375
00:44:05.930 --> 00:44:13.700
Zhongzheng Xu: Okay, so… Apart from, trial and error, so I think for a lot of

376
00:44:14.180 --> 00:44:22.450
Zhongzheng Xu: Like I mentioned before, so your project actually has the most number of agents that I have.

377
00:44:22.820 --> 00:44:33.870
Zhongzheng Xu: seen so far. A lot of other people, they have less agents, but each agent might, just get a much longer system prompt. They have,

378
00:44:33.900 --> 00:44:44.049
Zhongzheng Xu: like, many tool costs that each agent can do. So, the system prompt itself ended up being super long. And I'm thinking, so…

379
00:44:44.610 --> 00:44:55.240
Zhongzheng Xu: if you're… if you're a software engineer, and then you're… you're building the system, you're… you're editing the system from inside of whatever IDE that you're using, and…

380
00:44:55.400 --> 00:44:56.400
Zhongzheng Xu: Do you think…

381
00:44:57.500 --> 00:45:05.409
Zhongzheng Xu: if… if we have, like, a visual structure, because I feel like system prompts for a multi-agent system, they often come

382
00:45:05.550 --> 00:45:24.559
Zhongzheng Xu: with some structure in it. For example, you always define the input and output, and maybe some context, and maybe some other data that the agent should have access to. Do you feel like if there is some visual structure or a visual editing interface for System Prompt, it'll be helpful?

383
00:45:27.880 --> 00:45:34.480
au774186: Yes, in general, it needs… Also on the technical side, when you're a system builder.

384
00:45:34.860 --> 00:45:38.709
au774186: A certain type of… Let's say prompt management.

385
00:45:38.830 --> 00:45:44.280
au774186: By prompt management, I mean, that you should separate your prompts

386
00:45:44.380 --> 00:45:51.209
au774186: from the code, in a way that you have them stored into… inside of a different database or whatsoever. Like, you should have a…

387
00:45:51.250 --> 00:46:10.449
au774186: you should have a folder of all the system prompts to see, because when you… you don't want to run a new deployment whenever your prompts change. Maybe, as I said, there's something very urgently, then you want to fix them, so whenever somebody… the next person runs the query, you run the prompt automatically, because they are stored in a certain type of database, and this also led me to the question on how…

388
00:46:10.700 --> 00:46:15.569
au774186: how should you handle it? Because prompt management, you can even do it with LangViews, for example.

389
00:46:15.570 --> 00:46:16.730
Zhongzheng Xu: Hmm.

390
00:46:17.370 --> 00:46:24.540
au774186: But even there, it was… Kind of difficult to understand it a little bit, and it would have been…

391
00:46:25.720 --> 00:46:27.950
au774186: I mean, I'm just thinking out loud, maybe it would be nice.

392
00:46:27.950 --> 00:46:28.320
Zhongzheng Xu: Yes, yeah.

393
00:46:28.320 --> 00:46:41.449
au774186: a centralized, space of all the system prompts that you're using for different systems, and having this somehow integrated with the tracing, so that when you see there is a certain type of,

394
00:46:42.000 --> 00:46:50.479
au774186: errors always happening in certain prompts, that they are then highlighted in a certain way, so you know this is something where I have to take action, or that you…

395
00:46:50.760 --> 00:47:07.140
au774186: And you asked it before, to have certain error classes that we have for certain prompts, so that we say, for example, the input data oftentimes, or the data formatting is oftentimes very wrong, so we can assume the output

396
00:47:07.140 --> 00:47:19.469
au774186: part of the system prompt, because you're right, there are certain structures for prompts that we can maybe highlight the output part might be worth optimizing, or when we say the prompt always

397
00:47:19.710 --> 00:47:33.830
au774186: results in a very wrong answer, which just goes in a completely different direction. We can maybe assume that maybe in the beginning of the prompt, there might be something wrong, and then we highlight it, and then we can, give maybe the prompt

398
00:47:33.830 --> 00:47:44.389
au774186: Engineers, if such thing exists, we can give them some indication, look, your intro is too long, your output definition is too long, the context definition might be incomplete, and…

399
00:47:44.510 --> 00:48:03.699
au774186: And this way, it would be a very nice approach, as always, to try to break down things, trying to break down the system prompts into different sub-components, and optimizing them one by one, because as I said, we had this overall very complex idea, and I think it was a good idea to have many small prompts

400
00:48:03.700 --> 00:48:05.319
au774186: many small agents.

401
00:48:05.370 --> 00:48:16.139
au774186: to run as independently as possible on a very simple task. Of course, it's more easy and more difficult to set it up like this, when you have to program all these kind of things.

402
00:48:16.140 --> 00:48:28.819
au774186: But it led to better performance and better results. And I think the same is true for so many problems that we face, just in general. We have to break down things, divide and conquer, and I think same… we should try for system prompts, but so far I…

403
00:48:28.830 --> 00:48:32.810
au774186: I only see, like, this… this one system prompt.

404
00:48:33.150 --> 00:48:38.690
au774186: Yeah. Full stop. And now there's one full system prompt with 1, 2, 3, 4, 5 sub-components, or whatever.

405
00:48:39.080 --> 00:48:39.650
Zhongzheng Xu: Yup.

406
00:48:41.000 --> 00:48:50.920
Zhongzheng Xu: And also, like, the tracing error back to the, like, which part of the prompt that you just mentioned, I was thinking about this in my head, and I feel like…

407
00:48:51.730 --> 00:49:01.479
Zhongzheng Xu: is LLM… like, LLM probably, like, the only solution to do that? But again, we're having another LLM,

408
00:49:02.250 --> 00:49:07.760
Zhongzheng Xu: as, like, the… the verifier, I guess, or the debugger.

409
00:49:09.120 --> 00:49:15.240
au774186: Yes, I mean, you… you cannot do it very precisely, so you… I mean, there will be…

410
00:49:15.760 --> 00:49:31.440
au774186: I don't even have an overview of all types of errors that we will face with LLMs, and I would be very interesting to see all the types of errors. For, like, traditional programming, you can say, look, there's a stack overflow error, there's a null pointer exception, there's a whatever kind of exception.

411
00:49:31.850 --> 00:49:37.849
au774186: But for LLMs, I think it's much more difficult to name all of these, and then to be aware of it.

412
00:49:37.950 --> 00:49:51.059
au774186: And what you could do is to slowly get there. To slowly get there, for example, by having a deterministic check. Afterwards, as I said, you can have a schema validation, or you can run deterministic code, which

413
00:49:51.060 --> 00:49:59.179
au774186: maybe just gives you some data on it. The amount of token that was spent, or the runtime that it took, and then you can optimize from there so that you can…

414
00:49:59.260 --> 00:50:04.620
au774186: measure this kind of information, and then you can maybe see over time, like.

415
00:50:04.750 --> 00:50:11.940
au774186: That this metadata, for example, like, output length, input length, and time it took token consumption.

416
00:50:11.960 --> 00:50:28.120
au774186: These are just very easy numbers to get, but then you can go one step further, and for… depending on the query that you have, run further validations. Like, for a structured JSON, you can… of course, first of all, you can run the schema validation, but maybe later on, at a certain point, you can also…

417
00:50:28.530 --> 00:50:32.539
au774186: Collect some performance information about the content quality.

418
00:50:32.590 --> 00:50:33.750
Zhongzheng Xu: Yeah.

419
00:50:35.680 --> 00:50:36.950
au774186: so for…

420
00:50:37.750 --> 00:50:43.630
au774186: And then what you can also do is you can ask the LLM itself to generate some metadata.

421
00:50:43.770 --> 00:50:50.140
au774186: So, I was sometimes asking LLMs to categorize certain elements that they gave out.

422
00:50:50.750 --> 00:51:00.230
au774186: Right? And then you can use these as tags so… I think, in general.

423
00:51:00.660 --> 00:51:03.630
au774186: You will have to have another layer.

424
00:51:03.800 --> 00:51:16.140
au774186: you will have to have another layer in between. So I think it's hard to just have the LLM calls, and then the, monitoring calls of the system prompts, you know? I think you need to have one abstraction layer that somehow

425
00:51:16.290 --> 00:51:23.070
au774186: transforms all of this output into information on how you can improve system prompts.

426
00:51:23.100 --> 00:51:24.659
Zhongzheng Xu: Yeah. If that makes sense.

427
00:51:24.660 --> 00:51:26.260
au774186: Sounds a little bit abstract.

428
00:51:31.970 --> 00:51:32.990
Zhongzheng Xu: Yep.

429
00:51:33.480 --> 00:51:44.139
Zhongzheng Xu: I guess, yeah, the reason why we're having this interview is just that, we're trying to see, like, the problems that developers are having.

430
00:51:44.350 --> 00:51:53.759
Zhongzheng Xu: And I just kept thinking about system problems, because it's always mentioned by people, and, just trying to see, like, what we can do about it.

431
00:51:54.640 --> 00:51:55.320
au774186: Yes.

432
00:51:55.880 --> 00:51:59.460
au774186: Yes, it's about system prompts, and then it's also about…

433
00:52:00.930 --> 00:52:04.690
au774186: It's also a little bit about knowing, like, how to…

434
00:52:05.170 --> 00:52:15.450
au774186: divide them, right? So, of course, you can say a system prompts, how to build a single system prompt, but also about the system prompt intent. What's my overall goal with the system prompt? So.

435
00:52:15.750 --> 00:52:17.250
au774186: Should it be one prompt?

436
00:52:17.680 --> 00:52:27.250
au774186: Or should it be more? I also gave you the little show when I gave you this LangFuse, where I said, look, we have put it into different prompts, but maybe sometimes it makes sense

437
00:52:27.250 --> 00:52:40.730
au774186: who have many, many more, or maybe you can even combine completely different ones. Also, this is something that is always a question which is hard to say yes or no to. It's always, let's try to separate it. It feels more natural.

438
00:52:43.310 --> 00:52:54.609
au774186: But you never… yeah. And then, of course, like, building system prompts, all this telemetry and performance monitoring, and then also another challenge is knowing which model to use.

439
00:52:54.720 --> 00:52:59.040
au774186: Maybe also, like, having some kind of abstraction layer of saying.

440
00:52:59.420 --> 00:53:03.250
au774186: I just give you my prompt, Or my goal…

441
00:53:03.500 --> 00:53:09.959
au774186: As a system, and this system then chooses how to, which model to use.

442
00:53:11.100 --> 00:53:11.740
au774186: Because…

443
00:53:12.200 --> 00:53:12.870
Zhongzheng Xu: Yeah.

444
00:53:12.870 --> 00:53:31.440
au774186: to use, because now I said, yeah, we just use this model because it's small, and our tasks are simple, and we don't want to think about it so much. And every day there is a new model. Today, there was something I saw on any kind of benchmarks, there was Grok 1 Fast or something, I never heard it before. Code 1 fast or something, I don't know, and

445
00:53:31.860 --> 00:53:37.450
au774186: Maybe it's good, maybe it's better, and then you can have a self-optimizing system,

446
00:53:37.950 --> 00:53:40.490
au774186: Without having to spend so much efforts, because…

447
00:53:40.940 --> 00:53:44.500
au774186: In general, building agentic systems, it's very tedious.

448
00:53:44.700 --> 00:53:46.439
Zhongzheng Xu: It's very tedious, yeah.

449
00:53:47.190 --> 00:53:48.690
au774186: It's very tedious, and there's…

450
00:53:48.690 --> 00:53:58.989
Zhongzheng Xu: Yeah, just all sorts of different problems. I think… another person also made this analogy. He was saying how, like, agents

451
00:53:59.280 --> 00:54:13.700
Zhongzheng Xu: or, like, the nodes in multi-agent systems. It's just, like… like you talk about, like, divide and conquer is, they… they kind of… they're kind of just, like, the functions that we used to have before in programming, but they're… they're more intelligent.

452
00:54:14.150 --> 00:54:25.470
Zhongzheng Xu: And more, like… they fail, they fail more often because of its… because it's LLMs, they're non-deterministic sometimes.

453
00:54:25.700 --> 00:54:26.470
au774186: Hmm.

454
00:54:26.470 --> 00:54:29.910
Zhongzheng Xu: And they run into all kinds of Earth.

455
00:54:30.670 --> 00:54:33.390
au774186: Yes, yes, and this is true, and…

456
00:54:33.610 --> 00:54:45.830
au774186: I sometimes even try to think of how can we design our prompts and our system prompts, and our agents in a way that certain things… certain things we don't know yet about.

457
00:54:46.690 --> 00:54:49.639
au774186: But if we use these prompts in a way.

458
00:54:49.920 --> 00:55:02.780
au774186: that we can generate knowledge to solve this problem deterministically without using an LLM in the first place. I think it's always the main goal. So, I always… I would love a world where we have

459
00:55:03.260 --> 00:55:10.409
au774186: these agents, or LLM-based agents, just as an… as a step.

460
00:55:10.630 --> 00:55:28.360
au774186: that we use them as a step right now, because if we would solve this problem, like, with code, it would take too long, or it's certificate, or it doesn't work, so we use an LLM. But let's see if there are certain parts of this overall problem that the LLM maybe can be… get rid of, and we…

461
00:55:28.640 --> 00:55:33.969
au774186: Somehow transpose it into deterministic programming, so we become faster and more reliable.

462
00:55:33.970 --> 00:55:35.810
Zhongzheng Xu: Yeah. So, I think…

463
00:55:36.690 --> 00:55:47.660
au774186: I think this is why, what he says, they are very intelligent, and this is why we love it to just… okay, we don't solve this problem now programmatically, we just use an LLM, and we just use it as a function that just can do everything.

464
00:55:47.940 --> 00:56:01.340
au774186: But it would be nice to somehow always also see if there's something that we can take away from the LLM and put it into a traditional function, because I think they are still better. If they… if, I mean, they have a limited scope.

465
00:56:01.740 --> 00:56:05.789
au774186: But if we find these scopes for them, they will always be outperformed.

466
00:56:06.200 --> 00:56:11.630
au774186: I mean, using the calculator, 1 plus 1 will always outperform asking ChatGPT 1 plus 1.

467
00:56:11.970 --> 00:56:13.460
Zhongzheng Xu: Yeah, definitely.

468
00:56:14.810 --> 00:56:15.820
au774186: Yeah.

469
00:56:17.210 --> 00:56:22.859
Zhongzheng Xu: Okay, I think we're going over time, but you definitely shared a lot of insights.

470
00:56:23.160 --> 00:56:28.640
Zhongzheng Xu: That was… that was super helpful. Okay, I'll stop the recording just for now.

